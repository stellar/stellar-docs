---
title: "SPEEDEX Lane Policy and Parameterization"
description: "This discussion revisited CAP-42 with its revised lane-based fee semantics and introduced CAP-44, which defines how validators configure SPEEDEX trading. The conversation focused on validator discretion, fee transparency, downstream tooling impacts, and how SPEEDEX asset lists and parameters should be governed without harming usability or fairness."
authors:
  - david-mazieres
  - geoff-ramseyer
  - jonathan-jove
  - justin-rice
  - nicolas-barry
  - siddharth-suresh
tags:
  - legacy
  - CAP-21
  - CAP-40
  - CAP-42
  - CAP-44
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="lbYWhIEgUJE" />

The session opened by confirming CAP-21 (generalized transaction preconditions) and CAP-40 (new shared-signer type) had reached final comment, clearing the way for payment channels, bridges, and more advanced transaction sharing ahead of Protocol 19. With those nearly finalized, the group turned its attention to fee lanes and high-throughput exchange design.

Most of the discussion centered on the revised CAP-42 and the first public review of CAP-44. Participants explored how validator-controlled fee policies, execution phases, and SPEEDEX configuration interact, with particular focus on fairness, operator observability, and the burden placed on wallets, Horizon, and other downstream systems.

### Key Topics

- CAP-42 revisions and lane-based fee policy
  - Introduction of execution phases to prepare for SPEEDEX and future engines
  - Explicit acknowledgment that validators control fee regimes per lane
  - Renaming “surge pricing” behavior as discounts to better reflect user expectations
  - Trade-offs between Dutch auction guarantees and validator discretion
- Validator behavior, trust, and observability
  - Reliance on validator reputation and social contracts rather than hard guarantees
  - Need for better telemetry and audit tooling to detect unfair fee behavior
  - Discussion of exposing lane and fee data more clearly via Horizon
- Downstream ecosystem impact
  - Wallets may need more active fee management and retry strategies
  - Potential role for Horizon in automated fee bumping or submission retries
  - Education required as users can no longer safely overbid by large margins
- CAP-21 and CAP-40 status update
  - Both moved into final comment period targeting Protocol 19
  - Critical enablers for payment channels, interoperability, and shared signing workflows
- Introduction to CAP-44 (SPEEDEX configuration)
  - Validators configure SPEEDEX parameters and eligible asset sets
  - Motivation: batch trading for scalability, fairness, and reduced arbitrage spam
  - Separation of SPEEDEX into multiple smaller CAPs to avoid monolithic proposals
- SPEEDEX asset selection and governance questions
  - Concerns about validators “picking winners and losers”
  - Exploration of alternatives: issuer consent, reserve-based admission, multiple SPEEDEX pools
  - Computational limits (notably linear programming) driving caps on asset set size
  - Tension between flexibility, fairness, and operational simplicity

### Outcomes

- CAP-21 and CAP-40 were approved and entered Final Comment Period

### Resources

- [CAP-0021: Generalized transaction preconditions](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md)
- [CAP-0040: New signer type for shared transaction sets](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md)
- [CAP-0042: Multi-part transaction sets](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md)
- [CAP-0044: SPEEDEX configuration](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0044.md)

<details>
  <summary>Video Transcript</summary>

[00:00] I think that's our cue. Hello everyone and welcome to the Stellar Open Protocol Discussion. As always in these meetings, we discuss Core Advancement Proposal. Aka CAPs are technical specs that suggest changes to the Stellar protocol in order to allow the protocol to continue to drive forward, make progress, evolve to meet the needs of the ecosystem. These meetings- if you're watching, this is probably pretty obvious- are live streamed so that you can follow along. I do want to know this is a very- technical discussion, right? So if you're watching, you probably want to take a look at the CAPs that are linked to in the show descriptions, as are the previous conversations about those CAPs on the Stellar dev mailing list. I'm not going to identify all the participants. They're also listed there so you can look them up. They're qualified. So today, what are we doing today? We will again discuss [CAP-42](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md), multi part transaction sets, which was what we

[01:00] Discussed last time, but it's been significantly revised since that discussion. We are also going to have an initial discussion about [CAP-44](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0044.md), speedx configuration, and we'll get into what those proposals entail in a minute, but first I want to give a quick word about the life cycle of a CAP and there's a purpose for me saying all this. That I'll get to. So this meeting is really just part of the process of the creation and implementation of a CAP. Open discussion about CAPs. It happens on the Stellar dev mailing list. There's a link to that in the event description as well. So if you want to join, that's the current discussions or future discussions. Sign up for that mailing list. CAPs are drafted and sort of iterated on based on the async feedback and suggestions that come in via that list before they end up here being discussed in this meeting. So in this meeting we go over very specific questions and the goal is to get a CAP ready to move on. If goal is to get a CAP ready to move on, if a CAP is ready, it may be put up for a vote before the CAP committee and they decide on whether or not to accept it. So after a CAP committee vote, by the way, the CAP enters into a one week final

[02:00] Comment period and that gives everyone a chance to sort of raise questions again on the Stellar debt mailing list. If CAP makes it through a Final Comment Period it's implemented in a major Stellar Core release. Before that Stellar Core release actually hits the network, validators have to vote to upgrade the network to accept that release and so ultimately any sort of proposal that makes it into implementation into a Stellar Core release is accepted by the network through that governance mechanism. So part of the reason I'm bringing all that up today is that before we go on to the main discussion about CAPs 42 and CAPs 44, we're going to revisit a couple of CAPs: [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md), which generalizes transaction preconditions, and [CAP-40](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md), which adds a new signer type that multiple parties can use to more effectively share a transaction set for signing. So these are two CAPs that we sort of got pretty much across the finish line they have. They basically include changes that are helpful for people that are looking to develop innovative solutions on Stellar, specifically payment channels and bridges to other network. The goal of these is to sort of unlock the potential

[03:00] For greater interoperability. So again, these CAPs, they already went through the ringer. They're currently in a waiting decision. So that's their official status, awaiting decision. But first things first, I think the CAP committee is ready to approve both CAPs, and so I want to check in with the CAP committee and see if that's true. Now the CAP committee is Jed and Nico and David and Jed, who is not here. I checked with before and he is ready to approve. Nico and David, are you also ready to approve, or is there more to talk about? For capital 21, David, you approve, okay? Yeah, it's the same for me. Like the, there was a change yesterday or the day before on [CAP-40](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md), but it's, yeah, it's a minor change, so we're good, all right, approved. So after this meeting I'm going to change the status of those CAPs to Final Comment Period pending approval, and again, that gives everyone a one week window to like raise any final questions or concerns,

[04:00] I guess if a legitimate concern comes. up during that time, we can send it back to draft mode. But if there's no concerns- legitimate concerns- that arise during that time, those CAPs will be approved and that means they're going to be ready to move on to implementation for inclusion in the next protocol release, which is Protocol 19. So, as of right now, shortly after this meeting, [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md) and [CAP-40](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md) officially move into Final Comment Period. All right, now on to the real meat of the matter. We're going to talk about these two CAPs, and the first one that we're going to talk about is [CAP-42](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md): multi part transaction sets. This CAP gives validators control over which fee policy to use for individual transactions included in a ledger. We discussed it two weeks ago and there were several suggestions about how to improve it and, based on those suggestions, it was revised, and so I think, to start with, let's just get a quick overview of the changes to [CAP-42](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md). Nico, you want to take that? Yeah and yeah. So what I ended up-

[05:00] Yeah and yeah. So what I ended up doing- is basically I added the notion of execution phase, that in the short term, we are not going to have a new phase. It's more incorporation for things like speed x, where we expect to have that being done in as a separate phase, isolated from the existing transaction subsystem, and then within a phase. What I did is that, yeah, after our conversation last time, like I, it was clear that we needed to maybe clarify better like what, like when transactions, or make it more explicit, I guess that there was actually full control of the fee regime that was given to validators. So what I did was I added a new

[06:00] Tag that allows validators to basically use the actual feedback as fee being charged for certain transactions, as well as having the possibility of having a discounted rates when it comes to, for example, I mean, actually that's what we do today, like we actually give large discounts to transactions that are, when we're not in, search pricing. So that's what I did. I also added some clarifications on how this makes and why this makes sense. I think there were more questions actually- and that maybe, where we can maybe like spend some more time today around, like, are we basically defeating like the existing guidance that we give people

[07:00] When we say, bid whatever you feel comfortable with and then the network will, you will actually you're unlikely to actually pay for what you're bidding if you're over bidding- and I think there's a this is a fair question. So, on that, yeah, I don't know if David or John, I know like we use. I mean I, you know I like this new version. I think it's a lot cleaner. I think, possibly, in line with what you're saying, if I have one nit to pick in the backwards in the protocol- upgrade transition, we state, as this CAP does not change transition semantics, I think maybe we should just qualify this. Or, you know, like you know, changes the fee, but it's within this originally permitted semantics or something.

[08:00] But that's pretty, that's pretty nitpicky. Yeah, I mean, I think. yeah, like, yeah, I can fix that definitely. like I think the. interesting thing that was raised both by John, that is on the call, and by lee, that is not here today- are actually good questions around. Like today, you can basically bid by a factor of like a million, let's say, and you're actually very unlikely to be pay for, I mean you're exposing yourself to that, but then a bunch of other people would have to pay that type of fee, whereas with this proposal, depending on which the data you run into, you may actually pay that fee. So, like, for me, the rationale for that is that we are actually kind of

[09:00] Put it's. I think I put that as my notes, as we can actually rely on some of the a contract between validators, that the. You know the responsibility of a validator is to actually have lower fees, and then we can actually use in a way that as an expectation for validators when they just like today, when you run a validator, you have to have an archive, like you basically have a contract with other validators, and at any time, other validators can basically decide to remove that faulty or not fully compliant validator from their current set. And then we end up with, I think, a system that, yeah, that's realized more on this contract, that it's not necessarily a technical contract, it's a

[10:00] Kind of a in order to be aligned with the values of the network you have. You know, you shouldn't be forcing, like isolating, for example, transactions so that they just to mess with people. Just a question: if it's more of like a social contract or a set of expectations that validators follow, is there a method for them to know if other validators are living up to that? Yeah, so that's actually a good question. So, like it's, we actually have a similar contract today with like validators should not be excluding certain transactions from their current set, like you would like. The only way you can discover that today is by looking if there are patterns where some transactions- let's say, for certain assets or things like that are always excluded by some validators. You can actually go through history. This is all public and you know,

[11:00] You can basically kind of audit the behavior of the details, and I think we- this is a similar problem here, where you want to have the tools for auditing validators when it comes to this new power that we are giving them. so the data will be in history. I think what we have right now is that in Horizon, for example, we don't actually expose much of the sap information, so we don't know if what was actually happening during consensus runs, for example, like we don't know if some people are never aligned with other people, like, for example. So I think this is the type of work that can be done either by yeah by the SDF or by other people in the community, to kind of raise the bar, just like we have

[12:00] Raise the bar when it comes to what it means to run a validator with, you know, like projects like Stellar b right that are showing when validators are flailing or things like that. So I mean, this is like what you've just mentioned is sort of one solution like yes, we need a, you know some kind of validator conventions and ultimately there needs to be some kind of companion document that you know this guy might not be a CAP, but it discusses a like how you configure your Stellar Core to express these policies and like what we make easy and hard to do, like in the kind of default Stellar Core release. It's obviously gonna have a big impact on this, but I think there's another side of this which where you know, we don't want people to have the trust validators, so we should be maybe promoting fee bump transactions like a bit more heavily right. I mean certainly. You know one thing that like wallets could do is they could sort of by

[13:00] Default, like you know, whenever you sign a transaction, it would also sign like a couple of fee bump transactions that it would kind of like resubmit, you know, like more expensive versions if the first one didn't get through, or things like that. So I think we should be, you know, in terms of backwards compatibility, I think we should not, you know, I think we should make clear that there may be some expectations on the ecosystem, the sort of wallet side. Yep that, yeah, I'm sorry, Justin, go ahead. I was just gonna say because on some level, there needs to be education and tooling to understand how to manage dynamic fees more actively, because right now, as Nico said, you can just set your highest fee and dynamic fees, the network. You know what to expect, but you won't necessarily, is that? So what you're saying, David, is that we would need documentation and potentially tooling to, in order to educate people to the expectation that they, like wallets, will need to more actively managed dynamics.

[14:00] Or maybe we need a feature, an optional feature, for Horizon that people would enable on their private Horizons, maybe on the public ones, but where you submit three versions of a transaction and it sort of like keeps submitting like the more expensive one if the previous one hasn't cleared in some period of time or something right, you know I can't design the feature on the fly, but it seems like something you, we might want to think about. And if people if want to take their wallets offline or not, be sort of constantly monitoring the network, maybe this should be part of Horizon. Maybe this is an incentive for organizations to run their own Horizon, because they can, you know, control this better. Asked me to say the following: the cp is built on reputation. The concern that a reputable validator is going to go rogue and build an alternative transaction set, nomination rule set- is a bit far fetched and the ecosystem can build tools that monitor validators for behaving sensibly.

[15:00] Sorry, but there's a big space. between rogue and between like something like I might not agree with right. So I think, like the point is like there could be legitimate disagreement about this and we want users to be in control or to you know, they might, you know, rather not have their transaction be executed if they're gonna get charged way more than somebody else. I think, like, as far as tooling goes, I think like, you know, one thing that seller beat could do, that would do, would be really cool in this context- would be like, hey, like, what are the? Like, you know every 10th percentile- 10, 20, 30, etc. Fees that this validator is nominating. If validators are being like, pretty fair, you would expect every validator to nominate basically the same percentiles total fees, more or less, and that would give a lot of people evidence about like, oh, you know, like, this guy's being a big jerk

[16:00] And really just trying to get people to pay, whereas other people aren't doing that. But I agree with you, David, that you know. That doesn't change the fact that you know your validator might say like, hey, like, I think this type of transactions pay high fee and this one for payloafi, I think the opposite, they're equal distributions. And so, like, our stats look the same, but we're treating people differently. Maybe all the validators agree, but it just turns out that it's there's some small number of, like, legitimate transactions that are hard to separate from spam, right, and so you need to, I think, just not participate. We need an up, we don't. People shouldn't be forced into participating in the dutch auction, right. They should be able to, you know, control their- you know, maximum fee to ensure that they're not paying more than, like twice as much as other people, and that's something that's, it's doable. It just requires a little bit of support on the you know sort of platform side of things. For me, kind of the like. I've been thinking about this whole thing since we spoke about it

[17:00] Last was that last thursday, two thursdays ago, whenever it was, and I've kind of convinced myself that in the long run, like, looking like, let's say that we did this in x months, and then you look at x plus 12 months in the future, like probably everything is fine, everybody adapts to the new world. You know, maybe some new tooling develops, like David is talking about. Everybody switches to kind of this lower fee mentality where you don't just bid like 10x, 100x, whatever what you're willing, what you actually want to pay at the present. But like looking right now- you know I was looking at dashboard for the last couple minutes like every minute and the 50th, 60th, 70th, percentile fees have been sitting around 20 000 stroops pretty much non stop in that period. Sometimes it's been the 60th, 70th, 80th, but 20 000 troops and that's, you know, about one third of the total traffic. So there are a lot of people who are bidding way above the fees that are actually getting accepted. In the short run. If

[18:00] Those people aren't paying attention, those people are going to be pretty sad. I think, yeah, they're just 100 million stroop bid. Yeah, so I mean I guess that's 10 lumens or whatever, is that 250. Yeah, it's not insane I'm. I have to say I've literally said to many people: and I just do a hundred thousand troops, you'll be fine, don't worry about it, just said 100 and you know, I'll try to tell those same people. If that changes, that made sense right. So this change, I think what you're saying. Yes, yeah, this short term pain, long term gain thing that you're pointing out, John, may well be true. I mean, like, I think the difference is that like yeah, like if, for example, like these fees of the network are, like you know, around like the minimum, like when you're not in such pricing price- right, that's kind of where you are. Like, if you paid like even

[19:00] 100 times more than that, like that still gives you like some advantage compared to other people, like you're still doing some level of bidding. Like I think that's. The difference with this CAP is that you would not be bidding like yeah, like a million times more, like you would be bidding, only, you know, a thousand times more. I mean that's. I think that's. The shift is that you have to be maybe closer to what you want to pay, unless, like in some situations like the, I'm like the people like putting 10 lumen as a fee. I mean this is what we, I think, discussed in the past. Those are the type of numbers I would expect when, if you do like, some trades that actually are going to be valued at more than that, and I think those over the long term,

[20:00] And I think those over the long term, they will continue to actually be bidding that high and that makes sense, but they will actually pay, them pay. That that's the difference. Well, those fees should come down quite a lot actually, and that's one of the things that made me think. In the long run, this would be better. Like right now. Imagine that you're racing for an arbitrage opportunity. Let's say, the arbitrage opportunity, using the numbers that we were just using, is worth, you know, 11x allowance. You're like I'm willing to bring 10xl. I'm going to try to get you know, one excellent profit. In the worst case, maybe you're even willing to bid 11. But let's just say 10 for the sake of argument. Well, right now, it's like I'm going to bid 10, but I'm probably going to pay 100 troops, a thousand strips, whatever based on like what normally happens during surge pricing. And so you've got, let's say, like 500 people or something trying to do this. One of them wins. The other 499 transactions go in the garbage. But in the real world, where everybody is actually bidding for this, you know 10 XLM thing and they're going to pay 10. Meaning, with this proposal, when I say in the real,

[21:00] This proposal, when I say in the real world, everybody would actually lose money and expectation, so they'll all have to drop their fees by a factor of the competition. So fees will actually, like their peak fees would come down by a factor of 500 in this example. So that's pretty cool. That gives other opportunities a much better chance to be competitive. So I think long term there could be benefits beyond, just like you know, people bidding closer to reality. But also just like changing the discrepancy between, like I'm trying to send a payment to my friend and I'm trying to take advantage of some arbitrage opportunity. Yeah, I got one more question from Tomer here. Actually it's too. How do we envision validators controlling this? Will we give them configuration options in Stellar Core? So yeah, like I thought a little bit about this. All right, like last time, there were actually questions around that- like: are there like certain configs? I think it depends on the type of strategies we're going to implement. Some of the things where I can see some configs,

[22:00] The things where I can see some configs being useful, are, if we start to have like this type of surprising in a way. that is doing something maybe over historical data, like when you look at the last few ledgers, instead of just looking at the current pledger and then as soon as you said the last few ledgers were, then how many? Right? So that would be an example where you have a tunable for the cur, like there's another tunable. I think that I can think of. That is related to the. Actually, the first example I had in the CAP that was: can we limit the number of the operations in for that are potentially touching the DEX, right. So, like, that number is an example of a tunable also for that policy.

[23:00] But of course, you know, people can always go around any of those things, like they can write custom code if they want. So that's a, that's for that. But yeah, for the ones that I think that we will be implementing, like they likely will have, yes, some tunibles in them to potentially even disable them. Companion document: right, that would be more in the actual box of yeah, so is the general consensus at this point that people feel I feel like I literally just interrupted Justin, who's about to say the same thing. But do people generally feel good about this now, feel bad about this? I feel a lot better than I felt when we spoke about this two weeks ago. I feel like we're in a place where, like,

[24:00] I think we could get people behind this. and it'd be a bit of a mind shift, but- people would feel good about it. I don't know if everybody else feels the same way. Yeah, I think this has been better than the- much draft. I think, much better than the last draft. I think it's this is a tool and I think there's, understandably, some anxiety over, like, how is the tool going to be used? But I think the worst case scenario is not terrible. That, like, we just get rid of the dutch auction and the most likely scenario is that people that you know Stellar Core has some parameters and, like the thing you know it's done in such a way that you know it's like simple payments are very unlikely to do this because they would require changing the sequels plus, and that, yes, we understand that certain assets and then overloaded DEX may end up getting this treatment. Yeah, exactly like the worst case scenario is that we end up with what basically everybody else in the space is doing and what we used to have before we went from construction, which was before the

[25:00] Fee bump transaction, and so I think that there's some I still like the dutch, auction, but I think it would have been harder to make the case for the dutch auction if we already had the fee bump. And so you know, now we've got both options. Yes, I think, like I don't know if people so, and I think that would be actually part of how to explain to people like the shift here is that I think by renaming what we do to discount, I think it's actually helps quite a bit people understand what is actually happening in terms of like. You know, when you think of a discount, I'm potentially going to give you a discount- you go to a store, you actually expect to more or less pay. What is, you know, displayed before you start to haggle? And I think that's kind of the. You know, I think it's more similar to this via slack. Tomer is asking questions about downstream systems. So how do we expect downstream systems to understand

[26:00] The fees collected? And then he also said dutch auctions are already an oddity in the space, so this only makes fees more opaque and then again raises a concern. We need to figure out how to explain this to downstream systems. So I know that this is a. Well, I think that's a valid question. Downstream systems: how do we expect to explain this? That we've removed the oddity at least some fraction of the time? Right, I think there was a question around like being able to see which groups and all that from Horizon, from lee on the mailing list. I mean, the data will be there, like it's actually part of the consensus value. So, yeah, like I think, yeah, we'll probably have to think about how to maybe like expose this a little better from the main api that comes on. Maybe it's the api also is like going to give better

[27:00] Idea of, like, what's going on. And the current one is actually also kind of strange because the stats of other, if you look at as a developer, if I hit that api that gives me stats about the last five ledgers, it's actually not even today. It's actually not that great in terms of experience, because you're looking at, like a lot of people like, should I use the tenth percentile or my year's percentile? Like and actually you're not even guaranteed of anything by using this. They're sometimes because it's only five ledgers and you know past performance doesn't predict future. So, yeah, I think for a lot of developers, they just ignore. They don't. They just say: why do I need to check, like they've learned? Why do I need to check fee stats if I just can set my fee super high and only be charged the lowest? And so I guess this comes back in part to that question: right like, if suddenly, yeah, displaying information about fees in order to make an informed decision is more important, I think the so it will be actually.

[28:00] I think the so it will be actually, you can still have, like, the approach can still be done very much in isolation, because we, you know, by like similar to you know what we have today, right, where people build like some high number and then that's kind of it like now they will have to be a lower than, not better than that. if they are not comfortable with that higher number, and then retry and then do a, maybe like a like depending on the app. right, like, it depends. Like for many applications building like a tenth of a cent is actually fine, right in fee, and that's kind of the end of the story. If, like, if you really want that transaction to make it, but you don't want to necessarily expose yourself to variations,

[29:00] I think a fine strategy is just to have like a timeout and multiplier on top or something like that. Right, like, and it's not complicated. Okay, I mean, I guess if I had one reservation about this, maybe I should just voice this now I'm not sure this is a problem, but in the nomination value comparison, I'm not 100 convinced that the second item in the four tuple, the second most significant item, should be total fees that the transaction set will collect. An alternative, which I don't know if it's better, but the point is I don't know an alternative- would be to say it's the total pledged fees, even if the total collected is less, because some transactions ended up in the discount bucket. Now I understand there's definitely a good argument why total fees, of course,

[30:00] Is better. Otherwise maybe validators could like game the system or do other weird things. However, there's just one scenario that maybe I'm concerned about. Imagine a situation where we have this thing that we think is not very economically beneficial and it's spam. So we say, okay, we're going to put that in the non discount bucket and everything else goes in the discount bucket, and now you have these transactions that are very important, that are willing to pay a lot of money, but, because they're in the discount bucket, two kind of maxed out nomination values that are completely, both completely full, so have the same number of operations end up favoring the one that collects more fees where it actually even though more fees were pledged to the other one. It's just that the other people are getting a discount that maybe they don't even want. Yeah, I mean that, yeah, that I mean, I think I don't think it like, using the total feedback is necessarily a problem,

[31:00] Like we do. We just need a way to compare right, like the quality. So I think that you know it actually is aligned with the. You know the intent. Yeah, so I think, yeah, like the bid is fine, probably so. Are there other questions to raise here or points to discuss? I feel like there's still issues that maybe already exist on a mailing list. Is that correct or actually, I guess I'll just say it this way. I feel like this should that this is not quite ready to move into a vote or anything, that there still may be some time for marination just to think about a few of these issues, including maybe thinking about the downstream implications. Just, yeah, I think I want to maybe add a little bit of detail of like the type of expectations for the downstream systems, and then

[32:00] We can, and then we can actually move forward with this one. I mean, here's another thing we could do that I don't necessarily think is a good idea, but just so that it's been on the table, we've decided it's not a good thing to do. So we could. So unfortunately there's no like flags, you know, section for the fee bump transaction, but we could kind of add a new type of fee bump transaction where there's a flags field and sort of you know one of these flags could control, could say like, basically, like this is very important, like I'm willing to pay my full, I'd rather not have a discount if it, like you know, risks my transaction having like less chance of getting in. So then at least somebody who, like their transaction is very important, they say like: hey, don't exclude me just because you're giving me a discount. I don't even want that discount. Yeah, I don't. think we would, of all the strategies we talked so far, like we would basically have like those groups, right, and then within those

[33:00] Groups you may or may not get a discount, but even if you do get the discount, you, have to be the highest bidder. So I think, if you bid more than everybody else- like you know what I'm saying- we'll get in right, not necessarily because you might not be in the winning nomination value, right, I'm concerned what I mean. A validator, right, that's going to craft a set right with all those groups. If you bid more than everybody else, you will be included, not if you're yeah, sorry, you'll be included in the nomination value but it may get beaten out by another nomination value. Oh, in practice we normally only have like one nomination value right or two. Yeah, that's very small, yeah, but that's. But now we're starting to place, you know it, increase the amount of trust in you know, like that's not what SCP is designed to take to guarantee right. So, yeah, I mean at this point what you're saying, like, well, the other value- right, that's from a different validator- would also not

[34:00] Include that transaction. That is the highest bidder, right. So that's why I'm imagining, I think this is a different problem- that you're you have no a. That's the same problem. The problem is that somebody keeps nominating a block that has more spam transactions and therefore collects higher fees, and that value kind of ends up beating out a value with more useful transactions. But I think this is more of a like, why would other validator, if there are more spam values, right, like this high fee transaction would be nominated by this other validator, unless they are actually decided that they would no. But I mean, like SCP is byzantine, fault, tolerant, right, we're supposed to assume that, like, some fraction of validators could be misbehaving? Yeah, of course, yeah, and in this case, and so I want to make sure that doesn't like severely impact the,

[35:00] You know, the, what we consider the good transactions. Can I clarify here: is this not solved by switching the second criteria to the total feedback? Oh, sorry, if we're just gonna do that, then that's fine. Yeah, that's right, that's what I said earlier. Oh okay, sorry, I thought you were gonna consider it, but you weren't. So that's why I was saying: here's another thing that you do: yeah, okay, sure, I could still imagine the utility of like no, seriously, I really mean to pay this bid flag. I just wouldn't support that today. But if somebody were to come around and be like no, like seriously, I have a real reason why I really want to pay these ski bids and waste my money. Maybe we, maybe there is a real reason for that- I don't know it yet, but like that wouldn't be a hard change to make. Like you were describing about adding a flags thing to the transaction, to the debunk transaction, right, okay, so I guess that's another thing is switching that second value to be keep it instead of what it is now. Okay,

[36:00] Yeah, okay, cool, I feel like that's in progress and now I think it's time to move on to speed x. So the next CAP for today's discussion is [CAP-44](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0044.md): speed x configuration. Speed x is a new design for a fully on chain decentralized exchange that can scale to an arbitrarily high transaction throughput. It's implementing it is something we, SDF, mentioned in our 2022 roadmap. It's something that's important to sort of the future scalability of Stellar. It's going to take a few CAPs to implement speedx, and this first one is really just about setting up validators so they can configure speed x. But rather than me trying to explain that I will pass it, I guess, to John or maybe to jeff, who wants to sort of give an overview of what this CAP does. I'll pass it to John and John. If you want to pass it to jeff up to you. I'm gonna pass it straight to jeff. And what I was thinking is jeff could open with just like a quick pitch on what.

[37:00] Like what speed x is, why speed x, and then maybe I can go and kind of give a. quick pitch on like why we're breaking up the CAPs into a bunch of different things, like why we're going section by section and kind of what this one is about. And jeff wrote all the drafts of all of this stuff in the summer and we've just been cleaning it up and kind of like trying to like make it as physically simple as possible, but all of these ideas are really jeff's ideas, so thanks, yeah, so speedx is a design for trading, settling a bunch of trades in batches instead of serially, and the reason for this is, on the one hand, if you settle trades in batches, you can parallelize the operational exchange so you can scale to many transactions per second and also by trading in batches you get the market becomes more fair and you get better liquidity between assets that aren't very traded very much, and you cut down on a lot of the common front running attacks or the arbitrage opportunities that

[38:00] We've been talking about, that clog up the network as if. As for this particular CAP, speedx needs a certain amount of configuration data. In particular, the big thing is, one, control parameters on how you actually run the batch computation and two, a way of choosing which assets are traded in the batch and which ones aren't, and so what this CAP does is take a very like the simplest approach towards deciding which assets in the batch, namely, there's some set list and then the validators can upgrade that with some kind of upgrade protocol, but you can imagine the future doing some other way of choosing which assets are traded in the patch. And it's not quite clear exactly which kinds of configuration data we need in the CAP at this point. What actually we need will probably depend on what we finalize for other speed x related CAPs, but this is, we'll at least need the data that we have here

[39:00] And just to just give a little information about, like, why we're breaking this down into a bunch of small pieces. So I kind of learned a lesson after we did the liquidity pools work where, like, we produced this like 1500 line CAP and it was just like really unwieldy and people never knew what parts of things we were talking about and what are the interactions between things and stuff like that. So this time- different concept, let's break. Basically, let's break this problem into all the logical parts like this: the configuration of the upgrades. This is like the technical pricing algorithm that we're going to use. This is how we're going to get the trade, and those are kind of like the four verticals that I'm seeing, and so this one came out first because it was short and kind of easy, and like it was very well established, like what the space of options were. Pricing will come out in the next few days and then we'll keep going. But I think jeff already gave a good overview of what's kind of in this particular proposal. So, yeah, and this one contains actually the most controversial part of the speed. Example is it's actually related to what

[40:00] We just talked about with validators being able to do things that maybe they shouldn't have control of. So maybe we should start right on to that particular topic. Yeah, just quick question: like so, when they're determining this list of assets, this is an extra protocol, just like decision that they're somehow making. Like they're saying, hey guys, I think these are the assets that we should add, and then they all. I sort of imagine there's like a listing process or something where you know, like we have this whole set of upgrades for like changing the transaction, set size or whatever. So I'd sort of imagine we'd do a similar kind of mechanism there, but like, whatever we do, it would be kind of outside the scope of the protocol. You know, whatever the ecosystem wants to do is their system. That's what they should do. I my hope is that this list isn't changing like every second. That would be really unnecessary. Like so it should be the type of thing that people can coordinate pretty easily on

[41:00] Like a day to day basis. But the real question I have here is kind of like, are people going to be upset about handing this kind of power to validators? I mean it's. very much like, not in the kind of. It's that we currently explore, at least before [CAP-42](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0042.md). It's very much the same kind of power of like hey, like, these assets live in this privileged world where they have access to this feature and these other assets. Don't do people feel bad about this? Because I have something? I mean personally the way go ahead. I mean, I guess the thing that I would, like to at least consider, is that, instead of one magically privileged speed x you could, at least theoretically, have room for like multiple speed x's where asset issuers could, you know, by default be included in any speed x but could also like say like which speed x they want to be included in, so they could kind of revoke permission for being in a particular speed x. So if

[42:00] You did this, then this is seems like a. more Stellar way of doing things, because then in practice, like all the assets anyone would care about will probably all want to be in a speed x together. But if some another bunch of random assets wanted to be in a speed x together, like you know, importance is in the eye the beholder. No one's to no one is to say that like assets a, b and c are less important than assets x, y and z. It's just that you know x, y and z are happy together and a, b and x, y and z are happy together and a, b and c are happy together. This is an interesting concept, not something that actually came up in the earlier discussion at all. So already I'm intrigued. I guess my first kind of like reactionary question to this is, like the main motivation for breaking out like a speedex list is just computational concerns. So how does that square with having multiple lists or having to like? You could even imagine a world where, like everybody's

[43:00] Like, okay, well you know, like there's this one list which has this preferred status, but like let's put everything else into this other list. Well, because the problem is that the list themselves would have to be capped. Right, because there's a super linear stuff in the kind of number of assets. Right and so if you can, you know it's cheaper to compute two small, two speed x's with like 100 assets than one speed x with 200 assets, isn't it? Certain parts of it at least, I haven't figured out how to scale beyond some fixed quantity of assets effectively. Which parts are those? So the price computation, the number of rounds that you need seems to like increases somewhat as the number of assets goes up and but the big limitation I think is the linear programming follow up phase.

[44:00] The hand rolled the hand world solver that I wrote for Stellar over the summer, was turns out doesn't work very well at all. Past like 20 assets. If you take an office shelf solver- I was getting it to 50 to 80. And then a custom hand rolled solver that I had all sorts of you know, sparse optimizations and everything in it. I could push that to 100, but the performance of that really starts to increase very non nonlinearly after a while. I don't. I wasn't able to push it past like a 100 ish. Ish so I think some other technique might be needed there. What's the incentive for an asset issuer wanting their asset to be in a speedex list? You get basically more liquid trading and or like better usability on the asset,

[45:00] Going on what David was saying. So one of the things that comes up in speed x is like in the speedx pricing CAP that I'm still in the middle of working on, based on jeff's, but we- spoiler alert- we agreed on this kind of scheme where basically, like in the pre nomination phase, validators nominate a set of prices and then, in order to see if those prices are like relatively sane- you know, we also, like, have all the validators do like the standardized computation using the solver parameters in the config to try to produce a second thing- and then we choose whichever price to seem like they're better. That second phase requires that linear program that jeff was talking about, which he doesn't know how to scale up very high.

[46:00] But if we just said like hey, like we have this one speed x, that we're gonna do both phases, for we'll do the pre nomination and that's how we'll try to produce a really good, reliable set of prices, and then for the second phase- or sorry, for all the other speedx lists, you have to both present the prices and the network flow that's already rounded so we don't need to do the linear program, then we avoid that scaling issue, and then maybe we can do more we don't do the internal calculation, we just accept those prices. Maybe they're good, maybe they're bad, and that's the cost of not being in that main list. But that is a lot fairer than saying like: you're either in the list, you're not, nonetheless. I mean, somebody has to do the linear program. Either way though, right, sure, but I guess my argument would be: if you can't produce a solution, you won't trade right. But it's not necessarily people who want to trade who are

[47:00] Producing the solution. It's the people who have access to the orderbook because they're, you know, on, you know, validators in the network. But, like you could very much like- bear with me for crazy ideas, you could very much imagine running. You know like, hey, like you know, we're the people who like want the speed x to happen. We run this web service. All your validator has to do is hit this web service and we'll spit out the answer for you so that you don't have to actually do the computation. Well, oh, but the point is now it's not a sorry, but is that the valid or the end user is doing? Is it like, is there's a special operation type which is like speedx solution, or it's like right. So the point is like you were disempowering the end users. If we're saying like now you're kind of like, you're reliant on, like whatever solver your validator happens to be willing or not willing to use, but the flip side of it is like there's 90. When I looked at this, when we were working on this in the

[48:00] Early winter, there were 89 000 and something assets on Stellar. Even if we broke it into 100 groups of 90 to standard jeff's 100 threshold, we couldn't do all of that in the same way, like we couldn't give everybody the same level of attention. There's not enough time to do it Well, I think you just have to price it properly, right? I mean that you know we could have, you know, a super linear- you know we- reserve fee for, like putting your asset into a speed x or something, right. It could like increase with the number of assets and speed exits. I think you know I'd rather like take the actual costs we have and reflect those back onto users and say, like this is what it's going to cost to network, so like you have to be willing to pay for it, rather than having validators make value judgments about, like which assets are good and which are bad. I'm sorry you're so willing to have that it would cost. The issuer of the asset would be the one that would bear the cost of being listed into a speedx list. Not the cost, but the reserve

[49:00] Requirement. The reserve requirement potentially- and again I'm not saying that, like you know, I do think that there are kind of spam assets in their assets that everybody cares about. But I think that the va that we should kind of, instead of just magically decree the value of an asset, we should have existing asset issuers say, hey, I actually value having a market between my asset and this other asset, right, and that's the sense in which maybe you could end up with multiple speed x's, like maybe there's a weird like in game currency market for like a whole bunch of, and then there's like an actual real currency of like stablecoins or something. And you know, maybe those two markets don't need to inter operate or they only operate on the native asset or something. And then the validators aren't don't. We don't have to be in a position of saying, like actually we think like euros are more important than like game credits or whatever which like we might, but like why should we be making that value decision?

[50:00] So we could say, like each speed x batch has like an ID or something, and it's not the validators that choose what's in that, it's the asset issuers can like propose to put their asset into a batch and then if you put your asset into a batch, then all the other asset issues were there, like say yes or no, or they have a vote or something, and basically the asset issuers are the ones choosing who's in the batch and not the validators. Yes, I mean that is one way to do it. is sort of like another way to do. It is sort of like each person who joins the batch, it gets like exponentially more expensive and you need an exponentially larger like reserve. So it's like you know you can sort of out, or maybe you can outbid whoever it's, whoever has like the largest reserves who gets in, or some combination of the above. I feel like we probably want some amount of input. I mean, what are the signals that are sort of like in some sense egalitarian, at least kind of, you know, like a

[51:00] Objectively egalitarian, but like are potentially useful? So one signal is like: does an asset issuer value a market between his or her asset and some other asset. And another signal is: like: what is my? How much? How many native assets do I have? You know what do I meet? A high reserve requirement and so like if we could sort of factor these two things in. So the reserve requirement is an absolute thing and the whether one asset thinks another one is interesting is a relative thing. I think you still need some kind of veto power from the people who are already there. It's just so well, that's what I mean. That's the first one. That's the relative thing that I'm talking about. I'm saying, like what I mean, maybe it's a video or maybe it's a preference. I'm going to avoid something that Tomer just also shared on slack, which is a concern with, like creating too much complexity around these, around how these configurations work that make it really hard for people to interact with Stellar, just because there's too many.

[52:00] It's just like issuers don't have to do this kind of thing anywhere else. Adding a lot of complexity makes Stellar just less appealing for use. It's something I think that is worth considering in this design. Yeah, so I mean, and part of the way we can get around, that is by having, like, really sensible defaults, right, but I feel like we should, we at least need some of these. We need to like work through some of these designs before we decide that they're like too comp, complex, or before we give up like this is just there's too much like I just don't want validators picking winners and losers. You know, if we can at all avoid it, so we need to. If we can at all avoid it, so we need to either avoid that or convince ourselves that there's no way to avoid it. And I just don't think we're quite there yet. And like this, maybe different related questions. So like, what was the reason to

[53:00] Like if you had a limit, a limit, right? So that's what you're like. The only thing really there that you need for sure is the maximum number of assets involved in this fedex run. So then, what was what is? Was there any reason for not just using fees as a way to force people to just? Like you know, you basically only keep the top k, top k assets right that are involving speed x, and that is defined by fees. The reason that doesn't work is that you can imagine that there are time, like you can imagine, for example, like a us government bond- take that it's like one of the most liquid markets for anything in the world, probably like on the same par as like other, at like other very ultra liquid things, like usd euro, but like those are like some ultra liquid things. But like I want to focus

[54:00] Liquid things. But like I want to focus on the bonds because the prices don't change all the time. Like prices change but like if you look and then you look again one second later, you might see the exact same prices. In fact, you're probably pretty likely to see the exact same prices. In those cases there might be nobody who's willing to pay a fee to trade, but there'd probably be an incredibly liquid liquidity pool that also would be willing to trade to help settle out all the other trades that people want to make. That liquidity pool isn't bidding any fee. So do you just say like, oh sorry. Like we're not going to include the most liquid market in the world because nobody wants to trade this exact second? That seems really perverse to me, right? I think what you want is for all the other assets to say, hey, I want people to be able to like trade you know these treasuries or whatever from my asset, because that's like, it's a useful feature for users of my asset to have. Right, so that's why you need

[55:00] Some amount of kind of, you need some, you need configuration. You need some kind of input from the asset issuers. Right, we should at least consider a design that has some amount of input from asset issuers. I mean, like I still don't understand. Like you know in your example, John, yes, you have this asset that is very stable, and so on, but like, even if that asset is included in a pool, if people don't make money of you know, doing a trade against some other asset, like if the, if that trade is not that valuable, like how is such pricing going to work? No, it is valuable. It's just not valuable to the asset issuer, right? Imagine, suppose you know the fed issues a digital dollar on Stellar, right? I get it like it's more like what I was suggesting is that you, the way you pick the top 10 assets is by actually looking

[56:00] At the trades that are happening across you know the network, because you have to have this function that you know which top k, top end transactions are you keeping for that speed x round and as part of that, you can wait. Sorry, is it for a round? I thought this was a more coarse grained thing, like we just kind of. You know, every once in a while we'll configure a, you know, speed x to have a different set of assets. I mean, yeah, it should be a wrong. I mean it's there's a level of granite granularity there. I don't know like, but you know I mean if we are going to do it, find granularity, then we should do it through some kind of transaction as opposed to through, like, a protocol upgrade or whatever the upgrade part of the nomination message of value. If it's per asset issuer, it should be a yeah transaction and not an upgrade. I guess I think doing it to find grain is a little dangerous just because there's gonna be a lot of assets that are sporadically traded and so, like,

[57:00] If you try and like optimize, like I don't know, some max over the transaction set, assets are gonna be going in and out of the pool a lot and that's hard to predict for users and so usually kind of gonna get a lot of like really failed transactions, which is a bad experience. Yeah, the CAP mentions that and I totally agree, like I definitely wouldn't want a world where that set changes every ledger. That sounds like really impossible to use and everybody will just be really frustrated and hate it. Well, at this point we are out of time. For today's meeting, I guess just jeff and John. Do you sort of have some input that allows you to go move forward? I mean I know that we can bring this back to the mailing list too. Is there any like final question? That's like what should happen? What's the next step that we need to answer here? Are we ready to move? You sort of have enough. I think it's pretty clear that people want us to ask other questions about what could be done. Like I have some pretty strong opinions about what like why it should be

[58:00] The way I proposed, but like I'm really happy to go and explore some other things and try to write up what they would be like and like what pitfalls we might encounter. It'd probably be kind of fun honestly. So I kind of know what I need to do. Jeff, you feel the same. Yeah, I agree with that. Yeah, well, I mean, I think maybe it's like for me, it's like that. There are some questions here that I'm not sure we can answer without actually having like. How do you actually produce the denomination value, like, if I don't have a clear idea of like, how do we pick which transactions to be included? You know, like based on fees, like it sounds like some of the properties you're looking for. a more balanced like composition, as opposed to just high fees, and I'm not sure: are you talking about me or about David?

[59:00] No, about you. Yeah, John, I'm looking for whatever simple thing we can do that people can agree on, like. That's why I just gave it to the validators, because I thought we'd be able to agree on that and they could agree and figure out what is good for the network. That's what, like, when lee posted on the mailing list and about like other things we could do, and my response was basically like you'd have to convince me that a heuristic would outperform the validators who care about stewarding the network, assuming that we are actually restricting like a small number of speed x assets. In a world like David's where we have multiple sets, maybe, and we're expanding the capacity by, like breaking them up, maybe it's a different trade off but, like if we're going to look at a single block, you'd have to convince me that, like the heuristic could outperform the validators who just want to do what's good for the network. I mean the other question is: would it make sense to have two tiers? Like you, you'd have like the core speedx assets and then other assets that can be traded for any of the 20 core speedx assets. But it's not making a market between any

[01:00:00] Of the like a pair of non core assets. Shmedex has assets, shmedex and speedx. Sorry, we've exhausted Justin. He's trying to kick us out and all right, look to be continued. Laughter thanks everybody. Yeah, to be continued. Seeing a couple weeks and on the mailing list again. Everybody out there. Thanks for watching. If you're interested, join the Stellar dev mailing list to participate in the discussion as it unfolds.

</details>
