---
title: "Open Protocol Discussion"
authors: [justin-rice]
tags: [legacy]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="5pAl6LNzBi4" />

- so welcome to the stellar open protocol discussion in these discussions we discuss and plan for changes to upcoming versions of the stellar protocol um right now we're focused on project jump cannon which will bring smart contracts to stellar in addition to changes to the stellar protocol it will also lead to the creation of a new smart contracts platform and all of the discussion that we're having about this is being tracked and and you can participate in it much of it is happening here on discord in the jump cannon channel and the drum cannon dev channel there are also a series of core advancement proposals or caps that relate to changes that would enable jump cannon or the new smart contracts platform caps 46 through 55 i believe at this point they're pretty modular so each one sort of takes on an aspect of the changes that need to be made in order to bring smart contracts to stellar and we are working through those modules bit by bit and discussing sort of the segments necessary and the changes necessary to allow those segments to actually come to life at some point all of the work that we do will go through the normal process in other words caps are sort of uh put up in this github repository that's linked to in the show notes they're discussed here they're discussed on a mailing list and they're discussed in discord and uh after after they sort of reach a point where they are stable they move from being a draft into a formal acceptance period finally they're accepted and implemented in a version of the stellar protocol and before that version of the stellar protocol goes live validators actually vote to accept it um now we are still fairly well we've actually made a lot of progress in the in sort of the jump cannon trajectory but as of yet the caps that we have in front of us have not been accepted there's still a lot of questions and today we will dig into some of those questions um if you who are listening actually have questions you can leave them as text in the live chat channel i'll try to keep an eye on that um we're certainly trying to move this discussion forward and have the substantive issues um come to light so you know we may not be able to answer all the live chat questions but we definitely will later if we can't answer them in the course of this meeting so i think everyone is here and i think we are ready to kick off um today i know that um recently there was a new cap cap 55 fee model in smart contracts um i don't know if it just hit the mailing list yesterday there were a few comments that came in i guess i'm going to start by asking you uh nicola if we're ready to discuss that if that's where we should start maybe yeah like we can maybe um [Music] like just go over like a quick overview of like what's going on in that cap and then um you know like we we don't have to go in details basically so i'm not sure like we need to need to have like a lot of uh of a pre-reading basically as part of this is that good use of time of people yeah i think that sounds good also nico if you can you know obviously a lot of these things are kind of like normal quote unquote in the world of crypto but uh some of these are a bit more contentious so i would like emphasize specifically like the contentious bits so that we can have a good old-fashioned argument all right let's see uh and uh yeah i mean so do we want to start with this uh cap or i think there were there was also like the events one that cida uh opened that is maybe a little more scope i don't know yeah i argue let's start with the siddharth one because it's less uh contentious and we'll probably be arguing this okay great so we'll start there we'll end up at cat 55 uh sit down do you want to yeah i don't know if everyone had time to read the cap document i actually made a fix of this morning but i can give a quick uh overview of it and you can uh we can discuss after that so i made uh i added this this change on to cap 51 which is the host functions cap i just added uh the ability for contracts to log data so i added a contract logs back to the transaction meta um and as a part of this change i we also moved the transaction result into the meta as well both contracts so both contract logs and transact transaction results are hashed and a hash of these hashes are stored in the transaction result pair which is uh this is how you would cryptographically verify them and one change i haven't made to the document yet but we talked about yesterday was that uh we're gonna add another contract log type uh where the logs are only emitted if there's an error um so that's a very high high level overview are there any questions i guess like those logs they are like uh logs that are like the equivalent to events in ethereum uh yeah yeah so they well so the way it would work is uh you know that the contracts allow whatever they want since it gets sent to core or write some transaction meta and horizon can serve them up in any way you want so i i was i would imagine that you know if you want to listen to a specific event uh horizon would provide that ability allowing you to you know write applications that would hook on to specific events so one thing that i want to point out here is that it basically means that in order to sift through um data coming in from uh horizon as as an ingestor you need to basically read everything which may become a lot as as the network grows in capacity um ethereum has this concept of this uh uh bloom filter uh that's included in every ledger header so that you can actually uh get a like a strong indicator whether or not the smart contract that you're interested in or the the account that you're interested in is actually included uh or has emitted events in that specific block um should we consider doing something similar uh yeah i i don't think uh i can't think off top of my head why we wouldn't do um optimize like that that's that area i'm sorry i can look into that i got i don't think i i don't see i don't think i see anything uh wrong with that well actually i think it's kind of uh maybe like premature optimization type of situation like there are probably better ways to do it than doing this kind of arbitrary broom filter thing uh uh like you know i can imagine for example like we have just made a stream uh you could have horizon telco which which filters it wants to apply instead of kind of doing it after the fact and then the mirror would be a subset of the meta like maybe like you're not interested in ledger changes let's say maybe you're not interested in classic transactions you know like all those things well you're you're assuming a horizon here and i think that's like part well there's always a consumer right of but if you want something that's like an application specific consumer um which you do see in other ecosystems quite a bit um you know if i'm developing a dap and i want to have a you know like a stream coming in of my of my specific information why should i run a full-blown horizon rather than some sort of a light client that could just uh you know share logs that are specifically relevant to me and that's again like a subscriber i don't see why this is like part of this gap well i'm not saying i'm not saying it's part of this cap but i am saying like is is there a place in the in the protocol uh to to optimize for these use cases because we actually want more people to run nodes and uh what we're doing here right now is that we're like really tying them down to like the the horizon model which is consume everything in just everything no like horizon doesn't force you into uh consuming everything you're you're getting the entire meta and then you filter it yes uh like is the is the amount of meta being produced going to be a bottleneck in the you know even in the middle you know medium term i don't think so like you know xdr is fairly efficient like i would like to see the actual performance problems before picking arbitrary type of filtering technology because i don't know what the use cases are like you're saying they are events yeah sure then you just keep advance it's a very small subset actually of the of the media stream is anything that we're adding here in this cap prevent us from adding a bloom filter or other sort of strategies in the future i don't i don't think so but yeah you can do any kind of filtering i mean i think the the the place where i can see having an actual bottleneck in the future is the actual size of the meta may get too large for like uh if you want to run a light uh lighter node but then we are getting into custom logic in core to kind of you know filter that somehow and i think the the best way to do it is actually like when you're producing it instead of trying to do it after the fact like with like uh like with the bloom filters something that i don't see here when it comes to filtering is any way to filter beyond the contract so like presumably the contract being out of filter up by the contract um is there because that would probably be in the transaction matter but um if a contract wants to emit a whole lot of different logs how would an application filter on those specifically or or is that just too granular to micro well at the moment uh you know the body is an sc val so if you wanted to do that you would add the filtering in there um but you know i think we would discuss this okay like maybe that's not reasonable uh we should add a higher level filters above the sc bell something we can consider yeah like that right now this structure would make sense here yeah like originally i i i thought we would do something like right now you have this uh block type right uh system or contract info and basically if it's a i mean actually for both of them you probably want to have like an actual event name right like which is like a a short symbol of source right yeah i i think that makes sense i would i would i would honestly look at sort of what the subscription patterns that you see in other smart contracting platforms are because they they have explored this this space fairly extensively and i think it's what a lot of the sdks really lean on like if you're writing a dapp it's fairly common for for it to to latch onto a bunch of subscriptions so like however however they're normally doing it we kind of want to support those patterns is there more anyone else have thoughts questions suggestions is it sort of clear what the next move is for you here siddharth you move on to catholic actually like there is uh something yeah that i just thought about that i think you know from what uh i think graydon was asking in terms of use cases like like are there expectations for example that uh and that's related to this filtering question um that you want to have proofs of events that do not happen so like positive proofs are easy like uh like like with with the proposal you have like uh you know basically like uh you can prove that a given ledger had a specific event inside a space you know from a generated by a specific contract what if you want to prove the negative that is that a specific event yeah was not emitted in a ledger is that like the type of things that people try to do in other systems so yeah that's a question for yeah to look into the what happened the use cases yeah 55.
- let's move on to it all right yeah yeah so 55 is uh basically like uh like trying to layer fees on top of the various uh like resource uh uh metering uh that uh started to get introduced in the system so it's kind of a problem is it's a little bit ahead of that because we didn't actually finish all this uh like i think we have a the beginning of like gas metering for example um in cap 46 but there are like other things that are not covered yet um so yeah this is uh so the test disclaimer this gap has a bunch of uh open-ended things um let's see and um yeah so so like they are where the cap uh is uh covered there are like several aspects kind of uh maybe like more important to discuss uh i think um there's the the first one around the uh classification of resources and having market dynamics uh based on those resource types so this is an area where um if you look at uh other blockchains it's actually a mix of things like some systems um like historically started with just like ethereum um just like gas as being the uh yes i i i'm also a gas in the context in this context meaning um the kind of this uh metric right that allows to to to kind of uh count the cost of a um to to execute a transaction and cost here is kind of a pretty loose in terms of definition um uh like mostly computation but when you do like when you load like a like a ledger entry uh like when you access ledger you also pay for for gas um and then yeah like yeah and then so you have like gas cost which is uh this aggregate metric basically of multiple resource types and then uh more recently in ethereum there have been discussions around uh other types of resources that are kind of interesting um such as uh bandwidth uh and uh and any other like basically you have a kind of a funny crossroad there that is do i want to have uh a market for uh for each of those resources or do i want to kind of generate like a composite market for those so like the you have like uh i think in polkadot what they do is uh they put a the aggregation with utilities with a polynomial uh function so basically take all those resource types and then you assign them um a weight actually it's not even i think they are linear it's a linear thing and then yeah you combine all those things and you get with your synthetic i don't remember how they call it weight i think in in over there but like um yeah like that's that's a way to kind of compute this aggregate gas so the challenge um so to talking about challenges that comes with those aggregate uh models is that it's actually very hard to discover price of things like uh an example is uh um if you take a transaction that does a lot of i o and very little compute that is competing with a transaction that does very little i o but a lot of compute like uh with those aggregate functions uh if try to pay like 10 times more for example bid more right for one transaction um you don't know if you're signaling that you're that your i o is is what you want to prioritize or if it's your compute that you want to prioritize um so yeah so it basically causes the overall prices to have like this uncertainty in terms of like what should i bid so that's kind of one of the the problems with those kind of aggregate metrics um so with that said uh with john cannon like one of the things that we are doing is uh we have a very uh clean uh separation between the different resource types so io for example when we read or write the ledger those are done basically outside of the main execution like you can think of before applying a transaction before executing a contract we load all the ledger entries that this contract needs and then it does its thing and then at the end uh it produces potentially uh side effects uh that will be applied as like a post step that's kind of a logically the way to we can you can think about this and um the opportunity here for us is that because we have those uh kind of completely separate um we can actually dis um and and we also do it because of performance reasons uh for parallelism but like because of that we can um um we can actually express those markets uh like uh separately and we can therefore i i think um have like cheaper fees overall because you can price things uh properly so you don't have to do like to kind of articulate inflate for example inflate the price of of reading data from the ledger because compute happens to be expensive uh which kind of would happen with the aggregate model so in the proposal what i'm doing is actually i have like three categories three really brackets for fees so one is for gas which is the compute time exactly you can think of it as execution time in in our model really because like i said earlier we we have a full separation between i o and and uh and execution uh so this one you can bid [Music] second market that i have in the proposal is for reading and writing to the ledger so here there is actually a competition for uh there's like there are so many uh there's like a you have like constraints right in terms of a number like the bandwidth to the disk uh subsystem both in reason rights um so um because of that you have to have like a market for that and it is separate right now um and it is separate also because there is a interesting fee model for rights that i'm going to talk about uh later um let's see and the third category um is actually something that is not a market it's not really a market there are dynamic fees for what i would consider like commodity on the network so things like um like uh producing meta or data that ends up being stored in archives those do not there's no reason to have like really competition between uh transactions uh instead we have like limits per transaction like basically we say you can only produce i don't know like a 500k or something of meta right for a transaction and then that's your limit and then two transactions are actually not competing uh you know against each other so there is no need to uh you cannot have a market dynamics there so uh there are actually a few of those um of those resource types and because there are there is no market you can actually aggregate them so they end up in one big bucket of like like deterministic fees basically based on the current state of the ledger plus plus yeah like the actual transaction um so those are like the three three categories that we have in this proposal any question at this point on this i've been going back and forth actually on this uh like a should we or not separate piece we could go with one like i said one market but i think uh it pushes price quite a bit too much for like cheap cheaper whichever resource will be cheaper which is hard to predict do we anticipate that this will be difficult for users to reason about to like understand these different types of fees and to think about how to set them so yes and no i think that's actually one of the things that's kind of interesting is that when um we have uh already in the system a strong dependency on a pre-flight mechanism so like before submitting most transactions to the network uh like they will have to go through a pre-flight endpoint the pre-flight is the the thing that basically will um allow people to compute gas for example to estimate gas for for transaction uh in addition to that um let's see uh in addition to that yeah we have um uh like i was saying like uh certain fees that are like that more like dynamic because they are based on the uh current ledger uh like for example like a bunch of those things that i was saying are like the price of storage in archive this is uh voted uh by the or determined by the validators so at before submitting a transaction [Music] you have to know basically what those parameters are um and uh yeah so as part of the pre-flight endpoint you basically get a an estimate for your the minimum fee for for those categories well in the case of the non-market-based resources the the minimum fee is basically equal to your or very likely to be equal to to what you need in the case of um well you do have markets uh it's more like today where you have to decide how much do you want to over bid based on that because the minimum fee doesn't necessarily translate to uh to what the market is is willing to pay so you have to look more at historical data but this is like something we can uh yeah that uh that like endpoint scan like a horizon can uh can uh can expose right um and having yeah having them tracked as separate resources in terms of historical price uh allows you have actually something a little more stable i would imagine than if it was like an aggregate i think the yeah the complexity from multiple markets uh comes from um i mean one of the implications is actually when we construct one and say we validators construct a transaction set it's going to be a kind of a multi-dimensional nexa problem which is not great but that's the but that algorithm would not be part of the protocol it's more like it's uh if you have like five seconds to produce a block here that's uh there's so much compute you can span in assembling the perfect transaction set yeah to answer original question justin at the end of the day uh the you know the wallets should have an easy way to present an uh like an estimated cost in uh an xlm currency that they can understand and they can tell the user hey like um you know this is how much more you can uh you can propose for that like they don't need to actually understand the mechanics of how this works yeah it's true that when you over beat right like you're doing for example you say i want to to spend like 10 more than whatever happened in last few lectures that 10 percent you can put it i mean i imagine pretty safely across like those different those different resource types like the ones with markets uh because the assumption there is like the they are priced accurately but i imagine that more yeah if you want to really save like a it's hard to predict but like uh like if if there are like some of those like a gas for example becomes very expensive um yeah you don't like maybe you don't want to be as aggressive on on the other resource types um can i jump in with a couple comments yeah um so i guess so um one it's um uh i guess high level it's not clearly what this adds over uh for having like a multi-dimensional optimization problem over the the one-dimensional one um and the reason is that uh sorry i haven't thought about this all that much um but the reason is that like when when at least um in the current execution model sort of everything like everything executes um everything in one lane is going to execute sequentially right and so the main like resource that's that's truly limited in like a block is is time right and um it's it like unless there's some sort of weird um interleaving going on between like transaction executions um that's sort of the one-dimensional resource that we have to optimize anyway and so it's it's not clear to me why um like your example of like a transaction that does lots of i o versus one that does lots of compute well um both of them are going to take a lot of time if they're using a lot of one resource and so it's not clear to me that um uh at least in the current execution setup we have um that it's uh we gain by sort of allocating some um i don't know resource to like i o versus some to to compute as opposed to just like looking at the whole picture of like the total end-to-end time of the transaction um that said it it does seem like uh we it'd be good to like have some kind of like price discovery mechanism um for different resources and um certainly like you want like an overall limit perhaps on like the total number of ledger entries um and so um i know i don't think um sort of thinking off the top my head i don't think it's incompatible to have like a one-dimensional um like gas market and then like sort of price markets on each resource in the sense of um transactions could bid like you know for the the amount of resources they want to use and like the fee per resource um and then you do some like filtering step um but that's sort of thinking um very much off the top my head um i don't think we necessarily i guess high level i don't think we necessarily have to go to the full multi-dimensional operation problem that's but yeah that's possible like uh it it just looked like uh from historical kind of uh experience right like a like i o is a huge problem and um trying to model that as time is actually making it's actually a disservice in a way to the to the network because you have like very expensive uh from uh like this point of view right like something that's going to suck your your disk resources uh that now stole all your calls right on in a multi you know parallel uh execution model um like store as in you know because like i said we do all i o early on and if you're actually maxing out uh your drive then you're just stuck uh right i mean it makes sense to have um perhaps a limit on uh overall io i guess and then there's yeah the other aspect uh actually maybe if we can you know like uh i don't think we're going to necessarily like close on this multi-dimensional thing you know now but like there's the other aspect of yeah ledger size and rights uh that is actually another kind of key thing in there that i guess makes uh io a little more special also nico just to go back to to jeff's point like i understand that why i o needs to be you know priced significantly higher than uh you know the compute operations but i don't necessarily understand why it needs to have like its own market so the the pricing right that you have is the minimum price it's not the market price like when you uh market prices is uh like in the in the ideal like what is describing the cap is is trying to be closer to like the ideal situation where um you can actually construct a transaction set that's going to basically be like right at the edge in terms of the capacity that you have on your actual you know underlying hardware so like cpu and io for disk if we lose that visibility then you may actually allocate too many transactions to compute when then uh like uh you know you you don't have like basically like the the kind of natural way of of uh of having a um a transaction compete against other transactions that are paying for expensive stuff that's kind of what i'm getting to like uh like if you have like um what was it like a good example would be like uh um yeah i don't know which one of those resources would be more expensive but they are not going to be in the same order of magnitude let's say like a compute is the one more expensive at a given time so you have to pay like 10 times more right or 100 times more than the minimum fee for for compute to get to get into the ledger but your your your storage price is also kind of expensive and by bidding a hundred times you also bid a hundred times on storage and you're basically overshooting quite a bit compared to the ideal um model yeah i think i think the general point here is just that you cannot that in reality uh it's not the case that there's there's just time when uh a transaction is executing um there there are two different resources and there are different contention patterns on them and you can't trade one for the other the the the system does not actually trade one for the other like if i if i for example submit uh you know a hundred transactions every one of which is doing incredibly cpu and expensive stuff um that that doesn't saturate the i o system and there's there's still no contention on the i o system uh whereas if i submit a 100 transactions that are just doing i o and they're doing no cpu that doesn't saturate the cpu so they are really two separate resources and the point where one of them gets a limit uh and can no longer do transaction processing it doesn't represent a limit on the other and vice versa and so you you you can't trade between the two of them uh from from a market perspective sorry i'm not quite following something didn't didn't we say earlier that we were going to do like all of the sort of disk reads first and then do the executions right so that if we have a lot of disk reads then we have less time for execution and so the vice versa i i sort of understand that there's not they're not like directly tradable but they seem correlated or anti-correlated well they're they're they're different devices so like right i'm using the disk and then i'm doing the cpu right i'm not using it at the same time i mean like sure there's different offers and things but um yeah sure but the the execution characteristics of each of them are are different so you you use everyone uses the same desk and then everyone sort of farms out to multiple threats right i feel like we're talking past each other i mean yes one goes in in order of the other the two of them do get added together in order to represent the the total time but you can't trade time on one of them for time on the other that's what i'm saying i think i'm not quite following but that's okay nico are there any other networks and fee systems that uh introduce a split or that work similarly those are like the two like the yeah execution time and and uh like compute right and uh and disc are like the two big things that i isolated like the right side i figured it would probably is probably not needed so i kind of put a flat fee for the other ones like basically like uh um in a proposal i'm basically saying like we're okay with you know if you want to have like something that uh like if you have a transaction that is very important that happens to emit a lot of miller for example um but you have to get just over a bit like crazy on your compute even though you're not really that's not what it's about you have to find a way to get it prioritized um let's see are we talking about this okay like i think i kind of wanted to talk about it here is in your proposal you're talking a fair amount about state expiry yeah before we go into estate expiration which i i know is a big topic i'm still trying to to reason about like the kind of the wallet experience and user experience of having having these like multiple dimensions for for uh for gas like what is the expected behavior here for um for wallets to be clear right like tomorrow like single dimension or multi-dimension from a white point of view if you want to estimate it's the same problem right like it's like in the single uh like a cost model like like you aggregate everything into one you have to actually uh you have like a function right that just aggregates but you do have to estimate uh your your bid for each uh things so it's not uh it's actually like a funny thing that that you have except maybe the tools you have for discovering price are not as great because it's all implicit okay so so a wallet can can do a pre-flight can tell me like the expected cost i can um you know bid bit over um but how do i know how to divide that between like the the compute and the i o well like it's it's the same in you know like like i said like it it doesn't that question is not a question around multi-dimension versus single dimension because that that like if you want to multi like if you want to say like i want to pay 10 percent more for on top of the market rate right for storage let's say because that's where there is contention you have to know that that's exactly what you you have to have like you have to have the uh yeah market price for for storage if you just layer like 10 flat on everything uh you're just going to above overbid which is maybe okay right like for some people if the fees are relatively low you know what's the difference between you know uh you know half a lumen and two-thirds of aluminum or something i don't know like historically we've seen that uh like in some situations people are getting uh are bidding very high on on certain uh for certain patterns um it would be great to maybe um and forgive me if it's already in the cap but just like understanding what is the like the uh what's the expected wallet strategy or client strategy here uh when uh like in terms of user experience like what do they present to the user and what uh you know what kind of inputs do they expect from the users yeah sure okay let's talk about state exploration nico where is it well so state exploration yeah goes kind of hand to hand with the uh model that i have there for storing data on the ledger so like the in the proposal it's basically like there are two parts to it there's the how do you model a write and uh as in and so writers can be a create a ledger entry or an update um and how does it work um like how do we have like the the right price basically for the cost of storage um so in the proposal what i what i did is uh i basically used as an approximation for um for the cluster storage the bucket list size so like the uh some basically like the the ledger is organized into those uh like uh 19 buckets i think it's 19 uh and then uh um it you if you if you do a um an update or a create you basically append that to the to the very first bucket in the bucket list so that's how um basically like based on the size of the the the total size the or total size of the ledger um um i allocate like a a price function uh that kind of looks like an exponential from fall uh like basically it starts with a slow slope up to uh some number let's say you say oh like validators here kind of determining those parameters but like you can think of it as the leaders say oh yeah right now we are running on drives with uh i don't know like uh 25 gigs or 50 gigs of space right um and they're going to basically set parameters such that um they don't have to um kind of uh buy new drives uh you know like if there's too much traffic so so the price function is basically looks in this case like you have like your normal slope that goes to in like i don't know let's say you have 100 gig and it would be like i want to use maybe like the first 80 gig at a rate um that's going to be uh like a good rate but not not like overly aggressive and then the last 20 gigs i want to really slow down uh like the the growth so that's right like from 5 looks like this hockey stick type of shape right like an exponential and that's kind of the model for pricing growing the bucket list so you have that uh for that's four rights then the problem is that this is only like uh this is like um saying okay you can add to the bucket list but then uh like and and by the way like if you delete entries um eventually those get collapsed into the buckets and so the bucket is uh shrinks uh in that model a delete you still pay for or delete actually um because delete is actually adding a little bit of of data to the bucket list uh so that's like first thing to note here uh um and then uh yeah what i wanted to get here is uh as a kind of more like a desired property is that i want the price of storage for people to to kind of be the same for everybody regardless if they signed up for you know uh created an account uh like two years ago or you know in five years uh it should be uh over time same cost and um there should be no way to do like uh to have like a free ride on the on on the on the ledger right like so you shouldn't be able to have like store i don't know like nfts like jpegs whatever on chain uh you pay for for this when uh storage is cheap and then uh now you have like uh something that is cheaper than uh than even storing in aws right like that that doesn't make any sense um so this in with that said there's then a need for having some way of kind of uh resetting in a way uh the the the price of uh of storage over time and uh the mechanism that i use there is state expiration so state expiration here means that you have to basically [Music] pay for market market price of storage uh to maintain a ledger entry on on the like a live in the in the ledger if you do not pay for this uh for those uh for this for your brands basically you get uh perched that's kind of the the choice that i made in this cap there are a bunch of other ways uh that can be done uh that are actually mentioned in the recap in the other approaches um but like the the reason this kind of works with the other mechanism is that basically like if you if you set a policy for example by default you have to pay rent like a refresh every year let's say uh and then you don't pay your your your renewal after a year uh yeah your the data gets uh deleted um and yeah so there's this kind of constant churn i guess on the ledger which is kind of a new pattern and that construction is basically a way to to guarantee that everybody in the last year has has been paying basically something that is market rate how long do you expect the like the how far in the f like when i trade a letter entry how far is the maximum expiration date that i can choose or that will get chosen for me in the future this stuff isn't as far as i can tell it's not specified in the cap how that works so right so so right now the cap what it says is that it do not so you can renew indefinitely right uh like the the renewal window is determined by validators so that's why i said it's like a um like you say every year you have to pay run right and then every time you write you do an append that happen is valid uh for a year right that's good that's demo then isn't there a natural trade-off between like renewal time and um like fluctuations in this price of storage or are we expecting like this storage cost to not um increase too quickly well it depends uh like what we've seen on the on the current network is uh ledger size has been increasing actually rapidly over the last uh few months because of like some strange token activity which is not entirely uh i mean it's it's not i mean there are a combination of factors like uh one is uh yeah like just price of in crypto assets go you know uh going down uh but also like when they were still pretty high you had like an incentive to to create more crypto assets so it basically those things kind of cancel each other and the growth has been uh pretty significant so i would say like uh seeing a growth rate um that takes you to uh yeah something that will be you want the market basically to kind of get to an eco equilibrium right like where uh where like uh you do not have like uh like those weird use cases uh appearing on the network if they if they are like cheaper than right now i think on on the on the network the the the problem we have is uh yeah we are cheaper than aws fault in some situations isn't there a trade-off here between not necessarily a trade-off but isn't there a consequence here that people will have to go and touch their data from time to time and people are procrastinators and like let's say that like the expiration date is you know when you're in the future or something uh everybody at the end of that year then has to go and touch all their data and there's gonna be a huge log jam to get it done well that's there would be a large jump if everybody creates their stuff at the same time but you would that's not the case it's going to be you know like basically like the thing that expires in a year is the whatever happened today right like at the given date right but i mean like imagine that today you have a day with like a lot of activity like you can look back historically of stellar's history and there are periods when there were like lots of token creations and stuff you know there are days when there's hundreds of thousands of blood draw entries created um and then abundant well many of them are abandoned but like you could imagine a world where they're not all abandoned right and then what happens a year in the future well nothing well people are incentivized to come back some time between here and then i i don't i don't see this as any worse than the fact that we have to handle load spikes in general yeah i mean we have to handle load spikes and let's bikes may get replicated but yeah like what um so what i sketched uh or what we sketched actually in the cup which is uh you know just a more of a strowman type of thing because i'm sure we can do better than that is uh is we actually are um kind of uh ensuring that uh you do not have like giant spikes so like uh it like i think the spike would be not be because of situations don't like what you're what you mentioned because actually activity from today uh if you have like a comp a a a um a linear like a translation right like an actual just shapes right like all this activity gets translated exactly a year from now i you don't have a problem i think the it's just like additional uh cost of running a validator right like it's uh let's you say okay i need to when i set my limits right the number of rights i actually have to think about well actually my capacity in rights is half of of what i can add to the ledger because i also need to delete right but what can happen is uh more of a like um if we have different expiration times which i actually kind of briefly talked about there is that if you have different expiration times you can have actually different dates that end up expiring at the same date and uh and that's for that for those type of situations you have to have an algorithm that kind of uh smooths things out and that doesn't actually cause the system to kind of create a gigantic spike you know at a specific dates so just we don't have a lot of time but i just want to ask uh what like the biggest question i think that there is like what is the what's the expected behavior here like you know these ledger entries are representing financial instruments um let's say assets just for simplicity even though we have like a standard asset contract and i'm uh you know paddling on shares of something um is what is the expectations or what is what's expectations there's like am i supposed to like once a year like come and touch this is like the operator of this financial instrument supposed to do that for me um if you know if you look at uh you know various common immutable contracts like uni swap you know and so and and i'm holding on like these uni tokens what's the expectation here like who's going to touch these for me right so in the in the in the cap i i actually let this kind of flexible like uh there's a when you there's actually a special host function to that you can call that is basically a rewrite equivalent to like a rewrite uh ledger entry um so that you refresh the that that expiration time anybody can do that all right like there's no it's not i understand that anyone can do that but who is who do you expect to do that well it depends on the type of users right like uh like power users probably don't want to do it themselves uh like other situations you know if you're if you're like you said like this very passive type of person uh maybe you should pay somebody to maintain your stuff if that's what you really want uh in other situations i suspect uh if people are not active they probably should just be using centralized uh infrastructure like you know contracts or whatever that are a little more centralized i think trump's point is that there's a free rider problem here like imagine that all the people in this room are using a single contract right like which one of us can touch me all of us have an incentive to wait until the last second and play chicken and hope that somebody else oh like yeah for a short contract i think that uh yeah well it's if it's a shared goodness uh like uh just i that's not what actually what i asked john i assumed that uh like each of us will have like our own ledger entry within that contract so maybe did i not understand correctly i think i think i'm concerned about people's money vanishing into thin air which is completely reasonable these are balances and we're just going to delete them that's not that's not super great i recall there being a proposal to like uh have um but what happened to this um like when you have a ledger engine deleted it like gets um dumped into like some kind of merkle try and you store the hat root of that try and then like when i want to bring it back i can like bring in a proof that this is what the state was right so this is actually in the appendix yeah the alternative section so this there is actually a very detailed uh proposal in ethereum foreign v2 uh about this um so the complications from this archive approach is uh is when you uh want to so like uh restoring is actually yeah like a trivial like i said like you know you have like a maybe a way to do like a to just to basically store that that entry in in inside a merkle tri of source right um and then you just need to provide the proof for that uh the complexity comes from uh when you want to create an entry because you have to prove that that entry doesn't exist in historical data like that was actually archived and that gets really nasty very fast right right sure but like that that like not wanting to do that doesn't address creating this question like what do you do if your money gets deleted it's it's uh it's an event like uh like you know if it's a uh like with an issue it's like today if you're on the seller network if you're sending back to the issuer you know you basically burn it you can ask the issuer like hey sorry i didn't mean to do that but but i didn't burn it but like what if you're like what if it's not like you know an off-chain issuer that you can appeal to like what if your unit swap lp tokens get get deleted what do you do it's tough yeah uh like uh you know these those are the rules of the network wait but that's not that's not a great solution long term right like we're gonna have a lot of people consider the alternative right which is infinite growth of a ledger with infinite price which one do you prefer i mean like objectively like if somebody had a million dollars of like uniform lp shares get liquidated it would have been better to pay for a million dollars of storage so that one person yes but like uh what about everybody else and that person with a million dollars like if they have that it's kind of like key management like you you have procedures to make sure that you don't lose your million dollars it is it is currently the case that people with million dollar balances can in fact uh lose them because they can lose their keys so there's there is there is something to appeal to here like it is it is actually possible for you to lose money just by misusing the system but what about the other end of the spectrum though somebody who doesn't have a million dollars they have a small amount of their balance and they're just constantly eating that up by paying these fees to keep their balance alive i mean it sort of reminds me of like bank accounts where you're like bank accounts just disappearing because you're paying all these fees yeah they do eventually disappear if you if you put five dollars in a bank account and then wait for 20 years it'll go away i mean this just points to yeah you're not stirring your your balance in the right place like this is shared infrastructure like if you don't if you don't use it you lose it but nikki you can't just like ignore the entire industry that we're in and you know i'm not pretending that this is not a problem like people are overlooking this problem but um you know it's i think it will be really difficult to bring people into stellar telling them oh this is the way it works in selena right so it's a fader no it's not selena doesn't actually do any of this right now they have rant no they don't they literally don't charge it right now it's a to be done in the future feature that no one wants so they're never actually going to get around to doing it they have infinite ledger in memory they they allow they charge you money to make to allocate space but they never actually reclaim it there's no there's no active garbage collection process so we're over time at this point and i think that means that we have to stop i mean i know that this is an interesting conversation and there's seems like there's a lot to say about the concept of expiration um but i think we'll push it to next week's meeting and hopefully have some of this discussion on the stellar dev mailing list and here also in discord in the various john cannon channels so if anyone is watching and has thoughts about that feel free to join the stellar dev mailing list or to chime in on the discord here um we'll continue to share work and ideas and conversations debates as they happen and we will see you here again soon thanks everybody

The meeting opened with Siddharth's addendum to CAP-0051: contracts now get a formal logging primitive. Logs will be hashed into the transaction meta (alongside the result) and there is even a proposal for error-only logs so wallets can avoid noise. That sparked a debate about how Horizon or light clients should index events-Bloom filters, opt-in filtering, or layered RPC streams.

Nicolas then debuted CAP-0055, the Soroban fee model. Compute, ledger reads, writes, and storage rent each receive their own buckets, and validators will publish price brackets so wallets can estimate the minimum bid for a given resource profile. The call surfaced questions about throttling, congestion control, and how to keep the minimum fee from being gamed.

Key discussion threads:

- Adding contract logs to CAP-0051's host functions, hashing them into transaction meta, and deciding when error-only logs make sense.
- Strategies for filtering events (Bloom filters vs Horizon streams) so app developers are not forced to ingest the entire meta stream.
- Breaking Soroban fees into compute, memory, footprint, and rent components while still exposing a simple interface to wallets.
