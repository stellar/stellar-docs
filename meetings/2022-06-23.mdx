---
title: "Open Protocol Discussion"
authors: [justin-rice]
tags: [legacy]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="5pAl6LNzBi4" />

- so welcome to the stellar open protocol discussion in these discussions we discuss and plan for changes to upcoming versions of the stellar protocol um right now we're focused on project jump cannon which will bring smart contracts to stellar in addition to changes to the stellar protocol it will also lead to the creation of a new smart contracts platform and all of the discussion that we're having about this is being tracked and and you can participate in it much of it is happening here on discord in the jump cannon channel and the drum cannon dev channel there are also a series of core advancement proposals or caps that relate to changes that would enable jump cannon or the new smart contracts platform caps 46 through 55 i believe at this point they're pretty modular so each one sort of takes on an aspect of the changes that need to be made in order to bring smart contracts to stellar and we are working through those modules bit by bit and discussing sort of the segments necessary and the changes necessary to allow those segments to actually come to life at some point all of the work that we do will go through the normal process in other words caps are sort of uh put up in this github repository that's linked to in the show notes they're discussed here they're discussed on a mailing list and they're discussed in discord and uh after after they sort of reach a point where they are stable they move from being a draft into a formal acceptance period finally they're accepted and implemented in a version of the stellar protocol and before that version of the stellar protocol goes live validators actually vote to accept it um now we are still fairly well we've actually made a lot of progress in the in sort of the jump cannon trajectory but as of yet the caps that we have in front of us have not been accepted there's still a lot of questions and today we will dig into some of those questions um if you who are listening actually have questions you can leave them as text in the live chat channel i'll try to keep an eye on that um we're certainly trying to move this discussion forward and have the substantive issues um come to light so you know we may not be able to answer all the live chat questions but we definitely will later if we can't answer them in the course of this meeting so i think everyone is here and i think we are ready to kick off um today i know that um recently there was a new cap cap 55 fee model in smart contracts um i don't know if it just hit the mailing list yesterday there were a few comments that came in i guess i'm going to start by asking you uh nicola if we're ready to discuss that if that's where we should start maybe yeah like we can maybe um like just go over like a quick overview of like what's going on in that cap and then um you know like we we don't have to go in details basically so i'm not sure like we need to need to have like a lot of uh of a pre-reading basically as part of this is that good use of time of people yeah i think that sounds good also nico if you can you know obviously a lot of these things are kind of like normal quote unquote in the world of crypto but uh some of these are a bit more contentious so i would like emphasize specifically like the contentious bits so that we can have a good old-fashioned argument all right let's see uh and uh yeah i mean so do we want to start with this uh cap or i think there were there was also like the events one that cida uh opened that is maybe a little more scope i don't know yeah i argue let's start with the siddharth one because it's less uh contentious and we'll probably be arguing this okay great so we'll start there we'll end up at cat 55 uh sit down do you want to yeah i don't know if everyone had time to read the cap document i actually made a fix of this morning but i can give a quick uh overview of it and you can uh we can discuss after that so i made uh i added this this change on to cap 51 which is the host functions cap i just added uh the ability for contracts to log data so i added a contract logs back to the transaction meta um and as a part of this change i we also moved the transaction result into the meta as well both contracts so both contract logs and transact transaction results are hashed and a hash of these hashes are stored in the transaction result pair which is uh this is how you would cryptographically verify them and one change i haven't made to the document yet but we talked about yesterday was that uh we're gonna add another contract log type uh where the logs are only emitted if there's an error um so that's a very high high level overview are there any questions i guess like those logs they are like uh logs that are like the equivalent to events in ethereum uh yeah yeah so they well so the way it would work is uh you know that the contracts allow whatever they want since it gets sent to core or write some transaction meta and Horizon can serve them up in any way you want so i i was i would imagine that you know if you want to listen to a specific event uh Horizon would provide that ability allowing you to you know write applications that would hook on to specific events so one thing that i want to point out here is that it basically means that in order to sift through um data coming in from uh Horizon as as an ingestor you need to basically read everything which may become a lot as as the network grows in capacity um ethereum has this concept of this uh uh bloom filter uh that's included in every ledger header so that you can actually uh get a like a strong indicator whether or not the smart contract that you're interested in or the the account that you're interested in is actually included uh or has emitted events in that specific block um should we consider doing something similar uh yeah i i don't think uh i can't think off top of my head why we wouldn't do um optimize like that that's that area i'm sorry i can look into that i got i don't think i i don't see i don't think i see anything uh wrong with that well actually i think it's kind of uh maybe like premature optimization type of situation like there are probably better ways to do it than doing this kind of arbitrary broom filter thing uh uh like you know i can imagine for example like we have just made a stream uh you could have Horizon telco which which filters it wants to apply instead of kind of doing it after the fact and then the mirror would be a subset of the meta like maybe like you're not interested in ledger changes let's say maybe you're not interested in classic transactions you know like all those things well you're you're assuming a Horizon here and i think that's like part well there's always a consumer right of but if you want something that's like an application specific consumer um which you do see in other ecosystems quite a bit um you know if i'm developing a dap and i want to have a you know like a stream coming in of my of my specific information why should i run a full-blown Horizon rather than some sort of a light client that could just uh you know share logs that are specifically relevant to me and that's again like a subscriber i don't see why this is like part of this gap well i'm not saying i'm not saying it's part of this cap but i am saying like is is there a place in the in the protocol uh to to optimize for these use cases because we actually want more people to run nodes and uh what we're doing here right now is that we're like really tying them down to like the the Horizon model which is consume everything in just everything no like Horizon doesn't force you into uh consuming everything you're you're getting the entire meta and then you filter it yes uh like is the is the amount of meta being produced going to be a bottleneck in the you know even in the middle you know medium term i don't think so like you know xdr is fairly efficient like i would like to see the actual performance problems before picking arbitrary type of filtering technology because i don't know what the use cases are like you're saying they are events yeah sure then you just keep advance it's a very small subset actually of the of the media stream is anything that we're adding here in this cap prevent us from adding a bloom filter or other sort of strategies in the future i don't i don't think so but yeah you can do any kind of filtering i mean i think the the the place where i can see having an actual bottleneck in the future is the actual size of the meta may get too large for like uh if you want to run a light uh lighter node but then we are getting into custom logic in core to kind of you know filter that somehow and i think the the best way to do it is actually like when you're producing it instead of trying to do it after the fact like with like uh like with the bloom filters something that i don't see here when it comes to filtering is any way to filter beyond the contract so like presumably the contract being out of filter up by the contract um is there because that would probably be in the transaction matter but um if a contract wants to emit a whole lot of different logs how would an application filter on those specifically or or is that just too granular to micro well at the moment uh you know the body is an sc val so if you wanted to do that you would add the filtering in there um but you know i think we would discuss this okay like maybe that's not reasonable uh we should add a higher level filters above the sc bell something we can consider yeah like that right now this structure would make sense here yeah like originally i i i thought we would do something like right now you have this uh block type right uh system or contract info and basically if it's a i mean actually for both of them you probably want to have like an actual event name right like which is like a a short symbol of source right yeah i i think that makes sense i would i would i would honestly look at sort of what the subscription patterns that you see in other smart contracting platforms are because they they have explored this this space fairly extensively and i think it's what a lot of the sdks really lean on like if you're writing a dapp it's fairly common for for it to to latch onto a bunch of subscriptions so like however however they're normally doing it we kind of want to support those patterns is there more anyone else have thoughts questions suggestions is it sort of clear what the next move is for you here siddharth you move on to catholic actually like there is uh something yeah that i just thought about that i think you know from what uh i think graydon was asking in terms of use cases like like are there expectations for example that uh and that's related to this filtering question um that you want to have proofs of events that do not happen so like positive proofs are easy like uh like like with with the proposal you have like uh you know basically like uh you can prove that a given ledger had a specific event inside a space you know from a generated by a specific contract what if you want to prove the negative that is that a specific event yeah was not emitted in a ledger is that like the type of things that people try to do in other systems so yeah that's a question for yeah to look into the what happened the use cases yeah 55.
- let's move on to it all right yeah yeah so 55 is uh basically like uh like trying to layer fees on top of the various uh like resource uh uh metering uh that uh started to get introduced in the system so it's kind of a problem is it's a little bit ahead of that because we didn't actually finish all this uh like i think we have a the beginning of like gas metering for example um in cap 46 but there are like other things that are not covered yet um so yeah this is uh so the test disclaimer this gap has a bunch of uh open-ended things um let's see and um yeah so so like they are where the cap uh is uh covered there are like several aspects kind of uh maybe like more important to discuss uh i think um there's the the first one around the uh classification of resources and having market dynamics uh based on those resource types so this is an area where um if you look at uh other blockchains it's actually a mix of things like some systems um like historically started with just like ethereum um just like gas as being the uh yes i i i'm also a gas in the context in this context meaning um the kind of this uh metric right that allows to to to kind of uh count the cost of a um to to execute a transaction and cost here is kind of a pretty loose in terms of definition um uh like mostly computation but when you do like when you load like a like a ledger entry uh like when you access ledger you also pay for for gas um and then yeah like yeah and then so you have like gas cost which is uh this aggregate metric basically of multiple resource types and then uh more recently in ethereum there have been discussions around uh other types of resources that are kind of interesting um such as uh bandwidth uh and uh and any other like basically you have a kind of a funny crossroad there that is do i want to have uh a market for uh for each of those resources or do i want to kind of generate like a composite market for those so like the you have like uh i think in polkadot what they do is uh they put a the aggregation with utilities with a polynomial uh function so basically take all those resource types and then you assign them um a weight actually it's not even i think they are linear it's a linear thing and then yeah you combine all those things and you get with your synthetic i don't remember how they call it weight i think in in over there but like um yeah like that's that's a way to kind of compute this aggregate gas so the challenge um so to talking about challenges that comes with those aggregate uh models is that it's actually very hard to discover price of things like uh an example is uh um if you take a transaction that does a lot of i o and very little compute that is competing with a transaction that does very little i o but a lot of compute like uh with those aggregate functions uh if try to pay like 10 times more for example bid more right for one transaction um you don't know if you're signaling that you're that your i o is is what you want to prioritize or if it's your compute that you want to prioritize um so yeah so it basically causes the overall prices to have like this uncertainty in terms of like what should i bid so that's kind of one of the the problems with those kind of aggregate metrics um so with that said uh with john cannon like one of the things that we are doing is uh we have a very uh clean uh separation between the different resource types so io for example when we read or write the ledger those are done basically outside of the main execution like you can think of before applying a transaction before executing a contract we load all the ledger entries that this contract needs and then it does its thing and then at the end uh it produces potentially uh side effects uh that will be applied as like a post step that's kind of a logically the way to we can you can think about this and um the opportunity here for us is that because we have those uh kind of completely separate um we can actually dis um and and we also do it because of performance reasons uh for parallelism but like because of that we can um um we can actually express those markets uh like uh separately and we can therefore i i think um have like cheaper fees overall because you can price things uh properly so you don't have to do like to kind of articulate inflate for example inflate the price of of reading data from the ledger because compute happens to be expensive uh which kind of would happen with the aggregate model so in the proposal what i'm doing is actually i have like three categories three really brackets for fees so one is for gas which is the compute time exactly you can think of it as execution time in in our model really because like i said earlier we we have a full separation between i o and and uh and execution uh so this one you can bid second market that i have in the proposal is for reading and writing to the ledger so here there is actually a competition for uh there's like there are so many uh there's like a you have like constraints right in terms of a number like the bandwidth to the disk uh subsystem both in reason rights um so um because of that you have to have like a market for that and it is separate right now um and it is separate also because there is a interesting fee model for rights that i'm going to talk about uh later um let's see and the third category um is actually something that is not a market it's not really a market there are dynamic fees for what i would consider like commodity on the network so things like um like uh producing meta or data that ends up being stored in archives those do not there's no reason to have like really competition between uh transactions uh instead we have like limits per transaction like basically we say you can only produce i don't know like a 500k or something of meta right for a transaction and then that's your limit and then two transactions are actually not competing uh you know against each other so there is no need to uh you cannot have a market dynamics there so uh there are actually a few of those um of those resource types and because there are there is no market you can actually aggregate them so they end up in one big bucket of like like deterministic fees basically based on the current state of the ledger plus plus yeah like the actual transaction um so those are like the three three categories that we have in this proposal any question at this point on this i've been going back and forth actually on this uh like a should we or not separate piece we could go with one like i said one market but i think uh it pushes price quite a bit too much for like cheap cheaper whichever resource will be cheaper which is hard to predict do we anticipate that this will be difficult for users to reason about to like understand these different types of fees and to think about how to set them so yes and no i think that's actually one of the things that's kind of interesting is that when um we have uh already in the system a strong dependency on a pre-flight mechanism so like before submitting most transactions to the network uh like they will have to go through a pre-flight endpoint the pre-flight is the the thing that basically will um allow people to compute gas for example to estimate gas for for transaction uh in addition to that um let's see uh in addition to that yeah we have um uh like i was saying like uh certain fees that are like that more like dynamic because they are based on the uh current ledger uh like for example like a bunch of those things that i was saying are like the price of storage in archive this is uh voted uh by the or determined by the validators so at before submitting a transaction you have to know basically what those parameters are um and uh yeah so as part of the pre-flight endpoint you basically get a an estimate for your the minimum fee for for those categories well in the case of the non-market-based resources the the minimum fee is basically equal to your or very likely to be equal to to what you need in the case of um well you do have markets uh it's more like today where you have to decide how much do you want to over bid based on that because the minimum fee doesn't necessarily translate to uh to what the market is is willing to pay so you have to look more at historical data but this is like something we can uh yeah that uh that like endpoint scan like a Horizon can uh can uh can expose right um and having yeah having them tracked as separate resources in terms of historical price uh allows you have actually something a little more stable i would imagine than if it was like an aggregate i think the yeah the complexity from multiple markets uh comes from um i mean one of the implications is actually when we construct one and say we validators construct a transaction set it's going to be a kind of a multi-dimensional nexa problem which is not great but that's the but that algorithm would not be part of the protocol it's more like it's uh if you have like five seconds to produce a block here that's uh there's so much compute you can span in assembling the perfect transaction set yeah to answer original question justin at the end of the day uh the you know the wallets should have an easy way to present an uh like an estimated cost in uh an xlm currency that they can understand and they can tell the user hey like um you know this is how much more you can uh you can propose for that like they don't need to actually understand the mechanics of how this works yeah it's true that when you over beat right like you're doing for example you say i want to to spend like 10 more than whatever happened in last few lectures that 10 percent you can put it i mean i imagine pretty safely across like those different those different resource types like the ones with markets uh because the assumption there is like the they are priced accurately but i imagine that more yeah if you want to really save like a it's hard to predict but like uh like if if there are like some of those like a gas for example becomes very expensive um yeah you don't like maybe you don't want to be as aggressive on on the other resource types um can i jump in with a couple comments yeah um so i guess so um one it's um uh i guess high level it's not clearly what this adds over uh for having like a multi-dimensional optimization problem over the the one-dimensional one um and the reason is that uh sorry i haven't thought about this all that much um but the reason is that like when when at least um in the current execution model sort of everything like everything executes um everything in one lane is going to execute sequentially right and so the main like resource that's that's truly limited in like a block is is time right and um it's it like unless there's some sort of weird um interleaving going on between like transaction executions um that's sort of the one-dimensional resource that we have to optimize anyway and so it's it's not clear to me why um like your example of like a transaction that does lots of i o versus one that does lots of compute well um both of them are going to take a lot of time if they're using a lot of one resource and so it's not clear to me that um uh at least in the current execution setup we have um that it's uh we gain by sort of allocating some um i don't know resource to like i o versus some to to compute as opposed to just like looking at the whole picture of like the total end-to-end time of the transaction um that said it it does seem like uh we it'd be good to like have some kind of like price discovery mechanism um for different resources and um certainly like you want like an overall limit perhaps on like the total number of ledger entries um and so um i know i don't think um sort of thinking off the top my head i don't think it's incompatible to have like a one-dimensional um like gas market and then like sort of price markets on each resource in the sense of um transactions could bid like you know for the the amount of resources they want to use and like the fee per resource um and then you do some like filtering step um but that's sort of thinking um very much off the top my head um i don't think we necessarily i guess high level i don't think we necessarily have to go to the full multi-dimensional operation problem that's but yeah that's possible like uh it it just looked like uh from historical kind of uh experience right like a like i o is a huge problem and um trying to model that as time is actually making it's actually a disservice in a way to the to the network because you have like very expensive uh from uh like this point of view right like something that's going to suck your your disk resources uh that now stole all your calls right on in a multi you know parallel uh execution model um like store as in you know because like i said we do all i o early on and if you're actually maxing out uh your drive then you're just stuck uh right i mean it makes sense to have um perhaps a limit on uh overall io i guess and then there's yeah the other aspect uh actually maybe if we can you know like uh i don't think we're going to necessarily like close on this multi-dimensional thing you know now but like there's the other aspect of yeah ledger size and rights uh that is actually another kind of key thing in there that i guess makes uh io a little more special also nico just to go back to to jeff's point like i understand that why i o needs to be you know priced significantly higher than uh you know the compute operations but i don't necessarily understand why it needs to have like its own market so the the pricing right that you have is the minimum price it's not the market price like when you uh market prices is uh like in the in the ideal like what is describing the cap is is trying to be closer to like the ideal situation where um you can actually construct a transaction set that's going to basically be like right at the edge in terms of the capacity that you have on your actual you know underlying hardware so like cpu and io for disk if we lose that visibility then you may actually allocate too many transactions to compute when then uh like uh you know you you don't have like basically like the the kind of natural way of of uh of having a um a transaction compete against other transactions that are paying for expensive stuff that's kind of what i'm getting to like uh like if you have like um what was it like a good example would be like uh um yeah i don't know which one of those resources would be more expensive but they are not going to be in the same order of magnitude let's say like a compute is the one more expensive at a given time so you have to pay like 10 times more right or 100 times more than the minimum fee for for compute to get to get into the ledger but your your your storage price is also kind of expensive and by bidding a hundred times you also bid a hundred times on storage and you're basically overshooting quite a bit compared to the ideal um model yeah i think i think the general point here is just that you cannot that in reality uh it's not the case that there's there's just time when uh a transaction is executing um there there are two different resources and there are different contention patterns on them and you can't trade one for the other the the the system does not actually trade one for the other like if i if i for example submit uh you know a hundred transactions every one of which is doing incredibly cpu and expensive stuff um that that doesn't saturate the i o system and there's there's still no contention on the i o system uh whereas if i submit a 100 transactions that are just doing i o and they're doing no cpu that doesn't saturate the cpu so they are really two separate resources and the point where one of them gets a limit uh and can no longer do transaction processing it doesn't represent a limit on the other and vice versa and so you you you can't trade between the two of them uh from from a market perspective sorry i'm not quite following something didn't didn't we say earlier that we were going to do like all of the sort of disk reads first and then do the executions right so that if we have a lot of disk reads then we have less time for execution and so the vice versa i i sort of understand that there's not they're not like directly tradable but they seem correlated or anti-correlated well they're they're they're different devices so like right i'm using the disk and then i'm doing the cpu right i'm not using it at the same time i mean like sure there's different offers and things but um yeah sure but the the execution characteristics of each of them are are different so you you use everyone uses the same desk and then everyone sort of farms out to multiple threats right i feel like we're talking past each other i mean yes one goes in in order of the other the two of them do get added together in order to represent the the total time but you can't trade time on one of them for time on the other that's what i'm saying i think i'm not quite following but that's okay nico are there any other networks and fee systems that uh introduce a split or that work similarly those are like the two like the yeah execution time and and uh like compute right and uh and disc are like the two big things that i isolated like the right side i figured it would probably is probably not needed so i kind of put a flat fee for the other ones like basically like uh um in a proposal i'm basically saying like we're okay with you know if you want to have like something that uh like if you have a transaction that is very important that happens to emit a lot of miller for example um but you have to get just over a bit like crazy on your compute even though you're not really that's not what it's about you have to find a way to get it prioritized um let's see are we talking about this okay like i think i kind of wanted to talk about it here is in your proposal you're talking a fair amount about state expiry yeah before we go into estate expiration which i i know is a big topic i'm still trying to to reason about like the kind of the wallet experience and user experience of having having these like multiple dimensions for for uh for gas like what is the expected behavior here for um for wallets to be clear right like tomorrow like single dimension or multi-dimension from a white point of view if you want to estimate it's the same problem right like it's like in the single uh like a cost model like like you aggregate everything into one you have to actually uh you have like a function right that just aggregates but you do have to estimate uh your your bid for each uh things so it's not uh it's actually like a funny thing that that you have except maybe the tools you have for discovering price are not as great because it's all implicit okay so so a wallet can can do a pre-flight can tell me like the expected cost i can um you know bid bit over um but how do i know how to divide that between like the the compute and the i o well like it's it's the same in you know like like i said like it it doesn't that question is not a question around multi-dimension versus single dimension because that that like if you want to multi like if you want to say like i want to pay 10 percent more for on top of the market rate right for storage let's say because that's where there is contention you have to know that that's exactly what you you have to have like you have to have the uh yeah market price for for storage if you just layer like 10 flat on everything uh you're just going to above overbid which is maybe okay right like for some people if the fees are relatively low you know what's the difference between you know uh you know half a lumen and two-thirds of aluminum or something i don't know like historically we've seen that uh like in some situations people are getting uh are bidding very high on on certain uh for certain patterns um it would be great to maybe um and forgive me if it's already in the cap but just like understanding what is the like the uh what's the expected wallet strategy or client strategy here uh when uh like in terms of user experience like what do they present to the user and what uh you know what kind of inputs do they expect from the users yeah sure okay let's talk about state exploration nico where is it well so state exploration yeah goes kind of hand to hand with the uh model that i have there for storing data on the ledger so like the in the proposal it's basically like there are two parts to it there's the how do you model a write and uh as in and so writers can be a create a ledger entry or an update um and how does it work um like how do we have like the the right price basically for the cost of storage um so in the proposal what i what i did is uh i basically used as an approximation for um for the cluster storage the bucket list size so like the uh some basically like the the ledger is organized into those uh like uh 19 buckets i think it's 19 uh and then uh um it you if you if you do a um an update or a create you basically append that to the to the very first bucket in the bucket list so that's how um basically like based on the size of the the the total size the or total size of the ledger um um i allocate like a a price function uh that kind of looks like an exponential from fall uh like basically it starts with a slow slope up to uh some number let's say you say oh like validators here kind of determining those parameters but like you can think of it as the leaders say oh yeah right now we are running on drives with uh i don't know like uh 25 gigs or 50 gigs of space right um and they're going to basically set parameters such that um they don't have to um kind of uh buy new drives uh you know like if there's too much traffic so so the price function is basically looks in this case like you have like your normal slope that goes to in like i don't know let's say you have 100 gig and it would be like i want to use maybe like the first 80 gig at a rate um that's going to be uh like a good rate but not not like overly aggressive and then the last 20 gigs i want to really slow down uh like the the growth so that's right like from 5 looks like this hockey stick type of shape right like an exponential and that's kind of the model for pricing growing the bucket list so you have that uh for that's four rights then the problem is that this is only like uh this is like um saying okay you can add to the bucket list but then uh like and and by the way like if you delete entries um eventually those get collapsed into the buckets and so the bucket is uh shrinks uh in that model a delete you still pay for or delete actually um because delete is actually adding a little bit of of data to the bucket list uh so that's like first thing to note here uh um and then uh yeah what i wanted to get here is uh as a kind of more like a desired property is that i want the price of storage for people to to kind of be the same for everybody regardless if they signed up for you know uh created an account uh like two years ago or you know in five years uh it should be uh over time same cost and um there should be no way to do like uh to have like a free ride on the on on the on the ledger right like so you shouldn't be able to have like store i don't know like nfts like jpegs whatever on chain uh you pay for for this when uh storage is cheap and then uh now you have like uh something that is cheaper than uh than even storing in aws right like that that doesn't make any sense um so this in with that said there's then a need for having some way of kind of uh resetting in a way uh the the the price of uh of storage over time and uh the mechanism that i use there is state expiration so state expiration here means that you have to basically pay for market market price of storage uh to maintain a ledger entry on on the like a live in the in the ledger if you do not pay for this uh for those uh for this for your brands basically you get uh perched that's kind of the the choice that i made in this cap there are a bunch of other ways uh that can be done uh that are actually mentioned in the recap in the other approaches um but like the the reason this kind of works with the other mechanism is that basically like if you if you set a policy for example by default you have to pay rent like a refresh every year let's say uh and then you don't pay your your your renewal after a year uh yeah your the data gets uh deleted um and yeah so there's this kind of constant churn i guess on the ledger which is kind of a new pattern and that construction is basically a way to to guarantee that everybody in the last year has has been paying basically something that is market rate how long do you expect the like the how far in the f like when i trade a letter entry how far is the maximum expiration date that i can choose or that will get chosen for me in the future this stuff isn't as far as i can tell it's not specified in the cap how that works so right so so right now the cap what it says is that it do not so you can renew indefinitely right uh like the the renewal window is determined by validators so that's why i said it's like a um like you say every year you have to pay run right and then every time you write you do an append that happen is valid uh for a year right that's good that's demo then isn't there a natural trade-off between like renewal time and um like fluctuations in this price of storage or are we expecting like this storage cost to not um increase too quickly well it depends uh like what we've seen on the on the current network is uh ledger size has been increasing actually rapidly over the last uh few months because of like some strange token activity which is not entirely uh i mean it's it's not i mean there are a combination of factors like uh one is uh yeah like just price of in crypto assets go you know uh going down uh but also like when they were still pretty high you had like an incentive to to create more crypto assets so it basically those things kind of cancel each other and the growth has been uh pretty significant so i would say like uh seeing a growth rate um that takes you to uh yeah something that will be you want the market basically to kind of get to an eco equilibrium right like where uh where like uh you do not have like uh like those weird use cases uh appearing on the network if they if they are like cheaper than right now i think on on the on the network the the the problem we have is uh yeah we are cheaper than aws fault in some situations isn't there a trade-off here between not necessarily a trade-off but isn't there a consequence here that people will have to go and touch their data from time to time and people are procrastinators and like let's say that like the expiration date is you know when you're in the future or something uh everybody at the end of that year then has to go and touch all their data and there's gonna be a huge log jam to get it done well that's there would be a large jump if everybody creates their stuff at the same time but you would that's not the case it's going to be you know like basically like the thing that expires in a year is the whatever happened today right like at the given date right but i mean like imagine that today you have a day with like a lot of activity like you can look back historically of stellar's history and there are periods when there were like lots of token creations and stuff you know there are days when there's hundreds of thousands of blood draw entries created um and then abundant well many of them are abandoned but like you could imagine a world where they're not all abandoned right and then what happens a year in the future well nothing well people are incentivized to come back some time between here and then i i don't i don't see this as any worse than the fact that we have to handle load spikes in general yeah i mean we have to handle load spikes and let's bikes may get replicated but yeah like what um so what i sketched uh or what we sketched actually in the cup which is uh you know just a more of a strowman type of thing because i'm sure we can do better than that is uh is we actually are um kind of uh ensuring that uh you do not have like giant spikes so like uh it like i think the spike would be not be because of situations don't like what you're what you mentioned because actually activity from today uh if you have like a comp a a a um a linear like a translation right like an actual just shapes right like all this activity gets translated exactly a year from now i you don't have a problem i think the it's just like additional uh cost of running a validator right like it's uh let's you say okay i need to when i set my limits right the number of rights i actually have to think about well actually my capacity in rights is half of of what i can add to the ledger because i also need to delete right but what can happen is uh more of a like um if we have different expiration times which i actually kind of briefly talked about there is that if you have different expiration times you can have actually different dates that end up expiring at the same date and uh and that's for that for those type of situations you have to have an algorithm that kind of uh smooths things out and that doesn't actually cause the system to kind of create a gigantic spike you know at a specific dates so just we don't have a lot of time but i just want to ask uh what like the biggest question i think that there is like what is the what's the expected behavior here like you know these ledger entries are representing financial instruments um let's say assets just for simplicity even though we have like a standard asset contract and i'm uh you know paddling on shares of something um is what is the expectations or what is what's expectations there's like am i supposed to like once a year like come and touch this is like the operator of this financial instrument supposed to do that for me um if you know if you look at uh you know various common immutable contracts like uni swap you know and so and and i'm holding on like these uni tokens what's the expectation here like who's going to touch these for me right so in the in the in the cap i i actually let this kind of flexible like uh there's a when you there's actually a special host function to that you can call that is basically a rewrite equivalent to like a rewrite uh ledger entry um so that you refresh the that that expiration time anybody can do that all right like there's no it's not i understand that anyone can do that but who is who do you expect to do that well it depends on the type of users right like uh like power users probably don't want to do it themselves uh like other situations you know if you're if you're like you said like this very passive type of person uh maybe you should pay somebody to maintain your stuff if that's what you really want uh in other situations i suspect uh if people are not active they probably should just be using centralized uh infrastructure like you know contracts or whatever that are a little more centralized i think trump's point is that there's a free rider problem here like imagine that all the people in this room are using a single contract right like which one of us can touch me all of us have an incentive to wait until the last second and play chicken and hope that somebody else oh like yeah for a short contract i think that uh yeah well it's if it's a shared goodness uh like uh just i that's not what actually what i asked john i assumed that uh like each of us will have like our own ledger entry within that contract so maybe did i not understand correctly i think i think i'm concerned about people's money vanishing into thin air which is completely reasonable these are balances and we're just going to delete them that's not that's not super great i recall there being a proposal to like uh have um but what happened to this um like when you have a ledger engine deleted it like gets um dumped into like some kind of merkle try and you store the hat root of that try and then like when i want to bring it back i can like bring in a proof that this is what the state was right so this is actually in the appendix yeah the alternative section so this there is actually a very detailed uh proposal in ethereum foreign v2 uh about this um so the complications from this archive approach is uh is when you uh want to so like uh restoring is actually yeah like a trivial like i said like you know you have like a maybe a way to do like a to just to basically store that that entry in in inside a merkle tri of source right um and then you just need to provide the proof for that uh the complexity comes from uh when you want to create an entry because you have to prove that that entry doesn't exist in historical data like that was actually archived and that gets really nasty very fast right right sure but like that that like not wanting to do that doesn't address creating this question like what do you do if your money gets deleted it's it's uh it's an event like uh like you know if it's a uh like with an issue it's like today if you're on the seller network if you're sending back to the issuer you know you basically burn it you can ask the issuer like hey sorry i didn't mean to do that but but i didn't burn it but like what if you're like what if it's not like you know an off-chain issuer that you can appeal to like what if your unit swap lp tokens get get deleted what do you do it's tough yeah uh like uh you know these those are the rules of the network wait but that's not that's not a great solution long term right like we're gonna have a lot of people consider the alternative right which is infinite growth of a ledger with infinite price which one do you prefer i mean like objectively like if somebody had a million dollars of like uniform lp shares get liquidated it would have been better to pay for a million dollars of storage so that one person yes but like uh what about everybody else and that person with a million dollars like if they have that it's kind of like key management like you you have procedures to make sure that you don't lose your million dollars it is it is currently the case that people with million dollar balances can in fact uh lose them because they can lose their keys so there's there is there is something to appeal to here like it is it is actually possible for you to lose money just by misusing the system but what about the other end of the spectrum though somebody who doesn't have a million dollars they have a small amount of their balance and they're just constantly eating that up by paying these fees to keep their balance alive i mean it sort of reminds me of like bank accounts where you're like bank accounts just disappearing because you're paying all these fees yeah they do eventually disappear if you if you put five dollars in a bank account and then wait for 20 years it'll go away i mean this just points to yeah you're not stirring your your balance in the right place like this is shared infrastructure like if you don't if you don't use it you lose it but nikki you can't just like ignore the entire industry that we're in and you know i'm not pretending that this is not a problem like people are overlooking this problem but um you know it's i think it will be really difficult to bring people into stellar telling them oh this is the way it works in selena right so it's a fader no it's not selena doesn't actually do any of this right now they have rant no they don't they literally don't charge it right now it's a to be done in the future feature that no one wants so they're never actually going to get around to doing it they have infinite ledger in memory they they allow they charge you money to make to allocate space but they never actually reclaim it there's no there's no active garbage collection process so we're over time at this point and i think that means that we have to stop i mean i know that this is an interesting conversation and there's seems like there's a lot to say about the concept of expiration um but i think we'll push it to next week's meeting and hopefully have some of this discussion on the stellar dev mailing list and here also in discord in the various john cannon channels so if anyone is watching and has thoughts about that feel free to join the stellar dev mailing list or to chime in on the discord here um we'll continue to share work and ideas and conversations debates as they happen and we will see you here again soon thanks everybody

The meeting opened with Siddharth's addendum to CAP-0051: contracts now get a formal logging primitive. Logs will be hashed into the transaction meta (alongside the result) and there is even a proposal for error-only logs so wallets can avoid noise. That sparked a debate about how Horizon or light clients should index events-Bloom filters, opt-in filtering, or layered RPC streams.

Nicolas then debuted CAP-0055, the Soroban fee model. Compute, ledger reads, writes, and storage rent each receive their own buckets, and validators will publish price brackets so wallets can estimate the minimum bid for a given resource profile. The call surfaced questions about throttling, congestion control, and how to keep the minimum fee from being gamed.

Key discussion threads:

- Adding contract logs to CAP-0051's host functions, hashing them into transaction meta, and deciding when error-only logs make sense.
- Strategies for filtering events (Bloom filters vs Horizon streams) so app developers are not forced to ingest the entire meta stream.
- Breaking Soroban fees into compute, memory, footprint, and rent components while still exposing a simple interface to wallets.

<details>
  <summary>Video Transcript</summary>

[00:00] So welcome to the Stellar Open Protocol Discussion in these discussions we discuss and plan for changes to upcoming versions of the Stellar protocol right. Now we're focused on Project Jump Cannon, which will bring smart contracts to Stellar in addition to changes to the Stellar protocol it will also lead to the creation of a new smart contracts platform platform and all of the discussion, that we're having about this is being tracked and you can participate in it much of it is happening here on Discord in the Jump Cannon channel and the drum cannon dev channel there are also a series of Core Advancement Proposal or CAPs, that relate to changes, that would enable Jump Cannon or the new smart contracts platform platform CAPs 46 through 55 I believe at this point they're pretty modular. So each one sort of takes on an aspect of the changes, that need to be made in order to bring smart contracts to Stellar and we are working through those modules bit by bit and discussing sort of the segments necessary and the changes necessary to allow those segments to actually come to life

[01:00] life at some point all of the work, that we do will go through the normal process in other words CAPs are sort of put up in this github repository, that's linked to in the show notes they're discussed here they're discussed on a mailing list and they're discussed in Discord and after they sort of reach a point where they are stable they move from being a draft into a formal acceptance period finally they're accepted and implemented in a version of the Stellar protocol and before, that version of the Stellar protocol goes live validators actually vote to accept it. Now we are still fairly well we've actually made a lot of progress in the in sort of the Jump Cannon trajectory. But as of yet the CAPs, that we have in front of us have not been accepted there's still a lot of questions and today we will dig into some of those questions questions. If you who are listening actually have questions you can leave them as text in the live chat channel i'll try to keep an eye on, that we're certainly trying to move this discussion forward and have the substantive issues come to light

[02:00] so. So you know we may not be able to answer all the live chat questions. But we definitely will later. If we can't answer them in the course of this meeting. So I think everyone is here and I think we are ready to kick off today I know, that recently there was a new CAP 55 fee model in smart contracts I don't know. If it just hit the mailing list yesterday there were a few comments, that came in I guess I'm going to start by asking you nicola. If we're ready to discuss, that if, that's where we should start maybe yeah like we can maybe like just go over like a quick overview of like what's going on in, that CAP. And then then you know like we don't have to go in details basically. So I'm not sure like we need to have like a lot of of a pre-reading basically as part of this

[03:00] this is, that good use of time of people yeah I think, that sounds good also Nico. If you can you know obviously a lot of these things are kind of like normal quote unquote in the world of crypto. But some of these are are a bit more contentious. So I would like emphasize specifically like the contentious bits so, that we can have a good old-fashioned argument all right let's see and yeah I mean. So do we want to start with this CAP or I think there were there was also like the events one, that cida opened, that is maybe a little more scope I don't know yeah I argue let's start with the Siddharth one. Because it's less contentious and we'll probably be arguing this okay great. So we'll start there we'll end up at CAP 55 sit down do you want to yeah I don't know. If everyone had time to read the CAP document I actually made a fix of this morning

[04:00] morning. But I can give a quick overview of it and and you can we can discuss after, that. So I made I added this change on to CAP 51, which is the host functions CAP CAP I just added the ability for contracts to log data. So I added a contract logs back to the transaction meta and as a part of this change I we also moved the transaction result into the meta as well both contracts. So both contract logs and transact transaction results are hashed and a hash of these hashes are stored in the transaction result pair, which is this is how you would cryptographically verify them and one change I haven't made to the document yet. But we talked about yesterday was, that we're gonna add another another contract log type where the logs are only emitted. If there's an error so, that's a very high level overview are there any questions

[05:00] I guess like those logs they are like the equivalent to events in Ethereum yeah. So they well. So the way it would work is you know, that the contracts allow whatever they want since it gets sent to core core or write some transaction meta and Horizon can serve them up in any way you want. So I was I would imagine, that you know. If you want to listen to a specific event Horizon would provide, that ability allowing you to you know write applications, that would hook on to specific events. So one thing, that I want to point out here is, that it basically means, that in order to sift through through data coming in from Horizon as an ingestor you need to basically read everything

[06:00] which may become a lot as the network grows in capacity Ethereum has this concept of this bloom filter, that's included in every ledger header so, that you can actually actually get a like a strong indicator whether or not the smart contract, that you're interested in or the account, that you're interested in is actually included or has emitted events in, that specific block should we consider doing something similar yeah I don't think I can't think off top of my head why we wouldn't do optimize like, that that's, that area I'm sorry I can look into, that I got I don't think I don't see I don't think I see anything wrong with, that well actually I think it's kind of maybe like premature optimization type of situation

[07:00] like there are probably better ways to do it than doing this kind of arbitrary broom filter thing like you know I can imagine for example like we have just made a stream you could have Horizon telco, which filters it wants to apply instead of kind of doing it after the fact. And then the mirror would be a subset of the meta like maybe like you're not interested in ledger changes let's say maybe you're not interested in classic transactions you know like all those things things well you're assuming a Horizon here and I think, that's like part well there's always a consumer right of but. If you want something, that's like an application specific consumer, which you do see in other ecosystems quite a bit you know. If I'm developing a dApp and I want to have a

[08:00] you know like a stream coming in of my specific information why should I run a full-blown Horizon rather than some sort of a light client, that could just you know share logs, that are specifically relevant to me and, that's again like a subscriber I don't see why this is like part of this gap well I'm not saying it's part of this CAP. But I am saying like is there a place in the protocol to optimize for these use cases. Because we actually want more people to run nodes nodes and and what we're doing here right. Now is, that we're like really tying them down to like the Horizon model, which is consume everything in just everything no no like Horizon doesn't force you into consuming everything you're getting the entire meta

[09:00] and. Then you filter it yes like is the amount of meta being produced going to be a bottleneck in the you know even in the middle you know medium medium term I don't think. So like you know XDR is fairly efficient like I would like to see the actual performance problems before picking arbitrary type of filtering technology. Because I don't know what the use cases are like you're saying they are events yeah sure. Then you just keep advance it's a very small subset actually of the media stream is anything, that we're adding here in this CAP prevent us from adding a bloom filter or other sort of strategies in the future I don't think. So but yeah you can do any kind of filtering I mean mean I think the the place where I can see having an actual bottleneck in the future is the

[10:00] actual size of the meta may get too large for like. If you want to run a light lighter node. But then we are getting into custom logic in core to kind of you know filter, that somehow and I think the best way to do it is actually like. When you're producing it instead of trying to do it after the fact like with like like with the bloom filters something, that I don't see here. When it comes to filtering is any way to filter beyond the contract. So like like presumably the contract being out of filter up by the contract is there. Because that would probably be in the transaction matter. But but. If a contract wants to emit emit a whole lot of different logs

[11:00] how would an application filter on those specifically or is, that just too granular to micro well at the moment you know the body is an sc val so. If you wanted to do, that that you would add the filtering in there. But you know I think we would discuss this okay like maybe, that's not reasonable we should add a higher level filters above the sc bell something we can consider yeah like, that right. Now this structure would make sense here yeah like originally I thought we would do something like right. Now you have this block type right system or contract info and basically. If it's a I mean actually for both of them you probably want to have like an actual event name right like, which is like a short symbol of source right

[12:00] yeah I think, that makes sense I would honestly look at sort of what the subscription patterns, that you see in other smart contracting platforms are. Because they have explored this space fairly extensively and I think it's what a lot of the SDKs really lean on like. If you're writing a dapp it's fairly common for it to latch onto a bunch of subscriptions. So like. However they're normally doing it we kind of want to support those patterns is there more anyone else have thoughts questions suggestions is it sort of clear what the next move is for you here

[13:00] Siddharth you move on to catholic actually like there is something yeah, that I just thought about, that I think you know from what I think graydon was asking in terms of use cases like like are there expectations for example, that and, that's related to this filtering question, that you want to have proofs of events, that do not happen. So like positive proofs are easy like like with the proposal you have like you know basically like you can prove, that a given ledger had a specific event inside a space you know from a generated by a specific contract what. If you want to prove the negative, that is

[14:00] that a specific event yeah was not emitted in a ledger is, that like the type of things, that people people try to do in other systems. So yeah, that's a question for yeah to look into the what happened the use cases yeah 55. Let's move on to it all right yeah yeah. So 55 is basically like like trying to layer fees on top of the various like like resource metering, that metering, that started to get introduced in the system. So it's kind of a problem is it's a little bit ahead of, that. Because we didn't actually

[15:00] finish all this like I think we have a the beginning of like gas metering for example in CAP 46. But there are like other things, that are not covered yet. So yeah this is. So the test disclaimer this gap has a bunch of open-ended open-ended things things let's see and yeah. So like they are are where the CAP is covered there are like several aspects kind of maybe like more important to discuss I think there's the first one around the classification of resources and having market dynamics based on those resource types. So this is an area where. If you look at other blockchains it's actually a mix of things

[16:00] like some systems like historically started with just like Ethereum just like gas as being the yes I I'm also a gas in the context in this context meaning the the kind of this metric right, that allows to to kind of count the cost of a to execute a transaction and cost here is kind of a pretty loose in terms of definition like mostly computation but. When you do like. a ledger entry like. When you like ledger you also pay for gas. And then yeah like yeah. And then so you have like gas cost, which is this aggregate metric basically of multiple resource types and

[17:00] then more recently in Ethereum there have been discussions around other types of resources, that are kind of interesting such as bandwidth and any other like basically you have a kind of a funny funny crossroad there, that is do I want to have a market for for each of those resources or do I want to kind of generate like a composite market market for those. So like the you have like I think in polkadot what they do is they put a the aggregation with utilities with a polynomial function. So basically take all those resource types. And then you assign them a weight actually it's not even I think they are linear it's a linear thing

[18:00] thing. And then yeah you combine all those things and you get with your synthetic I don't remember how they call it weight I think in in over there. But like yeah like, that's a way to kind of compute this aggregate gas. So the challenge. So to talking about challenges, that comes with those aggregate models is, that it's actually very hard to discover price of things things like an example is. If you take a transaction, that does a lot of I o and very little compute, that is competing with a transaction, that does very little I o. But a lot of compute compute like like with those aggregate functions. If try to pay like 10 times more for example bid more

[19:00] right for one transaction you don't know. If you're signaling, that you're, that your I o is what you want to prioritize or. If it's your compute, that you want to prioritize. So so yeah. So it basically causes the overall overall prices to have like this uncertainty in terms of like what should I bid so, that's kind of one of the problems with those kind of aggregate aggregate metrics. So with, that said with John cannon like one of the things, that we are doing is we have a very clean separation between the different resource types. So io for example. When we read or write the ledger those are done basically outside of the main main execution like you can think of before applying a transaction before

[20:00] executing a contract we load all the ledger entries, that this contract needs. And then it does its thing. And then at the end it produces potentially side effects, that will be applied as like a post step, that's kind of a logically the way to we can you can think about this and the opportunity here for us is, that. Because we have those kind of completely separate we can actually dis and we also do it. Because of performance reasons for parallelism. But like. Because of, that we can we can actually express those markets like separately and we can. Therefore I think have like cheaper fees overall. Because you can price things properly. So you don't have to do like to kind of articulate inflate for

[21:00] example inflate the price of reading data from the ledger. Because compute happens to be expensive, which kind of would happen with the aggregate model. So so in the proposal what I'm doing is actually I have like three categories three really really brackets for fees. So one is for gas, which is the compute time exactly you can think of it as execution time in our model really. Because like I said earlier we have a full separation between I o and and execution. So this one you can bid bid second second market, that I have in the proposal is for reading and writing to the ledger. So here there is actually a competition for there's like there are. So many

[22:00] there's like a you have like constraints right in terms of a number like the bandwidth to the disk subsystem both in reason rights so. Because of, that you have to have like a market for, that and and it is separate right. Now and it is separate also. Because there is a a interesting fee model for rights, that I'm going to talk about later let's see and the third category is actually something, that is not a market it's not really a market there are dynamic fees for what I would consider like commodity on the network. So things like like producing meta or data, that ends up being stored in archives archives those do not there's no reason to have like really competition between transactions

[23:00] instead we have like limits per transaction like basically we say you can only produce I don't know like a 500k or something of meta right for a transaction and then, that's your limit. And then two transactions are actually not competing you know against each other. So there is no no need to you cannot have a market dynamics there. So so there are actually a few of those of those resource types and. Because there are there is no market you can actually aggregate them. So they end up in one big bucket of like like deterministic fees basically based on the current state of the ledger plus plus yeah like the actual transaction. So those are like the three three categories, that we have in this proposal

[24:00] proposal any question at this point on this i've been going back and forth actually on this like a should we or not separate piece we could go with one like I said one market. But I think it pushes price quite a bit too much for like cheap cheaper whichever resource will be cheaper, which is hard to predict do we anticipate, that this will be difficult for users to reason about to like understand these different types of fees and to think about how to set them. So so yes and no I think, that's actually one of the things, that's kind of interesting is, that. When we have already in the system a strong dependency on a pre-flight

[25:00] mechanism. So like before submitting most transactions to the network like they will have to go through a pre-flight endpoint the pre-flight is the thing, that basically will allow people to compute compute gas for example to estimate gas for transaction in addition to, that let's see in addition to, that yeah we have like I was saying like certain fees, that are like, that more like dynamic. Because they are based on the current ledger like for example like a bunch of those things, that I was saying are like the price of storage in archive this is voted by the or determined by the validators. So at before submitting a transaction you have to know basically what those parameters are

[26:00] and yeah. So as part of the pre-flight endpoint you basically get a an estimate for your the minimum fee for those categories well in the case of the non-market-based resources the minimum fee is basically equal to your or very likely to be equal to what you need in the case of well you do have markets it's more like today where you have to decide how much do you want to over bid based on, that. Because the minimum fee doesn't necessarily translate to to what the market is willing to pay. So you have to look more at historical data data. But but this is like something we can yeah, that that like endpoint scan like a Horizon can

[27:00] can expose right can having yeah having them tracked as separate resources in terms of historical price allows you have actually something a little little more stable I would imagine than. If it was like an aggregate I think the yeah the complexity from multiple markets comes from I mean one of the implications is actually. When we we construct one and say we validators construct construct a transaction set it's going to be a kind of a multi-dimensional nexa problem, which is not great but, that's the but, that algorithm would not be part of the protocol it's more like like it's

[28:00] if you have like five seconds to produce a block here, that's there's. So much compute you can span in assembling the perfect transaction set yeah to answer original question Justin at the end of the day the you know the wallets should have an easy way to present an like an estimated cost in in an XLM currency, that they can understand understand and they can tell the user hey like you know this is how much more you can you can propose for, that like they don't need to actually understand the mechanics of how this works yeah it's true, that. When you over beat right like you're doing for example you say I want to spend like 10 more than whatever happened in last few lectures lectures, that 10 percent you can put it I mean I imagine pretty safely across like those different resource types like the ones with

[29:00] markets markets. Because the assumption there is like the they are priced accurately. But I imagine, that more yeah. If you want to really save like a it's hard to predict. But like like. If there are like some of those like a gas for example becomes very expensive expensive yeah you don't like maybe you don't want to be as aggressive on the other resource types can I jump in with a couple comments yeah. So I guess. So one it's I guess high level it's not clearly what this adds over for having like a multi-dimensional optimization problem over the one-dimensional one and the reason is

[30:00] that sorry I haven't thought about this all, that much. But the reason is, that like. When when at least in the current execution model sort of everything like everything executes everything in one lane is going to execute sequentially right. And so the main like resource, that's truly limited in like a block is time right and it's it's it like unless there's some sort of weird weird interleaving going on between like transaction executions, that's sort of the one-dimensional resource, that we have to optimize anyway. And so it's not clear to me why like like your example of like a transaction, that does lots of I o versus one, that does lots of compute well both of them are going to take a lot of time. If they're using a lot of one resource. And so it's not clear to me, that at least in the current execution setup we have, that it's we gain by sort of allocating some I don't know

[31:00] resource to like I o versus some to compute as opposed to just like looking at the whole picture of like the total end-to-end time of the transaction, that said it does seem like we it'd be good to like have some kind of like price discovery mechanism for different resources and certainly like you want like an overall limit perhaps on like the total number of ledger entries. And so I know I don't think sort of thinking off the top my head I don't think it's incompatible to have like a one-dimensional like like gas market. And then like sort of price markets on each resource in the sense of transactions could bid like you know for the amount of resources they want to use and like the fee per resource. And then you do some like filtering step but, that's sort of thinking very much off the top my head I don't think we necessarily I guess high level I don't think we necessarily have to go to the full multi-dimensional

[32:00] operation problem, that's. But yeah, that's possible like it just looked like from from historical historical kind of experience right like a like I o is a huge problem and trying to model, that as time is actually making it's actually a disservice in a way to the network. Because you have like very expensive from like this point of view right like something, that's going to suck your disk resources, that. Now stole all your calls right on in a multi you know parallel execution model like store as in you know. Because like I said we do all I o early on and. If you're actually maxing out out your drive. Then you're just stuck

[33:00] right I mean it makes sense to have perhaps a limit on overall io I guess. And then there's yeah the other aspect actually maybe. If we can you know like I don't think we're going to necessarily like close on this multi-dimensional thing you know. Now but like there's the other aspect of yeah ledger size and rights, that is actually another kind of key thing in there, that I guess makes io a little more special also Nico just to go back to jeff's point like I understand, that why I o needs to be you know priced significantly higher than you know the compute operations. But I don't necessarily understand why it needs to have like its own market. So the pricing right, that you have is the minimum price it's not

[34:00] the market price like. When you market prices is like like in the ideal like what is describing the CAP is trying to be closer to like the ideal situation where you can actually construct a transaction set, that's going to basically be like right at the edge in terms of the capacity, that you have on your actual you know underlying hardware. So like cpu and io for disk. If we lose, that visibility. Then you may actually allocate too many transactions to compute. When then like you know you don't have like basically like the the kind of natural way of of having a

[35:00] a transaction compete against other transactions, that are paying for expensive stuff, that's kind of what I'm getting to like like. If you have like what was it like a good example would be like yeah I don't know, which one of those resources would be more expensive. But they are not going to be in the same order of magnitude let's say like a compute is the one more expensive at a given time. So so you have to pay like 10 times more right or 100 times more than the minimum fee for compute to get into the ledger. But your your storage price is also kind of expensive and by bidding a hundred times you also bid a hundred times on storage storage and you're basically overshooting quite

[36:00] a bit compared to the ideal model model yeah I think the general point here is just, that you cannot, that in reality reality it's not the case, that there's just time. When a transaction is executing there are two different resources and there are different contention patterns on them and you can't trade one for the other the the system does not actually trade one for the other like. If I for example submit you know a hundred transactions every one of, which is doing incredibly cpu and expensive stuff, that doesn't saturate the I o system and there's still no contention on the I o system whereas. If I submit a 100 transactions, that are just doing I o and they're doing no cpu, that doesn't saturate the cpu. So they are really two separate resources and the point where one of them gets a limit and can no longer do transaction processing it doesn't doesn't represent a limit on the other and vice versa. And so you you can't trade

[37:00] between the two of them from a market perspective sorry I'm not quite following something didn't we say earlier, that we were going to do like all of the sort of disk reads first. And then do the executions right so, that. If we have a lot of disk reads. Then we have less time for execution. And so the vice versa I sort of understand, that there's not they're not like directly tradable. But they seem correlated or anti-correlated well they're they're different devices. So like right I'm using the disk. And then I'm doing the cpu right I'm not using it at the same time I mean like sure there's different offers and things. But yeah sure. But the execution characteristics of each of them are different. So you use everyone uses the same desk. And then everyone sort of farms out to multiple threats right I feel like we're talking past each

[38:00] other I mean yes one goes in order of the other the two of them do get added together together in order to represent the total time. But you can't trade time on one of them for time on the other, that's what I'm saying I think I'm not quite following but, that's okay Nico are there any other networks and fee systems, that introduce a split or, that work similarly those are like the two like the yeah execution time and like compute right and and disc are like the two big things, that I isolated like the right side I figured it would probably is probably not needed. So I kind of put a flat fee for the other ones like basically like in a proposal I'm basically saying like

[39:00] we're okay with you know. If you want to have like something, that like. If you have a transaction, that is very important, that happens to emit a lot of miller for example. But you have to get just over a bit like crazy on your compute even, though you're not really, that's not what it's about you have to find a way to get it prioritized let's see are we talking about this okay like I think I kind of wanted to talk about it here is in your proposal you're talking a fair amount about state expiry yeah yeah before we go into estate expiration, which I know is a big topic I'm still trying to reason about like the kind of the wallet experience and user experience of having these like multiple dimensions for for gas like

[40:00] what is the expected behavior here for for wallets to be clear right like tomorrow like single dimension or multi-dimension from a white point of view. If you want to estimate it's the same problem right like it's like in the single like a cost model like you aggregate everything into one you have to actually you have like a function right, that just aggregates. But you do have to estimate your bid for each things. So it's not it's actually like a funny thing, that you have except maybe the tools you have for discovering price are not as great great. Because it's all implicit okay. So a wallet can do a pre-flight can tell me like the expected cost cost I can

[41:00] you know bid bid bit over. But how do I know how to divide, that between like the compute and the I o well like it's the same in you know like I said like it doesn't, that question is not not a question around multi-dimension versus single dimension. Because that like. If you want to say like I want to pay 10 percent more for on top of the market rate right for storage let's say. Because that's where there is contention you have to know, that that's exactly what you have to have the yeah market price for storage. If you just layer like 10 flat on everything everything you're just going to above overbid

[42:00] which is maybe okay right like for some people. If the fees are relatively low you know what's the difference between you know you know half a lumen and two-thirds of aluminum or something I don't know like historically we've seen, that like in some situations people are getting are bidding very high on certain on certain for certain patterns it would be great to maybe and forgive me. If it's already in the CAP. But just like understanding what is the like the what's the expected wallet strategy or client strategy here. When like in terms of user experience like what do they present to the user

[43:00] and what you know what kind of inputs do they expect from the users yeah sure okay let's talk about state exploration Nico Nico where is it well. So state exploration yeah goes kind of hand to hand with the model, that I have there for storing data on the ledger. So like the in the proposal it's basically like there are two parts to it there's the how do you model model a write and as in. And so writers can be a create a ledger entry or an update and how does it work like how do we have like the right price basically for the cost of storage. So in the proposal what I did is I basically used as an approximation for

[44:00] for the cluster storage the bucket list size. So like the some basically like the ledger is organized into those like 19 buckets I think it's 19. And then it you. If you do a an update or a create you basically append, that to the the to the very first bucket in the bucket list list so, that's how basically like based on the size of the the total size of the ledger I allocate like a price function, that kind of looks like an exponential from fall like basically it starts with a slow slope up to some number let's say you say oh like

[45:00] validators here kind of determining those parameters. But like you can think of it as the leaders say oh yeah right. Now we are running on drives with I don't know like 25 25 gigs or 50 gigs of space right and they're going to basically set parameters such, that they don't have to kind of buy new drives you know like. If there's too much traffic. So so the price function is basically looks in this case like you have like your normal slope, that goes to in like I don't know let's say you have 100 gig and it would be like I want to use maybe like the first 80 gig at a rate, that's going to be like a good rate. But not like overly aggressive. And then the last 20 gigs I want to

[46:00] really slow down like the growth so, that's right like from 5 looks like this hockey stick type of shape right like an exponential and, that's kind of the model for pricing growing the bucket list. So so you have, that for, that's four rights. Then the problem is, that this is only like this is like saying okay you can add to the bucket list. But then like like and by the way like. If you delete entries entries eventually those get collapsed into the buckets. And so the bucket is shrinks in, that model a delete you still pay for or delete actually. Because delete is actually adding a little bit of data to the bucket list so, that's like first thing to note here

[47:00] and. Then yeah what I wanted to get here is as a kind of more like a desired property is, that I want the price of storage for people to kind of be the same for everybody regardless. If they signed up for you know created an account like two years ago or you know in five years it should be over time same cost and there should be no way to do like to have like a free ride on the ledger right like. So you the on able to have like store store I don't know like NFTs like jpegs whatever on chain you pay for this. When storage is cheap. And then now you have like something, that is cheaper than than even storing in aws right like, that doesn't make any sense

[48:00] so this in with, that said there's. Then a need for having some way of of kind of resetting in a way the the price of of storage over time and the mechanism, that I use there is state expiration. So state expiration here means, that you have to basically pay for market price of storage to maintain a ledger entry on the like a live in the ledger. If you do not pay for this for your brands basically you get perched, that's kind of the choice, that I made in this cap

[49:00] there are a bunch of other ways, that can be done, that are actually mentioned in the recap in the other approaches approaches. But like the the reason this kind of works with the other mechanism is, that basically like. If you set a policy for example by default you have to pay rent like a refresh every year let's say say. And then you don't pay your your renewal after a year yeah your the data gets deleted and yeah. So there's this kind of constant churn I guess on the ledger, which is kind of a new pattern and, that construction is basically a way to guarantee, that everybody in the last year has been paying basically something, that is

[50:00] market rate how long do you expect the like the how far in the f like. When I trade a letter entry how far is the maximum expiration date, that I can choose or, that will get chosen for me in the future this stuff isn't as far as I can tell it's not specified in the CAP how, that works. So right. Now the CAP what it says is, that it do not. So you can renew indefinitely right like the renewal window is determined by validators so, that's why I said it's like a like you say every year you have to pay run run right. And then every time you write you do an append, that happen is valid for a year right, that's good, that's demo. Then

[51:00] isn't there a natural trade-off between like renewal time and like fluctuations in this price of storage or are we expecting like this storage cost to not increase too quickly well it depends like what we've seen on the current network is ledger size has been increasing actually rapidly over the last few months. Because of like some strange token activity, which is not entirely I mean it's not I mean there are a combination of factors like one is yeah like just price of in crypto assets go you know going down. But also like. When they were were still pretty high you had like an incentive to create more crypto assets. So it basically those things kind of cancel each other and the growth has been pretty significant

[52:00] so I would say like seeing a growth rate, that that takes you to yeah something, that will be you want the market basically to kind of get to an eco equilibrium right like where where where like you do not have like like those weird use cases appearing on the network. If they are like cheaper than right. Now I think on the network the the problem we have is yeah we are cheaper than aws fault in some situations isn't there a trade-off here between not necessarily a trade-off. But isn't there a consequence here, that people will have to go and touch their data from time to time and people are procrastinators and like

[53:00] let's say, that like the expiration date is you know. When you're in the future or something everybody at the end of, that year. Then has to go and touch all their data and there's gonna be a huge log jam to get it done well, that's there would be a large jump. If everybody creates their stuff at the same time. But you would, that's not the case it's going to be you know like basically like the thing, that expires in a year is the whatever happened today right like at the given date right. But I mean like imagine, that today you have a day with like a lot of activity like you can look back historically of Stellar's history and there are periods. When there were like lots of token creations and stuff you know there are days. When there's hundreds of thousands of blood draw entries entries created created. And then abundant well many of them are abandoned. But like you could imagine a world where they're not all abandoned right. And then what happens a year in the future well nothing well people are incentivized to come back some time between here. And then I don't

[54:00] see this as any worse than the fact, that we have to handle load spikes in general yeah I mean we have to handle load spikes and let's bikes may get replicated. But yeah like what. So what I sketched or what we sketched actually in the cup, which is you know just a more of a strowman type of thing. Because I'm sure we can do better than, that is is we actually are kind of ensuring, that you do not have like giant spikes. So like it like I think the spike would be not be. Because of situations don't like what you're what you mentioned. Because actually activity from today. If you have like a comp a a a linear like a translation right like an actual just shapes right like all this activity gets translated exactly a year from now I you don't have a problem I think the it's just like additional cost of running a validator right

[55:00] like it's let's you say okay I need to to. When I set my limits right the number of rights rights I actually have to think about well actually my capacity in rights is half of what I can add to the ledger. Because I also need to delete right. But what can happen is more of a like. If we have different expiration times, which I actually kind of briefly talked about there is, that. If you have different expiration times times you can have actually different dates, that that end up expiring at the same date and and, that's for, that for those type of situations you have to have an algorithm, that kind of smooths things out and, that doesn't actually cause the system to kind of create a gigantic spike you know

[56:00] at a specific dates. So just we don't have a lot of time. But I just want to ask what like the biggest question I think, that there is like what is the what's the expected behavior here like you know these ledger entries are representing financial instruments instruments let's say assets just for simplicity even, though we have like a standard asset contract and I'm you know paddling on shares of something is what is the expectations or what is what's expectations there's like am I supposed to like once a year like come and touch this is like the operator of this financial instrument supposed to do, that for me. If you know. If you look at you know various common immutable contracts like uni swap you know. And so and I'm holding on like these uni tokens tokens what's the expectation here like who's going to touch these for me

[57:00] right. So in the CAP I actually let this kind of flexible like there's a. When you there's actually a special host function to, that you can call, that is basically a rewrite equivalent to like a rewrite ledger entry so, that you refresh the, that expiration time anybody can do, that all right like there's no it's not I understand, that anyone can do, that. But who is who do you expect to do, that that well it depends on the type of users right like like power users probably don't want to do it themselves like other situations you know. If you're like you said like this very passive type of person maybe you should pay somebody to maintain your stuff. If that's what you really want in other situations I suspect. If people are not active they probably should just be using centralized infrastructure

[58:00] infrastructure like you know contracts or whatever, that are a little more centralized centralized I think trump's point is, that there's a free rider problem here like imagine, that all the people in this room are using a single contract right like like, which one of us can touch me all of us have an incentive to wait until the last second and play chicken and hope, that somebody else oh like yeah for a short contract I think, that yeah well it's. If it's a shared goodness like just just I, that's not what actually what I asked John I assumed, that like each of us will have like our own ledger entry within, that contract. So maybe did I not understand correctly I think I'm concerned about people's money vanishing into thin air, which is completely reasonable these are balances and we're just going to delete them, that's not super great I recall there being a proposal to like

[59:00] have. But what happened to this like. When you have a ledger engine deleted it like gets dumped into like some kind of merkle try and you store the hat root of, that try. And then like. When I want to bring it back I can like bring in a proof, that this is what the state was right. So this is actually in the appendix yeah the alternative section. So this there is actually a very detailed proposal in Ethereum foreign v2 about this. So the complications from this archive approach is is. When you want to. So like restoring is actually yeah like a trivial like I said like you know you have like a maybe a way to do like a to just to basically store, that entry in inside a merkle tri of source right. And then you just need to provide the proof for, that that the complexity comes from

[01:00:00] when you want to create an entry. Because you have to prove, that entry doesn't exist in historical data like, that was actually archived and, that gets really nasty very fast right sure. But like, that like not wanting to do, that doesn't address creating this question like what do you do. If your money gets deleted it's it's an event like like you know. If it's a like with an issue it's like today. If you're on the seller network. If you're sending back to the issuer you know you basically basically burn it you can ask the issuer like hey sorry I didn't mean to do, that. But I didn't burn it. But like what. If you're like what. If it's not like you know an off-chain issuer, that you can appeal to like what. If your unit swap lp tokens get deleted what do you do it's tough yeah like you know

[01:01:00] these those are the rules of the network wait but, that's not a great solution long term right like we're gonna have a lot of people people consider the alternative right, which is infinite growth of a ledger with infinite price, which one do you prefer I mean like objectively like. If somebody had a million dollars of like uniform lp shares get liquidated it would have been better to pay for a million dollars of storage so, that one person yes. But like what about everybody else and, that person with a million dollars like. If they have, that it's kind of like key management like you have procedures to make sure, that you don't lose your million dollars it is currently the case, that people with million dollar balances can in fact lose them. Because they can lose their keys. So there's there is something to appeal to here like it is actually possible for you to lose money just by

[01:02:00] misusing the system. But what about the other end of the spectrum, though somebody who doesn't have a million dollars they have a small amount of their balance and they're just constantly eating, that up by paying these fees to keep their balance alive I mean it sort of reminds me of like bank accounts where you're like bank accounts just disappearing. Because you're paying all these fees yeah they do eventually disappear. If you put five dollars in a bank account. And then wait for 20 years it'll go away I mean this just points to yeah you're not stirring your balance in the right place like this is shared infrastructure like. If you don't use it you lose it. But nikki you can't just like ignore the entire entire industry, that we're in and you know I'm not pretending, that this is not a problem like people are overlooking this problem problem. But but you know it's I think it will be really difficult to bring people into Stellar telling them oh this is the way it works

[01:03:00] in selena right. So it's a fader no it's not selena doesn't actually actually do any of this right. Now they have rant no they don't they literally don't charge it right. Now it's a to be done in the future feature, that no one wants. So they're never actually going to get around to doing it they have infinite ledger in memory they allow they charge you money to make to allocate space. But they never actually reclaim it there's no active garbage collection process. So we're over time at this point and I think, that means, that we have to stop I mean I know, that this is an interesting conversation and there's seems like there's a lot to say about the concept of expiration. But I think we'll push it to next week's meeting and hopefully have some of this discussion on the Stellar dev mailing list and here also in Discord in the various John cannon channels so. If anyone is watching and has thoughts about, that feel free to join the Stellar dev mailing list or to

[01:04:00] chime in on the Discord here we'll continue to share work and ideas and conversations conversations debates as they happen and we will see you here again soon thanks everybody

</details>
