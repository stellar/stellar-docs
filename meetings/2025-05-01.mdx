---
title: "Protocol 23 CAP Follow-up and Q&A"
description: "A Protocol 23 readiness check covering final questions on upcoming CAPs, with focus on CAP-70 consensus timing configurability and follow-ups on CAP-67 unified events and fee refund event semantics."
authors:
  - carsten-jacobsen
  - dmytro-kozhevin
  - garand-tyson
  - leigh-mcculloch
  - nicolas-barry
tags:
  - developer
  - CAP-6
  - CAP-62
  - CAP-63
  - CAP-65
  - CAP-66
  - CAP-67
  - CAP-68
  - CAP-69
  - CAP-70
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="fuX07T1HUf0" />

This protocol meeting serves as a final community Q&A before the Core CAP team vote on the Protocol 23 (Whisk) CAP set. The group reviews overall readiness, notes which CAPs are unchanged since prior discussions, and spends most of the time on newer or still-active items.

The primary deep-dive is CAP-70, which introduces configurable consensus timing parameters (without changing defaults yet). The remainder of the session recaps the broader Protocol 23 CAP bundle, with updates and open questions around unified events (CAP-67) and a short discussion on an executable-address getter proposal (CAP-69).

### Key Topics

- Protocol 23 status:
  - Implementation is nearing completion for Whisk and the Protocol 23 CAP bundle
  - Meeting goal: final recap and answering community questions before Core CAP team voting
- CAP-70 consensus timing configurability:
  - Make previously hard-coded timing values configurable (e.g., ledger close time and round timeouts)
  - No behavior change on activation; defaults match current hard-coded values
  - Motivation: enable small incremental tuning toward lower ledger close times over future upgrades
  - Risk discussion:
    - Misconfiguration could destabilize consensus if set to extreme values
    - Suggested mitigation: add tight implementation-level bounds to prevent unsafe ranges
    - Consideration of absolute bounds vs relative “percent change” limits, including disaster-recovery tradeoffs
    - Need for supercluster testing and careful rollout practices before any future parameter changes
- CAP bundle recap (no major semantic changes noted):
  - CAP-65 module cache, CAP-62/66 archival + in-memory resources, CAP-63 scheduling: progressing largely as previously discussed
  - CAP-68 noted as potentially optional depending on remaining concerns
- CAP-67 unified events follow-ups:
  - Remove muxed source support for classic events to avoid inconsistencies and unclear use cases
  - Ongoing open item: how to represent fee charging vs refund timing in events
  - Direction: keep events practical for common balance-tracking use cases; accept that some edge cases may require TX meta
- CAP-69 executable-address getter discussion:
  - Debate on whether exposing “is this address executable?” could encourage harmful patterns (ecosystem-specific branching)
  - Counterpoint: similar checks can be approximated today; a standardized API could be safer and less hacky
  - General agreement that the proposed API is reasonable, with caution around transitive dependency pinning and interoperability

### Resources

- [CAP-0062](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0062.md) — [Discussion](https://github.com/orgs/stellar/discussions/1575)
- [CAP-0063](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0063.md) — [Discussion](https://github.com/orgs/stellar/discussions/1602)
- [CAP-0065](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0065.md) — [Discussion](https://github.com/orgs/stellar/discussions/1615)
- [CAP-0066](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0066.md) — [Discussion](https://github.com/orgs/stellar/discussions/1585)
- [CAP-0067](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md) — [Discussion](https://github.com/orgs/stellar/discussions/1553)
- [CAP-0068](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0068.md) — [Discussion](https://github.com/orgs/stellar/discussions/1626)
- [CAP-0069](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0069.md) — [Discussion](https://github.com/orgs/stellar/discussions/1633)
- [CAP-0070](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md) — [Discussion](https://github.com/orgs/stellar/discussions/1719)

<details>
  <summary>Video Transcript</summary>

[00:00] Great. hi everyone. I think we have bit of an issue here without clear your agenda in terms of who is supposed to speak. I guess the purpose of today's meeting was just to revisit the CAPs, that we have before the final approval and the agenda suggests, that the CAP authors would present them and I think almost all the CAPs have been discussed before besides [CAP-70](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md). So maybe. If parent is here. Yeah. Maybe you could talk about [CAP-70](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md). Because it's completely new. And then we could I don't know I guess quickly recap other CAPs

[01:00] and there any questions concerning any. Any. Cool. Yeah, that sounds good. So I guess getting started with [CAP-70](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md). This one is a recent addition to Protocol 23. But it's fairly small and straightforward. And. So essentially what this does is it introduces a couple of new network config settings, that control some of the timing parameters around consensus. So essentially today like during consensus there's a couple of hard-coded values such as the block of the ledge or the length of the ledger being 5 seconds as well as some of the more nitty-gritty details on consensus as to like how long nodes wait for certain rounds like timeouts between certain consensus rounds and things like this. And. So essentially this CAP doesn't actually change anything. But it just allows us to change these values in the future. So for instance today Stellar Core all the

[02:00] nodes are hardcoded to have a 5-second ledger time. But with this CAP it allows, that to be dynamic and configurable. And. So kind of the motivation behind this is, that as we want to move towards you know higher throughput and lower block times making you know large jumps is difficult. Going from 5-second blocks to 2 and 1/2 second blocks is very challenging. So the goal with this is to allow us to kind of make small incremental changes via SLPs where you know to get to, that goal of 2 and a half seconds or whatever it may be you know we start by just trimming off like one or 200 millconds at a time and over time we'll get there. And so essentially this CAP introduces all these new config settings. But the default values are the exact same values, that are currently hardcoded into core. And so [CAP-70](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md) will have no noticeable change. When it's initially implemented. But just opens the door to have more configurability and hopefully tighter iteration cycles as we go to

[03:00] improve consensus and nomination timing in the future. Yeah. So, I think it's pretty straightforward and probably not super controversial, but. If anyone has any questions or wants to talk about, you know, feel free. Yes have a small question, which I kind of already asked in the. But basically is it possible like for example for seran network settings it is relatively easy to understand what every settings means and you know. If you're for example increasing the prov limit for instruction like 100x it is kind of easy to spot, that it's probably not going to work. Because we will take 100 times more time to close the ledger. So my question here is how risky is this set of parameters? Is it possible to accidentally v

[04:00] for an upgrade, that breaks the network in an unintuitive way. Because we're kind of altering the consensus algorithm, which on its own is used to well do everything on the network such as upgrades. So unbreaking the network would be tricky. So you know how risky it is to break the network. Yeah. So I do agree with you. These are pretty kind of you know low-level technical settings. I think kind of the reason it makes sense from a technical standpoint to be config settings is, that these are things, that we want to change often and kind of slowly over time to you know make small incremental progress and all the nodes have to agree on them. So it makes sense to be conflict setting. But they definitely are difficult to understand. And. So I think as far as validation pretty much the only tool, that we have is supercluster and kind of simulating the network and seeing the effect these values will have. And they definitely have effects, that are sometimes not

[05:00] super intuitive. And. So I think what we might want to do is at the especially. When we just launched these features is to put tight bounds onto the acceptable values. So for instance like. If today the default value for you know electric close timer is 500 or 5,000 milliseconds or 5 seconds. If you were to change, that to 50 milliseconds the network, that bad things would happen and the network would be very unstable. And so perhaps what we can do is given, that at least my thinking the goal with this gap is to make small incre improvements we could have very tight bounds just at the you implementation level. And. So say for instance like in Protocol 23 we could say, that the lower bound on the ledger close time is like four 4 seconds or maybe like 3 and a half or like, that. And. So worst case you know. If someone

[06:00] was to propose like a 50 millisecond change it just wouldn't go through. And you know even like these lower bounds I think these would be like kind of guesses at this point. Because there's still lots of testing and validity checking, that needs to be done on these values. But I think like you know. If you go from 5 seconds to 4 seconds, that might still cause some network instability. But I don't think you would break the network in the same way, that you would. If you went from like you know 5 seconds to 50 milliseconds. And. So would something like, that maybe some tighter bounds alleviate some of your concerns? Yeah. Yeah, that makes sense. This I still feel it might be a bit scary. But yeah I guess we can try make sure, that maybe one thing I thought of. When you were talking is, that maybe we could limit the upgrade not only in absolute sense. But also in relative sense, that you cannot reduce by more than

[07:00] x% the current value something, that so, that yeah we don't need to guess too hard. It just kind of enforces incrementality. Incrementality. Yeah, that's a good idea. I think I mean like relative values like might have some potential issues like. So for instance I think I like the limited bounds a little bit better than relative changes. Because you might be in a scenario where for some reason the network is broken either. Because you've you know changed limits in some way or maybe, that there's like some sort of like a thing, that's external to these parameters, that's gone wrong, that also needs fixing. So I could see a possibility where due to the network being in a bad state, it would be advantageous to make large changes to these values. And. So I don't know. If the relative sort of

[08:00] safety measure is as safe as it sounds from like a disaster recovery standpoint. But I think limited bounds at the minimum would be good and like even for like the relative you know limit the relative limitations. The issue is, that for some of these values you can you know do damage going in both directions right. So like you know increasing nomination timeout may improve performance. But you know. So what I mean by this is, that like these parameters it's not, that like you know the network gets more unstable. If you go you know one direction. And then less or. And then more stable. If you go the other. And so I think you. So having relative bounds might be a little tricky. I agree like the yeah having bounds in general is just absolute bones is probably the way we want this. like I think we may want to have

[09:00] those be more like some sort of like config settings. Because I could see like for as the goal of this is to understand it in you know like in the context of like we want eventually to move those in the on the public network have like the same defaults. So, that public network yeah is like it would take basically many you know many things to be overridden. For for a bad vote to be accepted. But in a other environment like test net or a test cluster maybe you want to actually see what happens. If you set those things to you know like half a second or whatever. So yeah. So I think it's the type of thing, that we should do on the thing I wanted to add, though to this CAP is, that yeah it's the it's only the first building block I mean or the first change I think I

[10:00] imagine, that is needed to really move on the latency front. Because I can see certain things like, that are going to be impacted as. If HD this runtime gets too small cuz today we have for example the a lot of things on the network, that are tied to like this 5 seconds. So like the thing I can think of is we take snapshots every 64 ledgers or the how fast do we spill from you know one level to another. Those things maybe need to change as the block time gets goes down and right. Now they are not configurable. of course changing the u speed of spilling is a much more involved change than changing the runtime. But, that's I

[11:00] think something, that maybe we'll have to think about. Yeah. And I think, that's one of the things, that makes like this is a complex thing. Interesting is, that you know lowering the block time is down is bound to break something. I mean you know we can test dollar Stellar Core and simulation environments and stuff and even. If you know the actual like layer one doesn't break downstream like something will break somewhere for sure. And. So the hopefully. If you know shave 100 milliseconds off the time you have you know one thing breaking at a time in minor ways whereas. If you went from like 5 seconds to two and a half I'd imagine many things would break all at once, which would be sad. I have more questions. But the only comment, that I have is, that again this is a similar property of all the settings. But since there is no like protocol version bump attached, that doesn't, that means, that network May

[12:00] run on different versions of the core software, which means, that the plan of like doing an upgrade based on some minor release changes may not be necessarily sustainable. Because it's kind of tricky to get everyone on the same version of the software and I guess this may like we can try to make it happen. But still imagine the like major changes this direction would still need to be tied to like major releases probably. So I'm understand config setting. It's just an observation, that even, though it's a config setting, we might not be able to change it too often. Often. Yeah. And I think, that's why I'm kind of in favor of like a runtime bound limits. I think Nico might have been hinting at like having like a config setting, that's like the actual value. And then an additional config setting for the bound.

[13:00] . But I think like to Dimma's point, you know, like a Protocol 23 package, it's it should be known, that like, you know, the 23.0 package probably can only run within like these timing envelopes. And. So I think, that might be what, what's interesting, is, that, you know, even, though technically it's a config setting, what we might do is, that we can only, you know, even, though the bounds are, part of runtime or, you know, they can even be part of the protocol at this point. And. Then like say the bound like the lower bound is 4 seconds in P23. And then the lower bound in P24 could be like 3 seconds. And. So you don't change the actual value on the P23 to P24 upgrade. But you chose you change how low the value can be. And I think, that might be the best way to kind of think about these settings is, that you know there is the bounds can change as you know our latency on ellipse gets better. But the actual value is dependent on the network config settings. Yeah. And I guess to, that point like

[14:00] since bounceration yeah actually not sure. If you can make some notes to disagree on the upgrade validity maybe yeah anyways, that's not super important. But yeah something to I don't think this really needs to be part of the protocol I mean I think this can be like a safety thing where you know there's like a contract, that the upper and lower bound are just you know constants in the C++ code similar to how the upper and lower bounds for other config settings are and there is just like a agreed upon you know contract, that you change these bounds on protocol boundaries I guess there's a question chat. So do we run the changes on the super cluster. And then provide the validators input on any hardware of AM upgrades in a timely manner I guess can answer yeah. So I think yeah I think the answer is

[15:00] yes. So we're doing lots of supercluster testing and I think part of this work too is, that I've been kind of running you know some one-off tests myself of late and I'm not sure. If the default values on mainnet today are actually the best values for the network as it exists. In particular the 1 second nomination timeout value and the 1 second ballot nomination or ballot timeout value are kind of like hard-coded magic constants, that have been in core forever, that we haven't really questioned. And so I think part of this. While not part of the actual you know CAP itself is you know after we've released 23 doing testing both in supercluster and you know intestance and things like this and finding out what the true value of these things should be I don't think you know ledger close time I think, that one's you know pretty set at 5 seconds. But for these other values, I think there probably is some experimentation to figure out what the actual correct value should be in Protocol 23. And then of course in future

[16:00] protot figuring out the correct values are there as well. I have a lot to say. The intention is not to you know I think to answer those question more specifically to not use these config parameters to like force hardware changes or like force validators to you know upgrade to beefier EKS. I think, that's a different conversation. But. So so I think yeah we'll definitely test and the goal is not to use these parameters to require you to you know buy more expensive boxes. Right. And guess. If there are no more questions on this topic, we can make a quick round up propos this protocol. This is what this med is supposed to be about. Yeah maybe can I suggest like yeah not spending a whole lot of time on each CAP and it's more like. If there are CAPs

[17:00] that maybe had like some significant or some changes in the last you know last month maybe we can briefly talk about, that. But otherwise yeah like just yeah exactly, that was my plan basically what I wanted to start with is quickly go over CAPs, that haven't changed and I, That's the only thing the only action item, that remains with them is just to approve them I guess. So no particular order [CAP-65](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0065.md) reusable module cache has been both finalized a. While ago and also implemented and merged. No changes there. There. Then. Then parallel sorry memories. Okay. [CAP-62](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0062.md) and 66 about state

[18:00] archival and in memory resource current are still in implementation progress. But there were no changes to the CAP semantics as well. So please stay as they are. Are. CAP. Yeah. [CAP-63](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0063.md) prior proism friendly transaction scheduling. again no changes. Changes. Protocol part of the implementation work is done for the most part. The actual parallel execution is work in progress. But again no substantial changes

[19:00] there. There. CAPs [CAP-70](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0070.md) has just been discussed. [CAP-69](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0069.md) new host functions hasn't been implemented. But there were no new suggestions there and what remains are [CAP-68](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0068.md) and 67 and I guess for [CAP-68](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0068.md) there weren't any changes themselves as a CAP about host function for getting executable for a contract address. But I know Lee is Lee here might not be here. We had some concerns about necessity of this function and we could maybe work it out during this meeting. But yeah, since we doesn't seem to be here, I guess we'll take this offline. But in

[20:00] any case, I don't think there will be changes to this cup. Like the worst case is, that we'll just not do it. If you think it is not necessary. And I guess what remains is [CAP-67](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md), which is the unified asset events. And there has been a lot of discussion about this CAP and pretty sure there were changes to it recently and the changes were basically ironing out some edge cases. Specifically specifically okay looking like working backwards from the history what we did update is, that we completely removed from max ID from all the classic events. So for some context

[21:00] originally CP 67 proposed for transfer events emitted by the classic operations this [CAP-67](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md) and by sban token transfers starting from prole 23. Initially there was a proposal to allow multiplexing for the transfer source. But since it leads to a number of inconsistencies and is generally not something super widely used basically it's like a transfer where you want to know, that some virtual cast account has initiated it even. So I still need to sign for it. This kind of a weird use case and I guess the outcome of the discussion on all the edge cases this comes with was, that let's not do this for a. While until at least next

[22:00] protocol and let's just focus on the minimum things, that we definitely know is useful, which is to multiplex idea and we know this is useful. Because this is how exchanges typically identify their users, that represent a non- custodial accounts. So they can attribute token deposits to their owner database. So this is one change. another change is actually what makes this CAP still not ready for the final approval I guess is the change to how we emit the information about the fury funds. So again from the previous discussion the current status is, that it will emit two events called fee

[23:00] and the first fee event is what we charge initially before applying the transaction and the second event is also a fee event. But with negative value and it represents a refund. So no and Soroban refund there is refund both portion of the resource fee. So whatever portion of the fees, that hasn't been used will be funded after applying the transaction and this is what we have in kept now. What we still haven't converged on is whether we want to add some additional attributes to this fee events, that tell the consumer of the event. When exactly did this charge appear. Because fees are charged before all the transactions are applied. So it's like not a part of the typical transaction

[24:00] flow and fee is refunded before Protocol 23 it is refunded after the transaction is applied and starting from Protocol 23. If you will be refunded after all the transactions are applied. So there is this annoying inconsistency with all the other events. Because all the Azure events happen during transaction being applied. Fees happen before sometimes after all the transactions were applied. And I guess the outcome is, that we might alter the MXR slightly to add some information on what is the timing of the FE to kind of make this more obvious. But yeah, I'm still not sure. If we have converged this value. So this is something I guess we'll try to prioritize and finalize this CAP. Yeah, it

[25:00] is yeah I guess not a major is happening here but, that's lots of small pieces, that it keep coming up. So kind of it. So I guess, that's the only CAP, that is still not fully finalized. Finalized. And I hope this timing will be the last small, that we kind of clarify. Yeah, that's pretty much it. Right any questions from this camp or any other CAPs? But again as I said before we proer CAPs are no changes.

[26:00] I guess there are no questions. So I don't know anything else to discuss it. Yeah. I was curious. If yeah. If if there was like any feedback on the restriction, that were put on the memo from anybody. Okay. Yeah, I don't see any feedback. My my concern generally is, that this stuff may not be super discoverable.

[27:00] my suspicion is, that. If we make some incorrect decisions in terms of event layout, we'll learn after we have launched [CAP-67](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md). When people start actually consuming the events and find out some use cases, that are not covered. To be frank at this point, I think we should just converge on something, that simply works. 95% of the cases and yeah. If we find some issues after it has been launched we can address them in the future protocols the good thing about ma events is, that part of the protocol. So we can issue a quick fix some information is missing or not necessary or not represents a value as it should. So it is fixable and I think a [CAP-66](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0006.md) to

[28:00] 67 is in a reasonable scale trip. Now and it's too much room for centralization. Centralization. So I mean we are obviously open to the feedback. But yeah as I said objectively like I feel like at this point like we got in such edge cases yeah not sure. If it is clear enough for anyone who's who was even going to consume this events I don't think it's clear enough for them. But things are happening or Okay, can see typing. I've been talking a bit about [CAP-69](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0069.md), the getter for the address executable. Executable. yeah, for [CAP-67](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md), the timing of

[29:00] the fees. I don't think this has been finalized. We need to finalize this I guess outside of this meeting. I think you're almost there. We can just add the timing email to the transaction events. But yeah, we still need to make the respective update and post some discussions thread I guess. So what I wanted to talk about is [CAP-69](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0069.md) I get for the address executable I know, that you had some comments on this and there was some amazing discussion. So maybe. If you wanted to talk about this like you're concerned about it in general we could go on the stage

[30:00] Yeah, I think I don't want to read just rehash what, we discussed about I think I was just like a little bit fuzzy on it. second I'm just trying to find the cut. Sorry, what was the number?, it's 69. Oh, sorry. 16. No, that's Yeah. Because we split it. We we, That's right. We split up into

[31:00] two discussions. So I guess I kind of like here is my point of view on this like I kind of understand some of the concerns about like the use cases for this, that say not as well defined but, that said just from the API standpoint I don't have like what I'm looking for is. If there are concerns for the API itself like. If someone thinks, that it is not the information we should be exposing at all. Because if there are concerns like, that then, that's valid we could discuss them. But you know. If you're like trying to understand like the nuances of the use cases I like I feel like we can fulfill some of the

[32:00] use cases at least and we can work on more advanced tools ess. Ess. Yeah, I think like the API, that's proposed is very reasonable. I think it's the question is what you just highlighted in the is it is there a reason not to expose this information? And it's sort of it reminds me a little bit of the. When we were looking at what APIs to expose and what ones not to expose. One, that we chose not to expose, that other blockchains do and Ethereum does is the concept of a message.cender. So like I think you know. If there was a proposal to add message sender to Soroban it would also look sort of similar to this one. It would look very reasonable. It sort of makes sense as a primitive, that a contractor would have access to. But we made an intentional decision not to

[33:00] include it in the original you know CAP 46s. Because in we have seen the effect of it in other ecosystems where you know it can be used inappropriately you know in a way, that it's like a security issue. and it sort of it provided a view into who's calling you. as opposed to like what the or framework, that we've designed is like a better safer way, that abstracts the concepts of like accounts and contracts. So yeah, I guess, that's, that's where I'm coming from. That's where my concerns are coming from. And. When I like. When I read through these motiv the motivations in here, I think I understand the motivations. Now but it doesn't really completely address those use cases. So you know. If you want to pin the exact

[34:00] implementations yes it does work for direct dependencies. But not for you know transitive dependencies. So as a primitive, it sounds like it will fit some use cases. It's unclear. If it's completely solving those use cases to me. And. Then yeah, I just have this concern as well of will this result in patterns of use, that harm interoperability. yeah, I agree on the parent's point. I'm not sure. If transitive dependencies necessarily like this have I think two concerns, that are kind of incompatible right. Because like. If we went down the pinning road. Then like and the concern is, that the pinning is not strong enough and we should also allow for transitive pin. Then we are actually going deeper

[35:00] into the whole like no interoperability. Because not Not only do you like pin the direct call. But you also want to pin something, that happens in the like as an implementation detail of, that call. honestly, like the harm well the harm impro interability piece. Sorry, that's I think there's like a few different, that's not. So much about the pinning. It's more to do with making contracts aware of say the Stellar asset contract versus another token. And then writing behavior, that you know is explicitly different for the two. Two. Yeah. I mean I kind of guess it's concern like this asset contract specifically say you can do this right now. If you want to. You need to like do a bit more work

[36:00] than do, that. So given, that like for the site versus non-sack like I kind of get the concern. But we cannot really hide the either. So I don't know like yeah not Oh you're saying you can do this today. You can already do this today. Because you can go and get the name. Yes. And you can encode the XDR. And then verify. If they match the same. That's a good point. Right. So. So basically like here the trade-off is not between like whether like this is faster or not. The trade-off is like. If people want to do it, should they keep doing this in a hacker, less efficient way? And you know, whether their use case is good or not, like they still miss themselves into it. Because they kind of messed up with extern versus providing something, that at least they kind of know works. And yes,

[37:00] you can build something better out of it. But you know, on the other hand, like there are different ways of kind of sensoring, which contracts can interact with your contract. And this is actually something, that people have been requesting and this is like how stuff like string key contract ID hard coding might end up working and people kind of keep coming up with this idea of like what. If I want my contract on the interact with for example XLM or something. So I don't know I feel like you're kind of past the point where you could make things truly abstract. So I kind of get this concern. But yeah at this point I feel like for the most part like much about it this guess you can get any of any contracts at least you can hard codes and contract IDs.

[38:00] Yep. Okay. Yeah. I don't have anything else to add to it. I'm not like strongly opposed to it. They were just my concerns. So I get the benefits and yeah I think it looks good. Thanks. Sounds good. Yeah. I mean yeah I'll think a bit more about this like and yeah maybe double check with other folks. But yeah honestly like I feel especially for something like transitive dependencies like this is I think this actually strikes reasonable balance. Because like you still can achieve, that. But you need to make this explicit and making this implicit sounds like a bit sketchy. Because technically you shouldn't have control over what other contracts are doing. So, I don't know. Yeah. I guess, that's

[39:00] kind of was it more or less. Yeah, I'll think a bit more about this. But hope should be more or less on the same page here and should be good to go with this kind. Great. I guess it should be truly for today and there are no more concerns. Thanks everyone. Yeah, thank you. Later the next protocol meetings and...

</details>
