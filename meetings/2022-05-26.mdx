---
title: "Open Protocol Discussion"
authors: [justin-rice]
tags: [legacy]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="NF8sWOTbyWI" />

- okay so i think we're gonna get started and uh hopefully david will soon join so hey everyone welcome to another protocol meeting i'm filling in for uh justin today uh i am seeing this so in these meetings we discuss potential protocol changes these changes are outlined in these documents called caps or advancement proposals and the big change we're working on right now is project jump cannon which are which is a feature to introduce native smart contracts on stellar so we've divided this massive change into a set of composable caps um and the agenda specifically for today is we're going to talk about cap 53 smart contract data this was recently published by graden uh we're going to talk about cap 52 smart contract interactions minimal which was recently introduced by john and we're going to talk about the smart contract life cycle cat 47 which was recently updated by siddharth um and uh requires some um for the discussion so uh let's do this uh graded can you kick this off with a review of cap53 yeah just give me one second all right got it yeah uh sorry i just i had the pull request open and then i actually wanted to switch over to the merged version of it so this is a fairly straightforward cap it's not um it's not really introducing anything that probably will be a surprise to anyone here uh it's just formalizing something that was left out or left sort of for future caps in the modularization that we've been doing splitting off conversation into different pieces so that we can work on them separately and land them separately but it's fairly tightly related to the data model that was presented in cap 46 so a lot of the motives in cap 46 uh around the data model implicitly talked about how uh how that data would be stored permanently so there's there's there's concerns that that bear on the data model that are uh sort of interacting with it while it's in memory and then there are concerns uh more related to its long-lived uh accessibility over over uh multiple invocations of a smart contract uh well it's stored on the blockchain so um some of the requirements uh that are sort of rephrased and and brought into the foreground here uh have to do with interoperability where we we want there to be something a little bit a little bit more general um or i say uh sort of a less general more interoperable more generally understood more widely understood structure to the data than just a byte buffer a lot of smart contract platforms essentially only provide a byte buffer storage service to smart contracts which means that nobody really accepts that exact version of the smart contract code can necessarily read any of the data that's stored there and that that produces interoperability problems uh if other third parties want to access it offline you know browsers that want to take a look at the data it also creates versioning problems because it means that you're originally locked to the schema language or serialization format that the contract used uh it means the contract wants to pass data it has to from one contract to another it has to transform it so there's a whole interoperability angle here uh which we wanted to address in in the cat 46 data model and we're carrying that forward here and i think a lot of the the concerns only make sense uh when considered in terms of persistent data but this is the persistent data uh cap so so uh here we're just talking about basically what the ledger entries is gonna store things looks like and a handful of host functions for for accessing it they're very basic functions they're just key values to our access functions they're point access functions they're not range range functions they don't include uh iterators or range queries or anything like that they're just get put does do we have a key and delete the key very simple uh host functions the interesting thing really is the choice of granularity which is left to the user so this is a little bit different in in that um many blockchains provide a this is this is different than many many smart contract systems you'll see many smart contract systems provide a key value store which is keyed by a byte string or some some kind of a prefix that goes into a merkle tree or something like that um we do not have uh we essentially don't expose any interior nodes of the mercalli storage that we use uh to use this anyway we just provide a single bucket list hash the fact that our data structure has internal localization doesn't really apply here anyway um and structurally it wouldn't make sense but um what we're what we're doing here is also allowing uh structured values as keys rather than um rather than byte string so of course you know you can serialize any value and you will in the case of using it as a string here but the api is encouraging users to have fairly structured values so they can have fairly rich keys um and implicit to all of this is that there's there's a parallel access and consistency model uh that's discussed in this cap um which is that we're we're trying to encourage the possibility of executing smart contracts in parallel and if you have parallel access to uh a data store you have to talk about what the consistency model is what is it what does it mean when two different users access that that model in parallel um so we're specifying here that it's a serializability consistency model uh which is the strongest possible it says you know equivalent to the exact order that the transaction sets uh specified the transactions executing in that has to be the observable side effect model and parallel models uh parallel consistency models uh imply the existence of some kind of a concurrency control mechanism how how you actually enforce that uh and in in this cap we're talking a little bit about um a very strong uh mechanism for concurrency control it's what's it's what's typically called deterministic uh scheduling or non-conflicting concurrency control the idea is that every uh transaction that enters the system will pre-declare a footprint so there's this thing called the footprint which is the the set of keys that a transaction is going to touch whether it's going to read them or write them it actually marks whether it reads or writes each key in its footprint and the footprint is is static information that accompanies a transaction so this cap doesn't describe exactly how a footprint is encoded or accompanies a transaction location because we don't even have a cap open right now that has transaction invocation or at least we haven't settled on one we have several caps open right now but when transaction invocation uh occurs it it's going to need to provide a footprint uh in in this cap this cap is asking that that uh footprints are available and the footprint defines um the keys that are uh that the transaction is allowed to perform these data access operations on so if you try to perform a get again something that's not in your footprint uh the get will fail even even if the the value is there if it's not in the transaction's implicit footprint um it's defined as failing uh similarly for a put or even has a point query anything like that um you have to have it in the footprint so so for simple transactions this is fairly straightforward you can tell what they're going to read or write and so you just put things in the footprint that they're going to read or write that's fine for complicated transactions that have a highly dynamic behavior maybe maybe it's not even clear what they're going to read or write because it's it's you know subsequent to a transaction uh it's it's determined by an earlier read in the transaction these are what are called uh transactions with dynamic footprints um the recommendation uh in this in this gap uh and what we what we're prototyping is a fairly standard technique from the literature um which is often called reconnaissance queries uh i think i'm using them here i'm using the term uh recording footprint recording here which is that you just run the transaction offline uh before you submit it on on a read snapshot and that gives you a a fairly good guess and an approximate footprint that you can then staple to the real transaction when you submit it and uh it will succeed if that footprint still matches so uh it essentially pushes concurrency control uh out of the the transaction processing loop and uh into the user's lap and so the the user is now racing on divergence between read snapshots that they use to construct their footprint and the footprint that they actually submit a transaction with and so theoretically if there's a very very highly contended key and it's a very different query they may have to retry multiple times because if they get if there's any significant divergence between the the recorded footprint and the footprint they submit uh their transaction could fail but the database itself doesn't have to actually perform the concurrency control so in some ways this is shedding load from a concurrency control mechanism inside of a database out to the users um and that has turned out to work very well for for maintaining a very even high throughput um on existing databases that adopt this technique so we're trying to adopt that technique as well so those are the the two sort of main topics in here that the fact that the user has control over granularity i should go back and talk a little bit more about granularity just for a second which is that the the granularity control that exists here um it has a natural tension in it so it's so doing a point read on a key value store necessarily has some overhead it has it has data framing overhead it has serialization overhead it involves going to the i o system at all it involves touching the disk doing a seek doing a read all of that overhead is um potentially quite high and and so it's it can be worth trying to amortize that overhead and read more than one item if you are going to access more than one item you don't necessarily uh want to to pay that on on a bit by bit or bite by bite basis you want to bring in a bunch of bytes at a time when you do an i o and so that amortization tension pushes you towards larger ios in a larger granularity of storage but then the flip side of that of course is that if you read or write data that you don't actually need if it's if it's wasted and you actually only wanted to change one byte in the middle of a large data structure that's waste and you're paying for that waste uh in terms of you know fees or cpu time or io or whatever so that that pushes you in the opposite direction of having fine grained data and that problem actually just magnifies itself when you start talking about parallelism because again your footprint is a unit of contention and so if two transactions contend on the same data value um they can't execute in parallel basically that's that's that's what the footprint is is doing is that it's giving a static scheduler the opportunity to partition execution into separate lanes and then those lanes will run with no coordination but those lanes necessarily are serial themselves you only get parallels between them and so if you have a whole lot of transactions for example that all touch some common data value in their footprint they will all be scheduled to run in serial and so to exploit parallelism it is it is in the favor of the user to have a finer grained uh footprints so so you have this sort of two different directions of pressure it's a natural trade-off between fine-grained and coarse-grained uh data access and so we don't specify what what the granularity is here we try to be very open about that and and so um that's why the key type is is literally just an arbitrary value um i think that's all i really had to say about that there's not a lot in this in this gap it's actually quite small and it it kind of just does exactly what you're expected as a key value type awesome can you um just quickly talk about the rationale for why there is uh point access only uh yeah so um range queries essentially aren't compatible with static footprints um because you know we don't we don't know how far they go um that's that's the simple version um we we could theoretically but we would lose parallelism so yeah awesome so we are kind of like actively trying to deter um you know contract developers from creating these like um um you know creating like a need for rangers with some you know like for example like a classic or the book is probably like not a great fit for this uh which is okay with us yeah and i mean there it's a good point which is that in in in in a broader sense um a static footprint actually bounds the i o you're going to do it allows us i mentioned this in in the contract it it it allows us to essentially have no surprises the contract is not going to be interrupted in the middle of the contract in order to actually go touch the disk dynamically everything that it's going to read it says upfront and therefore we can just do a bulk read at the beginning of the contract um just in fact we'll integrate into a single pass through the the storage system uh all all of the reads from all of the contracts in a given parallel execution lane will just read all their data at once at the beginning of a transaction set execution and then write it back at the end so that that kind of thing is is naturally incompatible with something like dynamic range queries but that said uh because you can store you know arbitrary values if you want to store a map um that's that's completely reasonable one of the values that you store it doesn't have to be just a small string or a number or something you can store a map that has a bunch of stuff in it uh and then do a range query on that map it's just that when you do an i o you're going to get the entire map is going to come off the disk so you have to sort of navigate that trade off yourself maybe shard your map into a bunch of different sub maps or something like that if you're interested in not loading and storing the whole thing every time when i tried to uh to use a very early version of this design like a month ago or something one of the kind of like um annoyances that i quickly encountered was like when i wanted to you know partition my namespace i basically was like okay well i need some kind of key that is a tuple and so i just used like the sc valve option uh and i just like pumped a vec full of stuff and then use it as my key that's i got like a petition namespace the thing was like doing that seemed like kind of inefficient because it's like okay like i need a three like you know a three tuple as my key so i like go i create a vector host function i push into it post function i push into it again host function i push into again host function uh and then i call the like you know get function host function again and it just seems like a ton of work to get a single piece of data so do you have any thoughts about that um off hand i guess i'm i'm i'm not sure that it is a ton of work like it would would be my first reaction in the sense that um i don't know so so so for example you know we could we could make a contract put one contract put two contract put three that takes three values as inputs three three keys four keys five keys you know we could we could reflect those usage patterns in function signatures as conveniences but i'm not sure they would do any less work and i don't think the calls in and out of the vm are actually all that expensive i think you're only talking about one extra op code and a couple of like like a push and a call so from a user perspective i think you have a good point and um i think if the sdk can't make that pattern fairly convenient uh in terms of putting you know uh sort of a superficial uh porcelain on top of it that makes it makes it look nice then perhaps we should expand uh the functional repertoire to provide additional support for that i think one thing you might be able to do just responding to your comment about the sdk is you know make it easy to use things like tuples um like trying to make it easier to use things like vex already um and maps uh you know tuples might just that just might be one thing we could we could have yeah that's that's kind of what i'm expecting is that you can you can do the the kind of thing that that um you know i hate to use this as precedent but but the the raw standard library does something similar here where it just says you know the people use tuffles up to about five or seven or twelve or whatever so like it just you know macrogenerate enough support for all the basic temple types that anyone's like likely to use and and just have them as conveniences and then you only wonder how to use this arbitrary vector sort of approach if you're if you're doing something weird yeah i mean as as long as the cost of doing all the push functions isn't particularly high then it it doesn't really matter to me because you could always put sdk support to do this indeed i just built my own thing that like i could pass the functions to and inside the parameters too and it would uh through the vect back out at me so it would look a lot less disgusting um we could generalize that of course as long as the cost isn't high at the protocol level so i think so i think the cost of a function call is is fairly small uh function call and uh i again absolutely it's the case that if if we measure this and it's miserable i mean the other thing is that i don't i don't honestly think there's masses of i o operations in the normal contract path right i think you're only talking about a couple of point accesses per contract call anyway so i'm i'm not super concerned about that path but if if if we measure it it turns out to be expensive we can absolutely revisit this and try to you know add fast pass or optimize versions for this braden d do you think they could be contracts that will be vulnerable to moving footprints so you mentioned about the situation where dynamic footprints is an inconvenience or you know you have to do reconnaissance queries and they potentially could be out of date but i'm wondering if there's an angle here where that actually makes the contract vulnerable in the sense that you know one participant of that contra contract could prevent another participant from interacting with it yes absolutely um this is as far as i can tell this is basically always the case with concurrency control if you have any kind of concurrency control mechanism somewhere um you can create a starvation you can even you can survive one party by by just hammering on a contended resource in this particular case the user has the contract developer has a fair amount of control over it because they can change the granularity so if a contract developer feels that this is a risk or sees this happening or something like that they can re-architect the contract to essentially sacrifice concurrency um to get rid of the ability to have that kind of concern so so you know at the extreme end your footprint is the contract data there's only one contract data everyone who talks to this contract always accesses the exact same contract data and that means that everyone knows exactly what their footprint should be it's always just the contract data there's only one element everyone specifies the same thing and they all get serialized and so there's no you can never you can never have to to have your footprint invalidate because your footprint is always correct um so you can do that if you find that's happening it's just the worst case right so you you move away from that if you want more more concurrency but if if you're seeing that people are able to um able to and and actively exploiting you know some kind of of starvation uh concurrency starvation situation then then you may have to move back towards that and i don't i don't personally know a way to avoid that i think if we did any kind of dynamic locking we would be in exactly the same situation where someone could just um flood the system with with uh transactions that take a lock and and deny anyone else the ability to make progress and we would be in a worse situation because dynamic and currency the the thing that's really good about static and currency control as an approach is that you um you have a guaranteed sort of uh throughput that the the things that you have scheduled um will complete one way or the other within their allotted time slice right they will they will either finish or they will abort um and so your abort rates go up but the system continues to run in uh in this particular strategy the other strategy would be more like um we would give people the ability to drag the entire system down so a transaction set uh would would potentially uh slow down dramatically because people are contending on a hot resource so it's there's there's there aren't a lot of free lunches in concurrency control and and i i kind of feel like that's a natural trait of you that answer the question i know it's not like a fun answer no yeah i think i think that makes a lot of sense the fact that the proposal really gives the contract developer a lot of control um and doesn't define the level of granularity that they have to use um i like yeah it's good um well great it looks like you've created one of the least contentious caps that has ever um came to life thanks for that great um cool so um let's let's dial up on the contentious or the contention levels uh john i think this is probably the third iteration of uh smart contract interactions um so for those of you who are here at last week's meeting you might recall that we had a pretty big debate about this and um i was in the interest of actually agreeing on something i decided to just remove all the functionality from the proposal which sounds a little backwards but um in the context of smart contracts you can kind of put all of these authorization questions down to contracts and let them do them do everything themselves and we had been kind of moving in that direction on cap 50 anyway with the introduction of like the invoker signature none option and stuff like that so basically at a high level of what's in this proposal there is a just like in the old proposal there is a new operation sorry a new transaction type and a corresponding envelope type called uh invoke contract transaction or invo contract transaction envelope and this and this contains the normal stuff like before uh source account sequence number fee it contains the contract that you're invoking you know the ID the parameter the symbol the parameters it contains the read write set that you would need for cap 53 as grading was just talking about um and then uh that's pretty much it so how does one actually use this there are some examples in the cap which were pretty instructive about what what the universe would look like if we actually did this um there's a whole example section where i hacked up like two versions of an erc20 type contract but they look quite different from your normal erc20 because there's not really like a reliable message.sender that you can use in this context um but uh basically the only other thing that's here is just a few um a few host functions that are useful for actually doing some of the things that we discussed last week um so there's some access to thresholds um from accounts there's access to getting the signer weight by account key by signer key the signer weight for an account by signer key kind of hard to say um i added a verified 25 519 function which i think is also in cap 51 which is being written at the same time and um that's uh that's pretty much it so um any questions about this um so i'm i'm just uh uh just trying to get into this can you talk a bit about like the implications around like accounts and you know we were talking about about this a bit last week um like what does this mean for like you know classic multi-sig accounts on the on the smart side uh the beauty of it is that the proposal means basically nothing for those things um because the contracts get to make their own decisions so for some context like if you scroll down almost all the way to the bottom the last example is like a sim a simple token based on account signatures and this builds on the classic stellar multisig mechanism um basically works exactly the same with two exceptions no pre-signed transactions here oh sorry no pre-auth transactions and no hashtag signers but it works exactly the same and it works at medium threshold and everything kind of is exactly what you would expect um but right above that there's another example that uses the like the single key version that i was proposing as the invoker signature last week um and so this this framework basically lets contracts build whatever they want um if you want some kind of uh you know support for seller multisig that'll really be up to contract developers and ecosystem standards and stuff like that my intuition is that those things won't really materialize because they're not efficient structures on the blockchain but they might they might materialize from case to case something that immediately jumps out at me with these examples is that it may be difficult to write these functions um some of these sorry i'm looking at the simple token based on account signatures example you were just referencing and the second code block has a check function it says internal function is that something that the network is providing or that's an internal function that the contract provides that's a contract function that's not exported so i can imagine um so i've had to write code like this for um for our sdks when we were implementing sept10 and i was yeah i think it was septum and one thing that um is quite difficult to get riders iterating over a set of signatures for a message and determining a set of weights because there's different things you have to do like you have to make sure that you don't if somebody includes the same signature twice you don't use it twice to get you know double the weight different things like that um so do you think by going this approach expecting people to implement their own authorization we're increasing the chance of foot guns where people are going to implement what they think is stellar maori sig signature authentic authorization verification but it doesn't actually exactly line up with it that that's definitely possible uh i mean my like kind of ideal universe here in the sense of like what i hope people would do is probably somebody would deploy one contract that has like a unified key structure a unified signer scheme basically you pass it some kind of opaque blob the beginning of the opaque blob is a discriminant saying like hey what kind of signature is this is it a single key ed-25519 is it a single key ecdsa is it a um is it like a stellar multi-sig is it some other scheme that i'm not thinking of like some kind of like quantum reset resistance scheme who cares um and basically like the entire ecosystem relies on this contract or sorry another example would be like i know lee you had requested like aliases um we could have like a single ed 25 519 alias version all of that implemented in one contract that everybody kind of relies on as an ecosystem standard you don't have to rely on it but if you do you kind of get compatibility across the entire universe for free that's what i would hope would happen instead of everybody rolling their own but like yes at the very worst case everybody rolls their own and if you don't know how to roll your own for example like i didn't account for the um for the repeated keys in this example uh working too fast you can get yourself in trouble and just just to follow up on that part of why i'm a big proponent of single key signatures and doing everything else is like you know secure multiple-party communication uh computation is because there's a lot less ways to blow yourself up on chain a single signature is easy to verify in fact my argument would generally be that if you want to write a good contract that is really safe and easy to audit you should use the simplest authorization scheme possible which is that all right can i ask a follow-up question is that i'm i'm a little bit well i'll be honest i'm a little bit behind on on this entire aspect of the interactions um if you're dealing with a case where people do use the simplest and safest approach but you know suppose you're a smart contract author who's trying to be conservative and you don't want to do anything too elaborate and you're using this interface am i correct in reading this that you are probably you're probably not going to have to include an awful lot of code in your contract to make this work right is that correct that the the sort of the number of calls that you have to make to host functions is not particularly huge you're talking about in the case of like a single 80 25 590 right that's right yeah in that case like it's very very simple um that example is in the first one and like the code is basically like check the nonce hash your hasher you know message do a do a 80 25 5 19 post function call that's pretty much it and everything just traps if the wrong thing happens and you could probably write this in like five lines three host function calls or four host function calls um pretty pretty lightweight all told and it's pretty hard to escape all of like some of these parts no matter what you do like at some point if you're going to do the authorization on-chain you probably need to do at least one 80 25 5 19 signature verification or ecdsa signature verification so i it probably could be like a little lighter than this but probably not significantly lighter than this well i guess this i'm i'm just being my typical um trying to shave things down approach uh this feels to me like even the minimal version is a blob of code that will have to get stapled onto every single smart contract and they will all run you know even even if they all wind up being conservative and they'll take your advice and be conservative um this is all in vm rather than extra vm there's no there's no way for them to say fastpath may just do this conservative thing so yes and no because like even though i think that everybody will be conservative i still think the ideal universe of everybody being conservative is them all using a single you know contract that implements this all so that you don't end up with the same code cloned everywhere you just have a cross-contract call you probably think that's worse than it is from a performance perspective but it doesn't end up with like 10 000 or a million copies of the same code everywhere on the blockchain so it's the plus the um the second part of it though is that if there is a lot of ecosystem adapt adoption around some kind of standardized uh signature verifier we could always deploy a native version of that that's super fast i see i see so so so you are i'm not i want to make sure i'm not promising you that that's ever going to happen no no no no i'm not i'm not hearing your promise i'm just trying to understand what level of code reuse you're assuming is going to work and also so you know calling calling a third party to do your authentication for you definitely gets us into the question that is the other thing we're going to be discussing today which is mutability of contracts and like you know uh versioning your dependencies like i think if there's anything someone is going to not want to trust to a third party it's it's the authentication path um unless they're 100 sure that that third party is you know immutable and like the code that they audited the last time they read it i think another option uh is you know you can it could be a cross-contract call to this one contract that's living in one place so we're talking about reduced wasm size um it could also just be the library code that everybody's sharing so people understand there is no mechanism for library sharing besides stapling this like including the code into the contract yeah yeah it's all right so obviously there's no space saving like all these contracts are going to have the same code within them but addressing the the concern of people implementing these things correctly if everyone's using this common piece of library code um that has either been audited or people generally have more trust in um then then then they don't really you don't really have to worry so much about the mutability concern because they're choosing to build that into their contract at build time there are some trade-offs there like i i i think it would generally be a thing that would would would occur that people would probably provide some of these like very standardized off functions like single key or you know based off of seller accounts or stuff in a library that you can use but if you want like a stateful system like lee you were talking about aliases and that's why i keep coming back to this like the stateful system is only really useful if your state lives in a centralized place that uh that people can rely on you know like it would be really annoying for me to have to go and set my alias in every single contract that i use like i could but it just that just seems really irritating uh i think people would much prefer a system where if that's an option there is some global contract for that state limits basically i guess well i guess people could you could have a really simple contract it's just like an aliasing contract and you could have library code that uses that i don't know there's a lot of options here when yeah like it is uh to me super interesting like is that so like what how do we think about that like when you say like when you make so this is like the smart wallet type of uh like situation where the smart wallet is is kind of shielding or separating the uh whatever key you're using at that time from your persistent ID on the network uh i wouldn't necessarily call it like a smart wallet it's kind of tangentially related i guess it would be what i'm really saying is like i sign using key x but my public key always stays as y and i can change x to x prime or x double prime or x triple prime but my public key always stays y yeah so for context i think the aliasing came about because we were talking about how do we replicate what exists on stellar today in the smart world and what we have today with mo like we often talk about stellar multisig but actually the other component of stella's multisig that we get is aliasing because you can have an account identifier and then you can attach other keys to it and so stellar accounts provide these two concepts aliasing and then multisig and i think um you know there's been the concern address uh presented that you know we shouldn't just implement the multi-sig that exists on classic over on smart um because there's a lot of trade-offs with doing that um but the aliasing alone is is like a feature that i think that's worth worth us exploring like what will that look like because it allows people to do things like rotate their keys um or have multiple devices or using the same key or using the same address and if i understand correctly john you're saying that that aliasing capability could actually just be a contract yeah that's exactly what i'm saying any other questions on cap 52 i think because it's so fresh we probably need a bit more time to get into the weeds john could you elaborate a little bit on how you see replay prevention happening so i see in the cap that there's this announce that that concept exists um yeah could you elaborate a little bit with how you see contracts would typically do that yeah i'm happy to do that uh so this whole replay prevention thing gets kind of annoying in this proposal um that's one of like the big downsides of this approach that i pointed out in cat 50 when i said like why we shouldn't do this um which is basically like every contract ends up implementing their own replay prevention when wherever it's needed uh and this means that like things get pretty annoying fast for example like on stellar you like consider stuff center today you might like submit a transaction and you know you have replay prevention on it because the sequence number and you also have like you know a uh a deadline effectively you know the uh max time and you know that if you get to that point everything is done it can't execute or it has already executed uh but like if you want that functionality here you also have to implement the deadlines in your contract and all these other things um and everything just gets kind of annoying fast basically uh now again same kind of thing that i was just talking about you could actually because like in this approach because everything is done by signed messages you can actually delegate all of this to uh to like some other contract that deals with it so you can imagine implementing stellar's times uh you know time downs and uh ledger bounds and all that other stuff in a contract and reusing it if you still want it or you can rebuild in your own contract as well but basically there's no generic nonce here and the cap goes into a little bit of detail about like why you can't use the sequence number uh and they're they're like my original proposal here actually had an example where the sequence number was like the transaction sequence number was used as an us but it had a couple like kind of annoying details about it specifically like such a contract is like really vulnerable or such a design is really vulnerable to uh what's it called uh confused deputy problems and if you try to fix the confused deputy problems then it becomes impossible to use the sequence number as a replay prevention tool so there's a lot of trade-offs here basically like i can imagine an argument where we just say like hey like people should be cognizant of their confused deputy problems etc and we make that an option again i don't know if i would personally feel good about that because confused deputy problems are like a very difficult foot gun to deal with i think like they're an easy thing to overlook so i don't know but basically yes every every contract is building their own replay prevention or relying on it from somewhere else on chain got it i think one nice side effect of um not exposing the transaction source account on sequence number two the contract is that contracts are getting really set up for that common relay pattern that we do see in other ecosystems where people design their contracts so that the message that's getting signed to be used uh like the contract call that's getting signed to be used on chain is independent of the participant who's actually submitting it and paying the fee and that that that participant could be like a third party um that has you know is playing that role of relaying making sure that the transaction is on network so in some ways it's nice it sort of sets up contracts to to really work well with that because if a contract is written to use the source count you don't get like you would have to then modify the contract to make it work with a relay yeah definitely you mentioned this to me like i don't know a week ago or something and that idea really kind of stuck in my head when i was writing this so i totally agree with you that that's a huge advantage of this design i think something that i've i've heard i think maybe grading like raises a concern is um if we encourage people to write their own replaying mechanisms which i don't think we can actually really get away from so maybe it's not worth having this conversation but um if we encourage people to write their own replay mechanisms people may write replay mechanisms that are really inefficient say is storing data on chain forever type of inefficient um do you think there's anything that we can provide that'll like maybe some utilities that we can provide in the sdk or even in the hose functions that might help people write replay prevention mechanisms that are more efficient that you know use the ledger in a less aggressive way i haven't given that too much thought honestly but like i i mean my general kind of perspective on this is you know it costs money to use the blockchain and people will be incentivized to do things that cost less money so basically if there's a reason to do a really inefficient replay prevention mechanism because it makes the rest of your contract much simpler or maybe it's the only way to even do it then i think people will do that but in the absence of that need i think people will favor the super simple mechanism that's cheap uh whether i can guarantee that i don't know and whether we can provide some utilities i'm not really sure i mean like a really simple replay mechanism like a sequence number is basically like you have a map you look it up you check you increment that's it it it could be hard to make it much simpler i mean like we could provide some like library functionality that literally does that exact thing but the thing is if you have an account that already has per user data you would probably want to of like wrap the non-sin with the other per user data and then the helper is not actually helpful in that case so that's kind of the main perspective there but i i have been thinking in general this is like a bit of an aside that it would be really helpful if the sdk provided some types like for example like there's the sc val map type which is like a map in the sense of like a conventional map but i've also been thinking like sometimes it's like you want to look at the data as in like i have a bunch of data stored in different ledger entries and be cool if there was a like a map type that did that very easily instead of having to use like the contract put contract gap etc in cap 53.
- so maybe there's some interplay between that and a replay prevention mechanism that we could learn from yeah i mean i think that yeah all those things are going to like as we develop like even basic applications will factor you know like this type of basic functionality in some traits right that that people would just use i'm not actually too concerned about yeah people having to write it because we are going to write it um i think for more like maybe like different type of uh like uh replay prevention like i think that's kind of the nice thing about this proposal is that uh i know in the past we discussed like uh you know potentially doing very different things like where you have like those uh um like ephemeral type of uh you know like things that only work in a specific time period right so that you um you could in theory like replay in a specific uh window but in some design it's actually acceptable and then you end up with like much simpler client-side code so yeah and can i ask um it don't question but um you have a function in here called knots of how does that work what does that do uh it's just a contract internal function i don't know if the com comments emphasize that they don't um it's not a host function basically it just it just reads the data i just read the larger entry and find the nonce in that would probably just be a single you know integer sort in a lever entry non-sub in those cases is user maintained data associated with an address yeah exactly contract maintain did i want to say but yeah okay in in general i mean i i i'm a broken record i don't want to waste too much time with this but i'm i am i am extremely nervous about suggesting that users roll their own authentication mechanisms i think this is just this is just asking for disaster but i understand that we've been around this like a lot so um you don't need to convince me i mean providing the really really simple authorization that mechanism is the only one i would strongly favor that approach um this just this just feels like it's going to be a disaster you're going to have people who completely fail to because this this data this code path if you get it wrong is came over for everything and it's so easy to get it almost right and your tests pass and you deploy it and you think everything's fine and then it's not so i'm i would love to not have users writing this code but okay so um you know obviously i think we need to dispense the value in this proposal um in the meantime we have 10 minutes remaining and i'd love to hand it over to siddharth to talk a bit about the changes to the smart contract life cycle and potentially any uh remit open questions that we need to answer yeah uh so the most recent change was a a small one about how the contract ID which is which is now a hash is calculated um and you can we can look at that change it's pretty simple where the uh it's created from a transaction you hash source account a user provided salt and if it's a contract created within another contract you hash the parent contract ID and assault provided by the contract um i mean if there are any questions there we can talk about it but i think the more interesting thing are these two other points i want to bring up one is mutability uh which is do we plan on adding like initially the cap right now does not have mutable contracts but the question is should we leave that question open for the future or should we just say contracts will always be immutable and the second thing is uh we allow contracts to be removed so the cap has a host function to remove the contract code entry um so i think we can start with the immutability question right if so if we do allow mutable contracts in the future a big question was how do we deal with versioning and i was taking the approach that let the contracts deal with it so for contract a calls contract b and contract b is mutable contract may just trust that just trust contracts bees uh creator or right so i think grading had some issues with this uh graham you want you want to talk about this well i think they're just i i i keep coming back to the general sense i have that um cross-contract calls are something like dynamic linking um or or package dependencies in software in general right that smart contractor software and this is this is a general software versioning problem and in general software has like natural tensions around versioning that people frequently want to lock to particular versions but they also frequently want to get the newest latest and there's a concept of newest latest that is compatible and a concept of newest latest that is not compatible that is often expressed in major version numbers or or separate apis or separate names for things and i'm concerned that we are not uh reproducing any of the infrastructure that would be normal to support points on that natural tension so i think i think it is worth trying to provide some of the building blocks that people are going to provide themselves anyway because it's it's bad enough to have like versioning uh it's worse if there are multiple versions of the versioning system and you have to like opt into different versioning regimes depending on which ecosystem you're adhering your contract to like i i understand there's just there's this natural tendency in our conversations here to try to push everything to the ecosystem and let the ecosystem figure stuff out let them let them develop patterns in the smart contract space that just solved the problem however contract users would like them to but that that is not actually as much of a solution as i think it sounds uh that that really strongly introduces the possibility of totally incompatible regimes developing in parallel or inadequate regimes that miss some important aspect of the design because they were cobbled together in a hurry so i i wouldn't mind us spending enough time to be able to provide the basics which is like i want to pin to a version uh i want to pin to um a major version and only get security updates or i want to follow you know any new features and additions that people had including modifications upgrades whatever i feel like there's there's got to be a future where those are those are things that someone's going to provide and it's it's our position to potentially furnish them um maybe it's not but that's what it feels like to me but like at our layer like is it is it just uh having a way for to have like committable versus uh mutable contracts and like the the versioning is metadata basically and it's up to when you write a contract you know to decide how you want to use this metadata okay like when you make a cross-contract call at the moment a cross-contract call only identifies a contract ID right right and it does not it does not say call this but give me version five or give me version seven or whatever like there's there's no version information in cross-country calling right now which means we're essentially always dynamically linking to either immutable exactly the same thing or immutable whatever the person up to updated that contract with the thing is that i mean you have a bunch of uh of things that feel like if we if we do that at the kind of protocol layer we're kind of printing those like for example like if you're you're talking about different versions right of a specific contract um like you have what is uh like there are a bunch of questions around like you know you have a 1.0 but maybe the 1.1 is actually deployed by some somebody else right like it's not even the same author like how do you deal with those type of situations like who do you trust to be 1.1 i don't think we have the notion of oh like a of actual like or like you know the organization or whatever that is the the publisher right over have a contract yeah i agree we don't we don't have any notion about that i'm i'm i'm advocating for us to come up with a notion rather than uh saying but that notion like i think it becomes i think in that space i don't think you can necessarily come up with a one size fits all because it's not like in you know in in like normal i mean they say like normal software like you have a company and they ship their their their thing and and that's kind of it like here and also like if you depend on on a specific version right as a as a my installer basically is going to just grab cause the os right to grab that version that i depend on uh only in a blockchain type of situation if that other version like the cost of of of um keeping that other version around is actually on the uh on whoever deployed that contract so if you say like oh i want to pin all version like everything is always pinned i think that that the implication there would be well okay the publisher is now has to keep around all versions of of their contract which is kind of you know weird okay so is is is could you make a a concrete proposal here are you talking about uh you would like all contracts to be mutable immutable not immutable but mutable so you can mutate them and uh if i ever want to uh have a pinned version i vendor it is that what you're saying not necessarily so i think it's uh like yeah you could have always vendor of course um uh i mean that's a solution and i'm i'm okay with that i just i think we have to think through the scenario is what i'm saying and i think if if what we're saying is we're not going to give any thought to the scenarios whatsoever and then we're going to tell everyone to proxy every single call they make because that's the only that's the only place they're going to have any ability to enforce policy i think we're kind of losing a good chance to to shape the system i think it's more like what are like the things that are like support like a that you get with tooling that you know we can provide versus things that are actually baked at the protocol layer um like i think that if you have a way to say uh because you know you'll also also have the other problem right of contract discovery on the network like you know which net which contract do i trust you know versus the ones that i partially trust versus the one i don't trust right uh there are some that were basically you trust you try that contract that even if it was mutable you trust that they are not going to or you're actually fine with um with them modifying it in other situations you do want to pin because you have like a you know an actual uh i mean maybe it's a stability thing or whatever right there you have like other problems there um so so some of these data yeah like the mutability aspect i think that's actually yeah that's a property of the of the contract so that you know basically can i even directly depend on this from my contract or is it potentially going to change under me but then you have a different problem i think that is um kind of like when you do uh you know like with you know when you have your manifest files when you're when you decide that you're going to pin your dependencies right in your own program um that decision is something where i think we have to just develop the the right tooling but it's not something that actually the the you know the network doesn't shouldn't have i think an opinion on on you know like the this even the schema of of uh how you pin things like i said i guess i'm saying that the network has to provide whatever is necessary to support it so we we come up with what we want we come up with what the network needs to support it and i'm i'm not seeing that developing in our conversations we we we talk about like bad things can happen and then we throw up our hands and say obviously we can't solve it like no no we have i think we don't fight this one i agree we just try to figure out what the pattern is and then figure out what the network needs to actually support it right uh i think it's we need to have to kind of to solve those problems what i'm saying is that we don't need to because like the things that i've heard so far they are in the context of a cap and in the context of the cap we are saying this is going to be like we kind of uh forced a specific model at the uh protocol layer oh and i see a raised hand who is that i think i think it um needs to be in a cup like it needs to be in the design rationale for this cup like if we're saying that we're going to provide this limited set of functionality and you can go and do whatever you want we still need to provide in the design rationale this is how we expect it will be used this is how we expect it will solve this problem of versioning i mean i think like at least i can already see like there are some gaps here you know if we say that um you're going to do all contracts are immutable so with an eye and you're going to do versioning yourself data migration how's data migration going to work because like right now we have cap 53 and contracts can't access other contracts data so how is contract you know v1 going its data going to be migrated over to v2 and then if we say that you know v1 and v2 are gonna coexist at the same time um you know how do contracts do that that seems rather complex yeah and especially yeah and actually like yeah in the context of cap 53 i agree that it's also you would change your like if your ID on the network is your contract like your you know the contract is your ID type of thing you don't want to be changing your ID like if you upgrade that contract we got a request to speak from the audience i know you don't know how to take them i'm not i don't think after right it's android question hello can you yep we can hear you great thanks um appreciate the conversation and glad i was able to hop in um i'm going to keep it short because it's going to be slightly off silo of what you're discussing um i am a week away from dropping um a tokenized community with a coin that i'm bringing over from raleigh into solana um and i want to develop on stellar and i've looked up online there's some people that you know help you punch i you know i'm just trying to understand based on what i know with moving forward i mean who can i connect with so that i can understand the liquidity and how the stellar network works and how i can make sure that that same liquidity i'm probably going to lose it but how i can transfer over to the stellar network and really make sure that i look at all aspects of where i'm launching so they don't have to wrap and do it differently in a month do you know what i'm saying yeah cassandra i appreciate the question this specific conversation is about a very like specific uh topic technical topic um and you can go to the uh to one of the other channels support for example uh ask that question and i'll be happy to help you there okay thank you i'm just looking for a personal contact i'm not bringing in the conversation i just i can't find anyone and i've left a couple messages in the chat but i'm not sure who to talk to so who is just speaking and then i'll tag you okay so um we're we're over time and uh i think there's some really interesting discussions here that we probably need to continue uh either on the jump cannon dev channel here on discord or on the mailing list so i think we should go with that thank you all for joining and tuning in and have a great rest of your day you

The agenda stacked three Jump-Cannon caps: CAP-0053 (Smart Contract Data), CAP-0052 (Smart Contract Interactions, minimal), and an updated CAP-0047 (contract lifecycle). Grady explained how CAP-0053 formalizes persistent data by reusing the CAP-0046 type system while introducing the notion of read/write footprints so Core knows which ledger keys a transaction plans to touch before execution.

John then summarized CAP-0052's low-level host functions for invoking contracts and enumerating their read/write sets, and Siddharth walked through CAP-0047's refreshed deploy/update/delete flows that keep ledger metadata tidy. Together these caps round out the primitives Soroban needs before higher-level features like events and fees land.

Key discussion threads:

- CAP-0053's ledger schema: typed contract data entries, how long-lived storage ties back to CAP-0046, and why footprints must be explicit.
- CAP-0052's minimal interaction surface and the guarantees it provides about deterministic host calls and read/write declarations.
- CAP-0047 updates that clarify how contracts are created, versioned, and removed so Core can garbage-collect metadata safely.

<details>
  <summary>Video Transcript</summary>

[00:00] Okay. So I think we're gonna get started and hopefully David will soon join. So hey everyone welcome to another protocol meeting I'm filling in for Justin today I am seeing this. So in these meetings we discuss potential protocol changes these changes are outlined in these documents called CAPs or Core Advancement Proposals and and the big change we're working on right. Now is Project Jump Cannon, which are, which is a feature to introduce native smart contracts on Stellar. So we've divided this massive change into a set of composable CAPs and the agenda specifically for today is we're going to talk about CAP 53 smart contract data this was recently published by graden we're going to talk about CAP 52 smart contract interactions minimal, which was recently introduced by John John and we're going to talk about the smart

[01:00] contract life cycle CAP 47, which was recently updated by Siddharth and requires some for the discussion. So let's do this graded can you kick this off with a review of CAP 53 yeah just give me one second all right got it yeah sorry I just I had the pull request open. And then I actually wanted to switch over to the merged version of it. So this is a fairly straightforward CAP it's not it's not really introducing anything, that probably will be a surprise to anyone here it's just formalizing something, that was left out or left sort of for future CAPs in the modularization, that we've been doing splitting off conversation into different pieces so, that we can work on them separately and land them separately. But it's fairly tightly related to the

[02:00] data model, that was presented in CAP 46. So a lot of the motives in CAP 46 around the data model implicitly talked about how how, that data would be stored permanently. So there's there's there's concerns, that that bear on the data model, that are sort of interacting with it. While it's in memory. And then there are concerns more related to its long-lived accessibility over over multiple invocations of a smart contract well it's stored on the blockchain. So some of the requirements, that are sort of rephrased and and brought into the foreground here have to do with interoperability where we we want there to be something a little bit more general or I say sort of a less general more interoperable more generally understood more widely understood structure to the data than just a byte buffer a lot of

[03:00] smart contract platforms essentially only provide a byte buffer storage service to smart contracts, which means, that nobody really accepts, that exact version of the smart contract code can necessarily read any of the data, that's stored there and, that that produces interoperability problems. If other third parties want to access it offline you know browsers, that want to take a look at the data it also creates versioning problems. Because it means, that you're originally locked to the schema language or serialization format, that the contract used it means the contract wants to pass data it has to from one contract to another it has to transform it. So there's a whole interoperability angle here, which we wanted to address in in the CAP 46 data model and we're carrying, that forward here and I think a lot of the the concerns only make sense. When considered in terms of persistent data. But this is the persistent data CAP. So so here we're just talking about basically what the ledger entries is gonna store things looks like and

[04:00] a handful of host functions for for accessing it they're very basic functions they're just key values to our access functions they're not range range functions they don't include iterators or range queries or anything like, that they're just get put does do we have a key and delete the key very simple host functions the interesting thing really is the choice of granularity, which is left to the user. So this is a little bit different in in, that many blockchains blockchains provide a this is different than many many smart contract systems you'll see many smart contract systems provide a key value store, which is keyed by a byte string or some some kind of a prefix, that goes into a merkle tree or something like, that we do not have we essentially don't expose any interior nodes of the mercalli storage, that we use to use this anyway we just provide a single bucket list

[05:00] hash the fact, that our data structure has internal localization doesn't really apply here anyway and structurally it wouldn't make sense sense. But but what we're doing here is also allowing structured values as keys rather than rather than byte string. So of course you know you can serialize any value and you will in the case of using it as a string here. But the api is encouraging users to have fairly structured values. So they can have fairly rich keys and implicit to all of this is, that there's there's a parallel access and consistency model, that's discussed in this CAP, which is, that we're we're trying to encourage the possibility of executing smart contracts in parallel and. If you have parallel access to a data store you have to talk about what the consistency model is what is it what does it mean. When two different users access, that that model in parallel. So we're specifying here

[06:00] that it's a serializability consistency model, which is the strongest possible it says you know equivalent to the exact order, that the transaction sets specified the transactions executing in, that has to be the observable side effect model and and parallel models parallel consistency models imply the existence of some kind of a concurrency control mechanism how how you actually enforce, that and in in this CAP we're talking a little bit about a very strong mechanism for concurrency control it's what's typically called deterministic scheduling or non-conflicting concurrency control the idea is, that every transaction, that enters the system will pre-declare a footprint. So there's this thing called the footprint, which is the the set of keys, that a transaction is going to touch whether it's going to read them or write them it actually marks whether it reads or writes each key in its footprint and the footprint is is

[07:00] static information, that accompanies a transaction. So this CAP doesn't describe exactly how a footprint is encoded or accompanies a transaction location. Because we don't even have a CAP open right now, that has transaction invocation or at least we haven't settled on one we have several CAPs open right. Now but. When transaction invocation occurs it it's going to need to provide a footprint in in this CAP is asking, that that footprints are available and the footprint defines the keys, that are, that the transaction is allowed to perform these data access operations on so. If you try to perform a get again something, that's not in your footprint the get will fail even even. If the the value is there. If it's not in the transaction's implicit footprint it's defined as failing similarly for a put or even has a point query anything like, that you have to have it in the footprint. So so for simple transactions this is fairly straightforward you can tell what they're going to read or write. And so you just put things in the footprint

[08:00] that they're going to read or write, that's fine for for complicated transactions, that have a highly dynamic behavior maybe maybe it's not even clear what they're going to read or write. Because it's it's you know subsequent to a transaction it's it's determined by an earlier read in the transaction these are what are called transactions with dynamic footprints the recommendation in this gap and what we what we're prototyping is a fairly standard technique from the literature, which is often called reconnaissance queries I think I'm using them here I'm using the term recording footprint recording here, which is, that you just run the transaction offline before you submit it on on a read snapshot and, that gives you a a fairly good guess and an approximate footprint, that you can. Then staple to the real transaction. When you submit it and it will succeed. If that footprint still matches. So it essentially pushes concurrency control

[09:00] out of the the transaction processing loop and into the user's lap. And so the the user is. Now racing on divergence between read snapshots, that they use to construct their footprint and the footprint, that they actually submit a transaction with. And so theoretically. If there's a very very highly contended key and it's a very different query they may have to retry multiple times. Because if they get. If there's any significant divergence between the the recorded footprint and the footprint they submit their transaction could fail. But the database itself doesn't have to actually perform the concurrency control. So in some ways this is shedding load from a concurrency control mechanism inside of a database out to the users and and, that has turned out to work very well for for maintaining a very even high throughput on existing databases, that adopt this technique. So we're trying to adopt, that technique as well. So those are the the two sort of main topics in here, that the fact, that

[10:00] the user has control over granularity I should go back and talk a little bit more about granularity just for a second, which is, that the the granularity control, that exists here it has a natural tension in it. So it's. So so doing a point read on a key value store necessarily has some overhead it has data framing overhead it has serialization overhead it involves going to the I o system at all it involves touching the disk doing a seek doing a read read all of, that overhead is potentially quite high and. And so it's it can be worth trying to amortize, that overhead and read more than one item. If you are going to access more than one item you don't necessarily want to to pay, that on on a bit by bit or bite by bite basis you want to bring in a bunch of bytes at a time. When you do an I o and so, that amortization tension pushes you towards larger ios in a larger granularity of storage. But then the flip side of, that of course is, that. If you read or write data, that you don't actually need. If it's

[11:00] wasted and you actually only wanted to change one byte in the middle of a large data structure, that's waste and you're paying for, that waste in terms of you know fees or cpu time or io or whatever so, that that pushes you in the opposite direction of having fine grained data and, that problem actually just magnifies itself. When you start talking about parallelism. Because again your footprint is a unit of contention and so. If two transactions contend on the same data value value they can't execute in parallel basically, that's, that's, that's what the footprint is is doing is, that it's giving a static scheduler the opportunity to partition execution into separate lanes. And then those lanes will run with no coordination coordination. But but those lanes necessarily are serial themselves you only get parallels between them and so. If you have a whole lot of transactions for example, that all touch some common data value in their footprint they will all be scheduled to run in serial

[12:00] and. And so to exploit parallelism it is in the favor of the user to have a finer grained footprints. So so you have this sort of two different directions of pressure it's a natural trade-off between fine-grained and coarse-grained data access. And so we don't specify what what the granularity is here we try to be very open about, that and and so, that's why the key type is is literally just an arbitrary value I think, that's all I really had to say about, that there's not a lot in this gap it's actually quite small and it it kind of just does exactly what you're expected as a key value type awesome can you just quickly talk about the rationale for why there is point access only yeah. So range queries essentially aren't

[13:00] compatible with static footprints. Because you know we don't know how far they go, that's, that's the simple version we we could theoretically. But we would lose parallelism. So yeah yeah awesome. So we are kind of like actively trying to deter you know contract developers from creating these like you know creating like a need for rangers with some you know like for example like a classic or the book is probably like not a great fit for this, which is okay with us yeah and I mean there it's a good point, which is, that in in a broader sense a static footprint actually bounds the I o you're going to do it allows us I mentioned this in in the contract it it it allows us to essentially have no surprises the contract is not going to be interrupted in the middle of the contract in order to actually go touch the disk dynamically everything, that it's going

[14:00] to read it says upfront and. Therefore we can just do a bulk read at the beginning of the contract just in fact we'll integrate into a single pass through the the storage system all all of the reads from all of the contracts in a given parallel execution lane will just read all their data at once at the beginning of a transaction set execution. And then write it back at the end so, that that kind of thing is is naturally incompatible with something like dynamic range queries but, that said. Because you can store you know arbitrary values. If you want to store a map, that's, that's completely reasonable one of the values, that you store it doesn't have to be just a small string or a number or something you can store a map, that has a bunch of stuff in it. And then do a range query on, that map it's just, that. When you do an I o you're going to get the entire map is going to come off the disk. So you have to sort of navigate, that trade off yourself maybe shard your map into a bunch of different sub maps or something like, that. If you're interested in not loading and

[15:00] storing the whole thing every time. When I tried to to use a very early version of this design like a month ago or something one of the kind of like annoyances, that I quickly encountered was like. When I wanted to you know partition my namespace I basically was like okay well I need some kind of key, that is a tuple. And so I just used like the sc valve option option and I just like pumped a vec full of stuff. And then use it as my key, that's I got like a petition namespace the thing was like doing, that seemed like kind of inefficient inefficient. Because it's like okay like I need a three like you know a three tuple as my key. So I like go I create a vector host function I push into it again host function I push into again host function. And then I call the like you know get function host

[16:00] function again and it just seems like a ton of work to get a single piece of data. So do you have any thoughts about, that off hand I guess I'm I'm I'm not sure, that it is a ton of work like it would would be my first reaction in the sense, that I don't know know. So so. So for example you know we could make a contract put one contract put two contract put three, that takes three values as inputs three three keys four keys five keys you know we could reflect those usage patterns in function signatures as conveniences. But I'm not sure they would do any less work and I don't think the calls in and out of the vm are actually all, that expensive I think you're only talking about one extra op code and a couple of like like a push and a call. So from a user perspective I think you

[17:00] have a good point and I think. If the SDK can't make, that pattern fairly convenient in terms of putting you know sort of a superficial porcelain on top of it, that makes it look nice. Then perhaps we should expand the functional repertoire to provide additional support for, that I think one thing you might be able to do do just responding to your comment about the SDK is you know make it easy to use things like tuples like trying to make it easier to use things like vex already and maps you know tuples might just, that just might be one thing we could have have yeah, that's, that's kind of what I'm expecting is, that you can do the the kind of thing, that that you know I hate to use this as precedent. But but the the raw standard library does something something similar here where it just says you know the people use tuffles up to about five

[18:00] or seven or twelve or whatever. So like it just you know macrogenerate enough support for all the basic temple types, that anyone's like likely to use and and just have them as conveniences. And then you only wonder how to use this arbitrary vector sort of approach. If you're doing something weird yeah I mean as as long as the cost of doing all the push functions isn't particularly high. Then it it doesn't really matter to me. Because you could always put SDK support to do this indeed I just built my own thing, that like I could pass the functions to and inside the parameters too and it would through the vect back out at me. So it would look a lot less disgusting we could generalize, that of course as long as the cost isn't high at the protocol level. So I think. the cost of a function call is is fairly small function call and I again absolutely it's the case, that if. If we measure this and it's miserable I mean the other thing is, that I don't honestly think there's masses of I o operations in the normal

[19:00] contract path right I think you're only talking about a couple of point accesses per contract call anyway. So so I'm I'm not super concerned about, that path but. If if. If we measure it it turns out to be expensive we can absolutely revisit this and try to you know add fast pass or optimize versions for this braden d do you think they could be contracts, that will be vulnerable to moving footprints. So you mentioned about the situation where dynamic footprints is an inconvenience or you know you have to do reconnaissance queries and they potentially could be out of date. But I'm wondering. If there's an angle here where, that actually makes the contract vulnerable in the sense, that you know one participant of, that contra contract could prevent another participant from interacting with it yes absolutely this is as far as I can tell this is basically always the case with concurrency control. If you have any kind of concurrency control mechanism somewhere you can create a starvation you can even you can survive

[20:00] one party by by just hammering on a contended resource in this particular case the user has the contract developer has a fair amount of control over it. Because they can change the granularity so. If a contract developer developer feels, that this is a risk or sees this happening or something like, that they can re-architect the contract to essentially sacrifice concurrency to get rid of the ability to have, that kind of concern. So so you know at the extreme end your footprint is the contract data there's only one contract data everyone who talks to this contract always accesses the exact same contract data and, that means, that everyone knows exactly what their footprint should be it's always just the contract data there's only one element everyone specifies the same thing and they all get serialized. And so there's no you can never have to to to have your footprint invalidate. Because your footprint is always correct. So you can do, that. If you find, that's happening it's just the worst case right. So you

[21:00] you move away from, that. If you want more more concurrency but. If if you're seeing, that people are able to able to and and actively exploiting you know some kind of of starvation concurrency starvation situation. Then then you may have to move back towards, that and I don't personally know a way to avoid, that I think. If we did any kind of dynamic locking we would be in exactly the same situation where someone could just flood the system with with transactions, that take a lock and and deny anyone else the ability to make progress progress and and we would be in a worse situation. Because dynamic and currency the the thing, that's really good about static and currency control as an approach is, that you you have a guaranteed sort of throughput, that the the things, that you have scheduled will complete one way or the other within their allotted time slice right they will either finish or they will abort. And so your abort rates go up. But the system continues to run

[22:00] in in this particular strategy the other strategy would be more like we would give people the ability to drag the entire system down. So a transaction set would would potentially slow down dramatically. Because people are contending on a hot resource. So it's there's there's there aren't a lot of free lunches in concurrency control and and I kind of feel like, that's a natural trait of you, that answer the question I know it's not like a fun answer no yeah I think, that makes a lot of sense the fact, that the proposal really gives the contract developer a lot of control and doesn't define the level of granularity, that they have to use I like yeah it's good

[23:00] well great it looks like you've created one of the least contentious CAPs, that has ever came to life thanks for, that great cool. So let's let's dial up on the contentious or the contention levels John I think this is probably the third iteration of smart contract interactions. So for those of you who are here at last week's meeting you might recall, that we had a pretty big debate about this and I was in the interest of actually agreeing on something I decided to just remove all the functionality from the proposal, which sounds a little backwards. But in the context of smart contracts you can kind of put all of these authorization questions down to contracts and let them do everything themselves

[24:00] and we had been kind of moving in, that direction on CAP 50 anyway with the introduction of like the invoker signature none option and stuff like, that. So basically at a high level of what's in this proposal there is a just like in the old proposal there is a new operation sorry a new transaction type and a corresponding envelope type called invoke contract transaction or invo contract transaction envelope and this contains the normal stuff like before source account sequence number fee it contains the contract, that you're invoking you know the ID the parameter the symbol the parameters it contains the read write set, that you would need for CAP 53 as grading was just talking about and then, that's pretty much it. So how does one actually use this there are some examples in the CAP, which were pretty instructive about what what the universe would look like. If we actually

[25:00] did this there's a whole example section where I hacked up like two versions of an erc20 type contract contract. But they look quite different from your normal erc20. Because there's not really like a reliable message.sender, that you can use in this context. But basically basically the only other thing, that's here is just a few a few host functions, that are useful for actually doing some of the things, that we discussed last week week. So there's some access to thresholds from accounts there's access to getting the signer weight by account key by signer key the signer weight for an account by signer key kind of hard to say say I added a verified 25 519 function, which I think is also in CAP 51, which is being written at the same time and, that's, that's pretty much it. So any questions about this

[26:00] so I'm I'm just just trying to get into this can you talk a bit about like the implications around like accounts and you know we were talking about about this a bit last week like what does this mean for like you know classic multi-sig accounts on the smart side the beauty of it is, that the proposal means basically nothing for those things. Because the contracts get to make their own decisions. So for some context like. If you scroll down almost all the way to the bottom the last example is like a sim a simple token based on account signatures and this builds on the classic Stellar multisig multisig mechanism mechanism basically works exactly the same with two exceptions no pre-signed transactions here oh sorry no pre-auth

[27:00] transactions transactions and no hashtag signers. But it works exactly the same and it works at medium threshold and everything kind of is exactly what you would expect. But right above, that there's another example, that uses the like the single key version, that I was proposing as the invoker signature last week. And so this this framework basically lets contracts build whatever they want. If you want some kind of you know support for seller multisig, that'll really be up to contract developers and ecosystem standards and stuff like, that my intuition is, that those things won't really materialize. Because they're not efficient structures on the blockchain. But they might materialize from case to case something, that immediately jumps out at me with these examples is, that it may be difficult to write these functions

[28:00] some of these sorry I'm looking at the simple token based on account signatures example you were just referencing referencing and the second code block has a check function it says internal function is, that something, that the network is providing or, that's an internal function, that the contract provides provides, that's a contract function, that's not exported exported. So I can imagine. So i've had to write code like this for for for our SDKs. When we were implementing sept10 and I was yeah I think it was septum and one thing, that is quite difficult to get riders iterating over a set of signatures for a message and determining a set of weights. Because there's different things you have to do like you have to make sure, that you don't. If somebody includes the same signature twice twice you don't use it twice to get you know double the weight different things like, that. So do you think by going this approach

[29:00] expecting people to implement their own authorization authorization we're increasing the chance of foot guns where people are going to implement what they think is Stellar maori sig signature authentic authorization verification. But it doesn't actually exactly line up with it, that that's definitely possible I mean my like kind of ideal universe here here in the sense of like what I hope people would do is probably somebody would deploy one contract, that has like a unified key structure a unified signer scheme basically you pass it some kind of opaque blob the beginning of the opaque blob is a discriminant saying like hey what kind of signature is this is it a single key ecdsa is it a is it like a Stellar multi-sig is it some other scheme, that I'm not thinking of like some kind of like quantum reset resistance scheme who cares and basically like the entire ecosystem relies on this contract or sorry another example would be like i

[30:00] know lee you had requested like aliases we could have like a single ed 25 519 alias version all of, that implemented in one contract, that everybody kind of relies on as an ecosystem standard you don't have to rely on it but. If you do you kind of get compatibility across the entire universe for free, that's what I would hope would happen instead of everybody rolling their own. But like yes at the very worst case everybody rolls their own and. If you don't know how to roll your own for example like I didn't account for the for the repeated keys in this example working too fast you can get yourself in trouble and just just to follow up on, that part of why I'm a big proponent of single key signatures and doing everything else is like you know secure multiple-party communication computation computation is. Because there's a lot less ways to blow yourself up on chain a single signature is easy to verify in fact my argument would generally be, that. If you want to write a good contract, that is really safe and easy to audit you should use the simplest

[31:00] authorization scheme possible, which is, that all right can I ask a follow-up question is, that I'm I'm a little bit well i'll be honest I'm a little bit behind on on this entire aspect of the interactions. If you're dealing with a case where people do use the simplest and safest approach approach. But you know suppose you're a smart contract author who's trying to be conservative and you don't want to do anything too elaborate and you're using this interface am I correct in reading this, that you are probably you're probably not going to have to include an awful lot of code in your contract to make this work right is, that correct, that the the sort of the number of calls, that you have to make to host functions is not particularly huge you're talking about in the case of like a single 80 25 590 right, that's right

[32:00] yeah in, that case like it's very very simple simple, that example is in the first one and like the code is basically like check the nonce hash your hasher you know message do a 80 25 5 19 post function call, that's pretty much it and everything just traps. If the wrong thing happens and you could probably write this in like five lines three host function calls or four host function calls pretty pretty lightweight all told and it's pretty hard to escape all of like some of these parts no matter what you do like at some point. If you're going to do the authorization on-chain you probably need to do at least one 80 25 5 19 signature verification or ecdsa signature verification verification. So I it probably could be like a little lighter than this. But probably not significantly lighter than this well I guess this I'm I'm just being my typical trying to shave things down approach

[33:00] this feels to me like even the minimal version is a blob of code, that will have to get stapled onto every single smart contract and they will all run you know even even. If they all wind up being conservative and they'll take your advice and be conservative this is all in vm rather than extra vm there's no way for them to say say fastpath may just do this conservative thing. So yes and no. Because like even, though I think, that everybody will be conservative I still think the ideal universe of everybody being conservative is them all using a single you know contract, that implements this all so, that you don't end up with the same code cloned everywhere you just have a cross-contract call you probably think, that's worse than it is from a performance perspective. But it doesn't end up with like 10 000 or a million copies of the same code everywhere on the blockchain. So it's the plus plus the the second part of it, though is, that. If there is a lot of ecosystem adapt adoption around some kind of standardized standardized signature verifier we could always

[34:00] deploy a native version of, that that's super fast I see. So so. So you are I'm not I want to make sure I'm not promising you, that that's ever going to happen no no I'm not hearing your promise I'm just trying to understand understand what level of code reuse you're assuming is going to work and also. So you know calling calling a third party to do your authentication for you definitely gets us into the question, that is the other thing we're going to be discussing today, which is mutability of contracts and like you know versioning your dependencies like I think. If there's anything someone is going to not want to trust to a third party it's it's the authentication path unless they're 100 sure, that that third party is you know immutable and like the code, that they audited the last time they read it I think another option is you know you can it could be a cross-contract call to this one contract, that's living

[35:00] in one place. So we're talking about reduced wasm size it could also just be the library code, that everybody's sharing. So people understand there is no mechanism for library sharing besides stapling this like including the code into the contract yeah yeah it's all right. So obviously there's no space saving like all these contracts are going to have the same code within them them. But addressing the the concern of people implementing these things correctly. If everyone's using this common piece of library code, that has either been audited or people generally have more trust in. Then then. Then they don't really you don't really have to worry. So much about the mutability concern. Because they're choosing to build, that into their contract at build time there are some trade-offs there like

[36:00] I I think it would generally be a thing, that would would would occur, that people would probably provide some of these like very standardized off functions like single key or you know based off of seller accounts or stuff in a library, that you can use but. If you want like a stateful system like lee you were talking about aliases and, that's why I keep coming back to this like the stateful system is only really useful. If your state lives in a centralized place, that that people can rely on you know like it would be really annoying for me to have to go and set my alias in every single contract, that I use like I could. But it just, that just seems really irritating irritating I think people would much prefer a system where. If that's an option there is some global contract for, that state limits basically I guess well I guess people could you could have a really simple contract it's just like an aliasing contract and you could have library code, that uses, that I don't know there's a lot of options here

[37:00] when yeah like it is to me super interesting like is, that. So like what how do we think about, that like. When you make. So this is like the smart wallet type of like situation where the smart wallet is is kind of shielding or separating the whatever key you're using at, that time from your persistent ID on the network I wouldn't necessarily call it like a smart wallet it's kind of tangentially related I guess it would be what I'm really saying is like I sign using key x. But my public key always stays as y and I can change x to x prime or x triple prime. But my public key always stays y yeah. So for context I think the aliasing came about. Because we were talking about how do we replicate what exists on Stellar today in the smart world and what we have today with mo like we often talk about Stellar multisig. But

[38:00] actually the other component of Stellar's multisig, that we get is aliasing. Because you can have an account identifier. And then you can attach other keys to it. And so Stellar accounts provide these two concepts aliasing. And then multisig and I think you know there's been the concern address presented, that you know we shouldn't just implement the multi-sig, that exists on classic over on smart. Because there's a lot of trade-offs with doing, that. But the aliasing alone is is like a feature, that I think, that's worth worth us exploring like what will, that look like. Because it allows people to do things like rotate their keys or have multiple devices or using the same address and. If I understand correctly John you're saying, that that aliasing

[39:00] capability could actually just be a contract contract yeah, that's exactly what I'm saying any other questions on CAP 52 I think. Because it's. So fresh we probably need a bit more time to get into the weeds John could you elaborate a little bit on how you see replay prevention happening. So I see in the CAP, that there's this announce announce, that that concept exists yeah could you elaborate a little bit with how you see contracts would typically do, that yeah I'm happy to do, that. So this whole replay prevention thing

[40:00] gets kind of annoying in this proposal, that's one of like the big downsides of this approach, that I pointed out in CAP 50. When I said like why we shouldn't do this, which is basically like every contract ends up implementing their own replay prevention. When wherever it's needed and this means, that like things get pretty annoying fast for example like on Stellar you like consider stuff center today you might like submit a transaction and you know you have replay prevention on it. Because the sequence number and you also have like you know a a deadline effectively you know the max time and and you know, that. If you get to, that point everything is done it can't execute or it has already executed. But like. If you want, that functionality here you also have to implement the deadlines in your contract and all these other things and everything just gets kind of annoying fast basically. Now again same kind of thing, that i

[41:00] was just talking about you could actually. Because like in this approach. Because everything is done by signed messages messages you can actually delegate all of this to to like some other contract, that deals with it. So you can imagine implementing implementing Stellar's times you know time downs and ledger bounds and all, that other stuff in a contract and reusing it. If you still want it or you can rebuild in your own contract as well. But but basically there's no generic generic nonce here and the CAP goes into a little bit of detail about like why you can't use the sequence number and they're they're like my original proposal here actually had an example where the sequence number was like the transaction sequence number was used as an us. But it had a couple like kind of annoying details about it specifically like such a contract is like really vulnerable or such a design is really vulnerable to what's it called confused deputy

[42:00] problems and. If you try to fix the confused deputy problems. Then it becomes impossible to use the sequence number as a replay prevention tool. So there's a lot of trade-offs here basically like I can imagine an argument where we just say like hey like people should be cognizant of their confused deputy problems etc and we make, that an option again I don't know. If I would personally feel good about, that. Because confused deputy problems are like a very difficult foot gun to deal with I think like they're an easy thing to overlook. So so I don't know. But basically yes every every contract is building their own replay prevention or relying on it from somewhere else on chain got it I think one nice side effect of not exposing the transaction source account on sequence number two the contract is, that contracts are getting really set up for, that common relay

[43:00] pattern, that we do see in other ecosystems ecosystems where people design their contracts so, that that the message, that's getting signed to be used like the contract call, that's getting signed to be used on chain chain is independent of the the participant who's actually submitting it and paying the fee and, that that, that participant could be like a third party, that has you know is playing, that role of relaying making sure, that the transaction is on network. So in some ways it's nice it sort of sets up contracts to to really work well with, that because. If a contract is written to use the source count count you don't get like you would have to. Then modify the contract to make it work with a relay yeah definitely you mentioned this to me like I don't know a week ago or something and, that idea really kind of stuck in my head. When I was writing this. So I totally agree with you, that that's a huge advantage of this design

[44:00] I think something, that i've i've heard I think maybe grading like raises a concern is. If we encourage people to write their own replaying mechanisms, which I don't think we can actually really get away from. So maybe it's not worth having this conversation but. If we encourage people to write their own replay mechanisms people may write replay mechanisms, that are really inefficient say is storing data on chain forever type of inefficient do you think there's anything, that we can provide, that'll like maybe some utilities, that we can provide in the SDK or even in the hose functions, that might help people write replay prevention mechanisms, that are more efficient, that you know use the ledger in a less aggressive way I haven't given, that too much thought honestly

[45:00] honestly. But like I mean my general kind of perspective on this is you know it costs money to use the blockchain and people will be incentivized to do things, that cost less money money. So so basically. If there's a reason to do a really inefficient replay prevention mechanism mechanism. Because it makes the rest of your contract much simpler or maybe it's the only way to even do it. Then I think people will do, that. But in the absence of, that need I think people will favor the super simple mechanism, that's, that's cheap whether I can guarantee, that I don't know and whether we can provide some utilities I'm not really sure I mean like a really simple replay mechanism like a sequence number is basically like you have a map you look it up you check you increment, that's it it it could be hard to make it much simpler I mean like we could provide some like library functionality, that literally does, that exact thing. But the thing is. If you have an account, that already has per user data you would probably want to of like wrap the

[46:00] non-sin with the other per user data. And then the helper is not actually helpful in, that case so, that's kind of the main perspective there. But I have been thinking in general this is like a bit of an aside, that it would be really helpful. If the SDK provided some types like for example like there's the sc val map type, which is like a map in the sense of like a conventional map. But i've also been thinking like sometimes it's like you want to look at the data as in like I have a bunch of data stored in different ledger entries and be cool. If there was a like a map type, that did, that very easily instead of having to use like the contract put contract gap etc in CAP 53. So maybe there's some interplay between, that and a replay prevention mechanism, that we could learn from yeah I mean I think, that yeah all those things are going to like as we develop like even basic applications will factor you know like

[47:00] this type of basic functionality in some traits right, that that people would just use use I'm not actually too concerned about yeah people having to write it. Because we are going to write it I think for more like maybe like different type of like replay prevention prevention like like I think, that's kind of the nice thing about this proposal is, that I know in the past we discussed like you know potentially doing very different things like where you have like those like ephemeral type of you know like things, that only work in a specific time time period right so, that you you could in theory like replay in a specific window. But in some design it's actually acceptable acceptable. And then you end up with like much simpler client-side code. So so yeah

[48:00] and can I ask it don't question. But you have a function in here called knots of how does, that work what does, that do it's just a contract internal function I don't know. If the com comments emphasize, that they don't it's not a host function basically it just reads the data I just read the larger entry and find the nonce in, that would probably just be a single you know integer sort in a lever entry non-sub in those cases is user maintained data associated with an address address yeah exactly contract maintain did I want to say. But yeah okay in in general I mean I I'm a broken record I don't want to waste too much time with this. But I'm I am extremely nervous about suggesting, that users roll their own authentication mechanisms I think this

[49:00] is just this is just asking for disaster. But but I understand, that we've been around this like like a lot. So you don't need to convince me I mean providing the really really simple authorization, that mechanism is the only one one I would strongly favor, that approach this just feels like it's going to be a disaster you're going to have people who completely fail to. Because this this data this code path. If you get it wrong is came over for everything and it's. So easy to get it almost right and your tests pass and you deploy it and you think everything's fine. And then it's not. So I'm I would love to not have users writing this code. But okay. So you know obviously I think we need to dispense the value in this proposal in the meantime we have 10 minutes

[50:00] remaining and i'd love to hand it over to to Siddharth to talk a bit about the changes to the smart contract life cycle and potentially any remit open questions, that we need to answer answer yeah. So the most recent change was a a small one about how the contract ID, which is. Now a hash is calculated and you can we can look at, that change it's pretty simple where the it's created from a transaction you hash source account a user provided salt salt and. If it's a contract created within another contract you hash the parent contract ID and and assault provided by the contract I mean. If there are any questions there we can talk about it. But I think the more interesting thing are these two other points I want to bring up one is mutability mutability, which is do we plan on adding like initially the CAP right. Now does not have mutable contracts. But the question is should we leave, that question open for the future

[51:00] or should we just say contracts will always be immutable and and the second thing is we allow contracts to be removed. So the CAP has a host function to remove the contract code entry. So I think we can start with the immutability question right. If so. If we do allow mutable contracts in the future a big question was how do we deal with versioning and I was taking the approach, that let the contracts deal with it. So for contract a calls contract b and contract b is mutable contract may just trust, that just trust contracts bees creator or right. So I think grading had some issues with this graham you want to talk about this well I think they're just I keep coming back to the general sense I have, that

[52:00] cross-contract calls are something like dynamic linking or or package dependencies in software in general right, that smart contractor software and this is a general software versioning problem and in general software has like natural tensions around versioning, that people frequently want to lock to particular versions. But they also frequently want to get the newest latest and there's a concept of newest latest, that is not compatible, that is often expressed in major version numbers or or separate apis or separate names for things and and I'm concerned, that we are not reproducing any of the infrastructure, that would be normal to support support points on, that natural tension. So I think it is worth trying to provide some of the

[53:00] building blocks, that people are going to provide themselves anyway. Because it's it's bad enough to have like versioning versioning it's worse. If there are multiple versions of the versioning system and you have to like opt into different versioning regimes depending on, which ecosystem you're adhering your contract to like I understand there's just there's this natural tendency in our conversations here to try to push everything to the ecosystem and let the ecosystem figure stuff out let them develop patterns in the smart contract space, that just solved the problem. However contract users would like them to but, that that is not actually as much of a solution as I think it sounds, that that really strongly introduces the possibility of totally incompatible regimes developing in parallel or inadequate regimes, that miss some important aspect of the design. Because they were cobbled together in a hurry. So I wouldn't mind us spending enough time to be able to provide the basics, which is like like I want to pin to a version

[54:00] I want to pin to a major version and only get security updates or I want to follow you know any new features and additions, that people had including modifications upgrades whatever I feel like there's there's got to be a future where those are things, that someone's going to provide and it's it's our position to potentially furnish them maybe it's not but, that's what it feels like to me. But like at our layer like is it just having a way for to have like committable versus mutable contracts and like the the versioning is metadata basically and it's up to. When you write a contract you know to decide how you want to use this metadata okay like. When you make a cross-contract call at the moment a cross-contract call only identifies a contract ID right right and it does not say say call this

[55:00] but give me version five or whatever like there's there's no version information in cross-country calling right now, which means we're essentially always dynamically linking to either immutable exactly the same thing or immutable immutable whatever the person up to updated, that contract with the thing is, that I mean you have a bunch of of things, that feel like. If we do, that at the kind of protocol layer we're kind of printing those like for example like. If you're you're talking about different versions right of a specific contract like you have what is like there are a bunch of questions around like you know you have a 1.0. But maybe the 1.1 is actually deployed by some somebody else right like it's not even the same author author like how do you deal with those type of situations

[56:00] like who do you trust to be 1.1 I don't think we have the notion of oh like a of actual like like or like you know the organization or whatever, that is the the publisher right over have a contract yeah I agree we don't have any notion about, that I'm I'm I'm advocating for us to come up with a notion rather than than saying but, that notion like I think it becomes I think in, that space I don't think you can necessarily come up with a one size fits all all. Because it's not like in you know in in like normal I mean they say like normal software like you have a company and they ship their their their thing and and, that's kind of it like here and also like. If you depend on on a specific version right as a my installer basically is going to just grab grab cause the os right to grab, that version

[57:00] that I depend on only in a blockchain type of situation. If that other version like the cost of of of keeping, that other version around is actually on the on whoever deployed, that contract so. If you say like oh I want to pin all version like everything is always pinned I think, that that the implication there would be well okay the publisher is. Now has to keep around all versions of of their contract, which is kind of you know weird okay. So is is is could you make a a concrete proposal here are you talking about you would like all contracts to be mutable immutable not immutable. But mutable. So you can mutate them and and. If I ever want to have a pinned

[58:00] version I vendor it is, that what you're saying saying not necessarily. So I think it's like yeah you could have always vendor of course course I mean, that's a solution and I'm I'm okay with, that I just I think we have to think through the scenario is what I'm saying and I think. If if what we're saying is we're not going to give any thought to the scenarios whatsoever. And then we're going to tell everyone to proxy every single call they make. Because that's the only, place they're going to have any ability to to enforce policy I think we're kind of losing a good chance to to shape the system I think it's more like what are like the things, that are like support like a, that you get with tooling, that you know we can provide versus things, that are actually baked at the protocol layer like I think, that. If you have a way to say. Because you know you'll also also have the other problem right of

[59:00] contract discovery on the network like you know, which net, which contract do I trust you know versus the ones, that I partially trust versus the one I don't trust right there are some, that were basically you trust you try, that contract, that even. If it was mutable you trust, that they are not going to or you're actually fine with with them modifying it in other situations you do want to pin. Because you have like a a you know an actual I mean maybe it's a stability thing or whatever right there you have like other other problems there. So so some of these data yeah like the mutability mutability aspect I think, that's actually yeah, that's a property of the contract so, that you know basically can I even directly depend on this from my contract or is it potentially going to change under me. But then you have a different problem I think, that is kind of like. When you do you

[01:00:00] know like with you know. When you have your manifest files. When you're. When you decide, that you're going to pin your dependencies right in your own program, that decision is something where I think we have to just develop the the right tooling. But it's not something, that actually the the you know the network doesn't shouldn't have I think an opinion on on on you know like the this even the schema of of how you pin things things like I said I guess I'm saying, that the network has to provide whatever is necessary to support it. So we we come up with what the network needs to support it and I'm I'm not seeing, that developing in our conversations we we we talk about like like bad things can happen. And then we throw up our hands and say obviously we can't solve it like no no we have I think we don't fight this one I agree we just try

[01:01:00] to figure out what the pattern is. And then figure out what the network needs to actually support it right I think it's we need to have to kind of to solve those problems what I'm saying is, that we don't need to. Because like the things, that i've heard. So far they are in the context of a CAP we are saying this is going to be like we kind of forced a specific model at the the protocol layer oh and I see a raised hand who is, that I think it needs to be in a cup like it needs to be in the design rationale for this cup like. If we're saying, that we're going to provide this limited set of functionality and you can go and do whatever you want we still need to provide in the design rationale this is how we expect it will solve this problem of versioning I mean I think like at least I can already see like there are some gaps here you know. If we say, that

[01:02:00] you're going to do all contracts are immutable. So with an eye eye and you're going to do versioning yourself data migration how's data migration going to work. Because like right. Now we have CAP 53 53 and contracts can't access other contracts data. So how is contract you know v1 going going its data going to be migrated over to v2 and then. If we say, that you know v1 and v2 are gonna coexist at the same time you know how do contracts do, that that seems rather complex yeah and especially yeah and actually like yeah in the context of CAP 53 I agree, that it's also you would change your like. If your ID on the network is your contract like your you know the contract is your ID type of thing thing you don't want to be changing your ID like. If you upgrade, that contract

[01:03:00] we got a request to speak from the audience I know you don't know how to take them I'm not I don't think after right right it's android question hello can you yep we can hear you great thanks appreciate the conversation and glad I was able to hop in I'm going to keep it short. Because it's going to be slightly off silo of what you're discussing discussing I am a week away from dropping a tokenized community with a coin, that I'm bringing over from raleigh into solana and I want to develop on Stellar and i've looked up online there's some people, that you know help you punch I you know I'm just trying to understand based on what I know with moving forward I mean who can I connect with so, that i

[01:04:00] can understand the liquidity and how the Stellar network works and how I can make sure, that that same liquidity I'm probably going to lose it. But how I can transfer over to the Stellar network and really make sure, that I look at all aspects of where I'm launching. So they don't have to wrap and do it differently in a month do you know what I'm saying yeah cassandra I appreciate the question this specific conversation is about a very like specific topic technical topic topic and you can go to the to one of the other channels support for example ask, that question and i'll be happy to help you there okay thank you I'm just looking for a personal contact I'm not bringing in the conversation I just I can't find anyone and i've left a couple messages in the chat. But I'm not sure who to talk to. So who is just speaking. And then i'll tag you okay. So we're we're over time and I think

[01:05:00] there's some really interesting discussions here, that we probably need to continue either on the Jump Cannon dev channel here on Discord or on the mailing list. So I think we should go with, that thank you all for joining and tuning in and have a great rest of your day you

</details>
