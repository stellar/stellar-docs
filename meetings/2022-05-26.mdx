---
title: "Smart Contract Storage Interaction Lifecycle"
description: "A deep technical discussion on Soroban’s smart contract storage model, interaction lifecycle, and concurrency guarantees, covering how CAP-53, CAP-52, and CAP-46-2 work together to enable deterministic execution, parallelism, and safe contract evolution."
authors:
  - graydon-hoare
  - jonathan-jove
  - leigh-mcculloch
  - nicolas-barry
  - siddharth-suresh
  - tomer-weller
tags:
  - legacy
  - CAP-46-1
  - CAP-46-2
  - CAP-46-3
  - CAP-46-5
  - CAP-50
  - CAP-52
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="NF8sWOTbyWI" />

This discussion continues Project Jump Cannon by focusing on how Soroban smart contracts store data, declare access patterns, and safely interact with the Stellar ledger. The session explores why storage design, determinism, and explicit access declarations are foundational for enabling parallel execution without sacrificing correctness.

The conversation walks through three closely related CAPs—CAP-53 (now CAP-46-5), CAP-52 (Base64 Encoding/Decoding), and updates to CAP-47 (now CAP-46-2). Together, these proposals define how contracts persist state, how Core pre-validates read/write behavior, and how contracts are created, upgraded, or removed while keeping ledger metadata manageable.

### Key Topics

- CAP-46-5 Smart Contract Data model:
  - Typed, persistent ledger entries derived from CAP-46’s value system (now CAP-46-1).
  - Explicit read/write footprints that declare all ledger keys a transaction may access.
  - Deterministic, serializable execution that enables parallelism without dynamic locking.
- Footprints and concurrency control:
  - Why transactions must pre-declare accessed keys.
  - Trade-offs between fine-grained and coarse-grained storage for performance and contention.
  - Handling dynamic access patterns via offline “recorded” footprint generation.
- Point-access-only storage APIs:
  - Rationale for excluding range queries to preserve static footprints and parallel execution.
  - How developers can model higher-level structures (maps, tuples) within stored values.
- CAP-52 minimal contract interactions:
  - New invoke-contract transaction shape and host functions.
  - Contract-defined authorization and replay-prevention strategies.
  - Design trade-offs around developer flexibility versus foot-gun risk.
- Authorization and replay prevention debates:
  - Risks of rolling custom auth logic inside contracts.
  - Potential for ecosystem-standard verifier contracts or shared libraries.
  - Benefits for relayer-based transaction models.
- CAP-46-2 Contract Lifecycle:
  - Contract ID derivation via hashing and user-provided salt.
  - Discussion of immutability vs future mutability.
  - Removing contract code and keeping ledger metadata tidy.
- Open questions around versioning and upgrades:
  - How contracts should depend on mutable vs immutable dependencies.
  - Challenges of data migration when contracts cannot access each other’s storage.
  - Tension between protocol-level guarantees and ecosystem-driven patterns.

### Resources

- [Project Jump Cannon: Choosing WASM](https://stellar.org/blog/developers/project-jump-cannon-choosing-wasm)
- [CAP-0046-01: WebAssembly Smart Contract Runtime Environment](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md)
- [CAP-0046-02: Smart Contract Life Cycle](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-02.md)
- [CAP-0046-05: Smart Contract Data](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md)
- [CAP-0050: Smart Contract Interactions](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0050.md)
- [CAP-0052: Base64 Encoding/Decoding](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0052.md)

<details>
  <summary>Video Transcript</summary>

[00:00] Okay. So I think we're gonna get started and hopefully David will soon join. So hey everyone welcome to another protocol meeting I'm filling in for Justin today I am seeing this. So in these meetings we discuss potential protocol changes these changes are outlined in these documents called CAPs or Core Advancement Proposals and the big change we're working on right. Now is Project Jump Cannon, which are, which is a feature to introduce native smart contracts on Stellar. So we've divided this massive change into a set of composable CAPs and the agenda specifically for today is we're going to talk about [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md) smart contract data this was recently published by graden we're going to talk about [CAP-52](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0052.md) smart contract interactions minimal, which was recently introduced by John and we're going to talk about the smart

[01:00] contract life cycle [CAP-47](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-02.md), which was recently updated by Siddharth and requires some for the discussion. So let's do this graded can you kick this off with a review of [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md) yeah just give me one second all right got it yeah sorry I just I had the pull request open. And then I actually wanted to switch over to the merged version of it. So this is a fairly straightforward CAP it's not really introducing anything, that probably will be a surprise to anyone here it's just formalizing something, that was left out or left sort of for future CAPs in the modularization, that we've been doing splitting off conversation into different pieces so, that we can work on them separately and land them separately. But it's fairly tightly related to the

[02:00] data model, that was presented in [CAP-46](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md). So a lot of the motives in [CAP-46](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md) around the data model implicitly talked about how, that data would be stored permanently. So there's concerns, that bear on the data model, that are sort of interacting with it. While it's in memory. And then there are concerns more related to its long-lived accessibility over multiple invocations of a smart contract well it's stored on the blockchain. So some of the requirements, that are sort of rephrased and brought into the foreground here have to do with interoperability where we want there to be something a little bit more general or I say sort of a less general more interoperable more generally understood more widely understood structure to the data than just a byte buffer a lot of

[03:00] smart contract platforms essentially only provide a byte buffer storage service to smart contracts, which means, that nobody really accepts, that exact version of the smart contract code can necessarily read any of the data, that's stored there and, that produces interoperability problems. If other third parties want to access it offline you know browsers, that want to take a look at the data it also creates versioning problems. Because it means, that you're originally locked to the schema language or serialization format, that the contract used it means the contract wants to pass data it has to from one contract to another it has to transform it. So there's a whole interoperability angle here, which we wanted to address in the [CAP-46](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-01.md) data model and we're carrying, that forward here and I think a lot of the concerns only make sense. When considered in terms of persistent data. But this is the persistent data CAP. So so here we're just talking about basically what the ledger entries is gonna store things looks like and

[04:00] a handful of host functions for accessing it they're very basic functions they're just key values to our access functions they're not range functions they don't include iterators or range queries or anything like, that they're just get put does do we have a key and delete the key very simple host functions the interesting thing really is the choice of granularity, which is left to the user. So this is a little bit different in, that many blockchains provide a this is different than many smart contract systems you'll see many smart contract systems provide a key value store, which is keyed by a byte string or some kind of a prefix, that goes into a merkle tree or something like, that we do not have we essentially don't expose any interior nodes of the mercalli storage, that we use to use this anyway we just provide a single bucket list

[05:00] hash the fact, that our data structure has internal localization doesn't really apply here anyway and structurally it wouldn't make sense. But but what we're doing here is also allowing structured values as keys rather than byte string. So of course you know you can serialize any value and you will in the case of using it as a string here. But the api is encouraging users to have fairly structured values. So they can have fairly rich keys and implicit to all of this is, that there's a parallel access and consistency model, that's discussed in this CAP, which is, that we're trying to encourage the possibility of executing smart contracts in parallel and. If you have parallel access to a data store you have to talk about what the consistency model is what is it what does it mean. When two different users access, that model in parallel. So we're specifying here

[06:00] that it's a serializability consistency model, which is the strongest possible it says you know equivalent to the exact order, that the transaction sets specified the transactions executing in, that has to be the observable side effect model and parallel models parallel consistency models imply the existence of some kind of a concurrency control mechanism how you actually enforce, that and in this CAP we're talking a little bit about a very strong mechanism for concurrency control it's what's typically called deterministic scheduling or non-conflicting concurrency control the idea is, that every transaction, that enters the system will pre-declare a footprint. So there's this thing called the footprint, which is the set of keys, that a transaction is going to touch whether it's going to read them or write them it actually marks whether it reads or writes each key in its footprint and the footprint is

[07:00] static information, that accompanies a transaction. So this CAP doesn't describe exactly how a footprint is encoded or accompanies a transaction location. Because we don't even have a CAP open right now, that has transaction invocation or at least we haven't settled on one we have several CAPs open right. Now but. When transaction invocation occurs it's going to need to provide a footprint in this CAP is asking, that footprints are available and the footprint defines the keys, that are, that the transaction is allowed to perform these data access operations on so. If you try to perform a get again something, that's not in your footprint the get will fail even. If the value is there. If it's not in the transaction's implicit footprint it's defined as failing similarly for a put or even has a point query anything like, that you have to have it in the footprint. So so for simple transactions this is fairly straightforward you can tell what they're going to read or write. And so you just put things in the footprint

[08:00] that they're going to read or write, that's fine for complicated transactions, that have a highly dynamic behavior maybe it's not even clear what they're going to read or write. Because it's you know subsequent to a transaction it's determined by an earlier read in the transaction these are what are called transactions with dynamic footprints the recommendation in this gap and what we're prototyping is a fairly standard technique from the literature, which is often called reconnaissance queries I think I'm using them here I'm using the term recording footprint recording here, which is, that you just run the transaction offline before you submit it on a read snapshot and, that gives you a fairly good guess and an approximate footprint, that you can. Then staple to the real transaction. When you submit it and it will succeed. If that footprint still matches. So it essentially pushes concurrency control

[09:00] out of the transaction processing loop and into the user's lap. And so the user is. Now racing on divergence between read snapshots, that they use to construct their footprint and the footprint, that they actually submit a transaction with. And so theoretically. If there's a very highly contended key and it's a very different query they may have to retry multiple times. Because if they get. If there's any significant divergence between the recorded footprint and the footprint they submit their transaction could fail. But the database itself doesn't have to actually perform the concurrency control. So in some ways this is shedding load from a concurrency control mechanism inside of a database out to the users and, that has turned out to work very well for maintaining a very even high throughput on existing databases, that adopt this technique. So we're trying to adopt, that technique as well. So those are the two sort of main topics in here, that the fact, that

[10:00] the user has control over granularity I should go back and talk a little bit more about granularity just for a second, which is, that the granularity control, that exists here it has a natural tension in it. So it's. So so doing a point read on a key value store necessarily has some overhead it has data framing overhead it has serialization overhead it involves going to the I o system at all it involves touching the disk doing a seek doing a read all of, that overhead is potentially quite high and. And so it's it can be worth trying to amortize, that overhead and read more than one item. If you are going to access more than one item you don't necessarily want to pay, that on a bit by bit or bite by bite basis you want to bring in a bunch of bytes at a time. When you do an I o and so, that amortization tension pushes you towards larger ios in a larger granularity of storage. But then the flip side of, that of course is, that. If you read or write data, that you don't actually need. If it's

[11:00] wasted and you actually only wanted to change one byte in the middle of a large data structure, that's waste and you're paying for, that waste in terms of you know fees or cpu time or io or whatever so, that pushes you in the opposite direction of having fine grained data and, that problem actually just magnifies itself. When you start talking about parallelism. Because again your footprint is a unit of contention and so. If two transactions contend on the same data value they can't execute in parallel basically, that's, that's, that's what the footprint is doing is, that it's giving a static scheduler the opportunity to partition execution into separate lanes. And then those lanes will run with no coordination. But but those lanes necessarily are serial themselves you only get parallels between them and so. If you have a whole lot of transactions for example, that all touch some common data value in their footprint they will all be scheduled to run in serial

[12:00] and. And so to exploit parallelism it is in the favor of the user to have a finer grained footprints. So so you have this sort of two different directions of pressure it's a natural trade-off between fine-grained and coarse-grained data access. And so we don't specify what the granularity is here we try to be very open about, that and so, that's why the key type is literally just an arbitrary value I think, that's all I really had to say about, that there's not a lot in this gap it's actually quite small and it kind of just does exactly what you're expected as a key value type awesome can you just quickly talk about the rationale for why there is point access only yeah. So range queries essentially aren't

[13:00] compatible with static footprints. Because you know we don't know how far they go, that's, that's the simple version we could theoretically. But we would lose parallelism. So yeah awesome. So we are kind of like actively trying to deter you know contract developers from creating these like you know creating like a need for rangers with some you know like for example like a classic or the book is probably like not a great fit for this, which is okay with us yeah and I mean there it's a good point, which is, that in a broader sense a static footprint actually bounds the I o you're going to do it allows us I mentioned this in the contract it allows us to essentially have no surprises the contract is not going to be interrupted in the middle of the contract in order to actually go touch the disk dynamically everything, that it's going

[14:00] to read it says upfront and. Therefore we can just do a bulk read at the beginning of the contract just in fact we'll integrate into a single pass through the storage system all of the reads from all of the contracts in a given parallel execution lane will just read all their data at once at the beginning of a transaction set execution. And then write it back at the end so, that kind of thing is naturally incompatible with something like dynamic range queries but, that said. Because you can store you know arbitrary values. If you want to store a map, that's, that's completely reasonable one of the values, that you store it doesn't have to be just a small string or a number or something you can store a map, that has a bunch of stuff in it. And then do a range query on, that map it's just, that. When you do an I o you're going to get the entire map is going to come off the disk. So you have to sort of navigate, that trade off yourself maybe shard your map into a bunch of different sub maps or something like, that. If you're interested in not loading and

[15:00] storing the whole thing every time. When I tried to use a very early version of this design like a month ago or something one of the kind of like annoyances, that I quickly encountered was like. When I wanted to you know partition my namespace I basically was like okay well I need some kind of key, that is a tuple. And so I just used like the sc valve option and I just like pumped a vec full of stuff. And then use it as my key, that's I got like a petition namespace the thing was like doing, that seemed like kind of inefficient. Because it's like okay like I need a three like you know a three tuple as my key. So I like go I create a vector host function I push into it again host function I push into again host function. And then I call the like you know get function host

[16:00] function again and it just seems like a ton of work to get a single piece of data. So do you have any thoughts about, that off hand I guess I'm not sure, that it is a ton of work like it would be my first reaction in the sense, that I don't know. So so. So for example you know we could make a contract put one contract put two contract put three, that takes three values as inputs three keys four keys five keys you know we could reflect those usage patterns in function signatures as conveniences. But I'm not sure they would do any less work and I don't think the calls in and out of the vm are actually all, that expensive I think you're only talking about one extra op code and a couple of like a push and a call. So from a user perspective I think you

[17:00] have a good point and I think. If the SDK can't make, that pattern fairly convenient in terms of putting you know sort of a superficial porcelain on top of it, that makes it look nice. Then perhaps we should expand the functional repertoire to provide additional support for, that I think one thing you might be able to do just responding to your comment about the SDK is you know make it easy to use things like tuples like trying to make it easier to use things like vex already and maps you know tuples might just, that just might be one thing we could have yeah, that's, that's kind of what I'm expecting is, that you can do the kind of thing, that you know I hate to use this as precedent. But but the raw standard library does something similar here where it just says you know the people use tuffles up to about five

[18:00] or seven or twelve or whatever. So like it just you know macrogenerate enough support for all the basic temple types, that anyone's like likely to use and just have them as conveniences. And then you only wonder how to use this arbitrary vector sort of approach. If you're doing something weird yeah I mean as long as the cost of doing all the push functions isn't particularly high. Then it doesn't really matter to me. Because you could always put SDK support to do this indeed I just built my own thing, that like I could pass the functions to and inside the parameters too and it would through the vect back out at me. So it would look a lot less disgusting we could generalize, that of course as long as the cost isn't high at the protocol level. So I think. the cost of a function call is fairly small function call and I again absolutely it's the case, that if. If we measure this and it's miserable I mean the other thing is, that I don't honestly think there's masses of I o operations in the normal

[19:00] contract path right I think you're only talking about a couple of point accesses per contract call anyway. So so I'm not super concerned about, that path but. If if. If we measure it turns out to be expensive we can absolutely revisit this and try to you know add fast pass or optimize versions for this braden d do you think they could be contracts, that will be vulnerable to moving footprints. So you mentioned about the situation where dynamic footprints is an inconvenience or you know you have to do reconnaissance queries and they potentially could be out of date. But I'm wondering. If there's an angle here where, that actually makes the contract vulnerable in the sense, that you know one participant of, that contra contract could prevent another participant from interacting with it yes absolutely this is as far as I can tell this is basically always the case with concurrency control. If you have any kind of concurrency control mechanism somewhere you can create a starvation you can even you can survive

[20:00] one party by just hammering on a contended resource in this particular case the user has the contract developer has a fair amount of control over it. Because they can change the granularity so. If a contract developer feels, that this is a risk or sees this happening or something like, that they can re-architect the contract to essentially sacrifice concurrency to get rid of the ability to have, that kind of concern. So so you know at the extreme end your footprint is the contract data there's only one contract data everyone who talks to this contract always accesses the exact same contract data and, that means, that everyone knows exactly what their footprint should be it's always just the contract data there's only one element everyone specifies the same thing and they all get serialized. And so there's no you can never have to have your footprint invalidate. Because your footprint is always correct. So you can do, that. If you find, that's happening it's just the worst case right. So you

[21:00] you move away from, that. If you want more concurrency but. If if you're seeing, that people are able to and actively exploiting you know some kind of starvation concurrency starvation situation. Then then you may have to move back towards, that and I don't personally know a way to avoid, that I think. If we did any kind of dynamic locking we would be in exactly the same situation where someone could just flood the system with transactions, that take a lock and deny anyone else the ability to make progress and we would be in a worse situation. Because dynamic and currency the thing, that's really good about static and currency control as an approach is, that you have a guaranteed sort of throughput, that the things, that you have scheduled will complete one way or the other within their allotted time slice right they will either finish or they will abort. And so your abort rates go up. But the system continues to run

[22:00] in this particular strategy the other strategy would be more like we would give people the ability to drag the entire system down. So a transaction set would potentially slow down dramatically. Because people are contending on a hot resource. So it's there's there aren't a lot of free lunches in concurrency control and I kind of feel like, that's a natural trait of you, that answer the question I know it's not like a fun answer no yeah I think, that makes a lot of sense the fact, that the proposal really gives the contract developer a lot of control and doesn't define the level of granularity, that they have to use I like yeah it's good

[23:00] well great it looks like you've created one of the least contentious CAPs, that has ever came to life thanks for, that great cool. So let's dial up on the contentious or the contention levels John I think this is probably the third iteration of smart contract interactions. So for those of you who are here at last week's meeting you might recall, that we had a pretty big debate about this and I was in the interest of actually agreeing on something I decided to just remove all the functionality from the proposal, which sounds a little backwards. But in the context of smart contracts you can kind of put all of these authorization questions down to contracts and let them do everything themselves

[24:00] and we had been kind of moving in, that direction on [CAP-50](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0050.md) anyway with the introduction of like the invoker signature none option and stuff like, that. So basically at a high level of what's in this proposal there is a just like in the old proposal there is a new operation sorry a new transaction type and a corresponding envelope type called invoke contract transaction or invo contract transaction envelope and this contains the normal stuff like before source account sequence number fee it contains the contract, that you're invoking you know the ID the parameter the symbol the parameters it contains the read write set, that you would need for [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md) as grading was just talking about and then, that's pretty much it. So how does one actually use this there are some examples in the CAP, which were pretty instructive about what the universe would look like. If we actually

[25:00] did this there's a whole example section where I hacked up like two versions of an erc20 type contract. But they look quite different from your normal erc20. Because there's not really like a reliable message.sender, that you can use in this context. But basically the only other thing, that's here is just a few host functions, that are useful for actually doing some of the things, that we discussed last week. So there's some access to thresholds from accounts there's access to getting the signer weight by account key by signer key the signer weight for an account by signer key kind of hard to say I added a verified 25 519 function, which I think is also in [CAP-51](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-03.md), which is being written at the same time and, that's, that's pretty much it. So any questions about this

[26:00] so I'm just trying to get into this can you talk a bit about like the implications around like accounts and you know we were talking about this a bit last week like what does this mean for like you know classic multi-sig accounts on the smart side the beauty of it is, that the proposal means basically nothing for those things. Because the contracts get to make their own decisions. So for some context like. If you scroll down almost all the way to the bottom the last example is like a sim a simple token based on account signatures and this builds on the classic Stellar multisig mechanism basically works exactly the same with two exceptions no pre-signed transactions here oh sorry no pre-auth

[27:00] transactions and no hashtag signers. But it works exactly the same and it works at medium threshold and everything kind of is exactly what you would expect. But right above, that there's another example, that uses the like the single key version, that I was proposing as the invoker signature last week. And so this framework basically lets contracts build whatever they want. If you want some kind of you know support for seller multisig, that'll really be up to contract developers and ecosystem standards and stuff like, that my intuition is, that those things won't really materialize. Because they're not efficient structures on the blockchain. But they might materialize from case to case something, that immediately jumps out at me with these examples is, that it may be difficult to write these functions

[28:00] some of these sorry I'm looking at the simple token based on account signatures example you were just referencing and the second code block has a check function it says internal function is, that something, that the network is providing or, that's an internal function, that the contract provides, that's a contract function, that's not exported. So I can imagine. So i've had to write code like this for our SDKs. When we were implementing sept10 and I was yeah I think it was septum and one thing, that is quite difficult to get riders iterating over a set of signatures for a message and determining a set of weights. Because there's different things you have to do like you have to make sure, that you don't. If somebody includes the same signature twice you don't use it twice to get you know double the weight different things like, that. So do you think by going this approach

[29:00] expecting people to implement their own authorization we're increasing the chance of foot guns where people are going to implement what they think is Stellar maori sig signature authentic authorization verification. But it doesn't actually exactly line up with it, that's definitely possible I mean my like kind of ideal universe here in the sense of like what I hope people would do is probably somebody would deploy one contract, that has like a unified key structure a unified signer scheme basically you pass it some kind of opaque blob the beginning of the opaque blob is a discriminant saying like hey what kind of signature is this is it a single key ecdsa is it a is it like a Stellar multi-sig is it some other scheme, that I'm not thinking of like some kind of like quantum reset resistance scheme who cares and basically like the entire ecosystem relies on this contract or sorry another example would be like i

[30:00] know lee you had requested like aliases we could have like a single ed 25 519 alias version all of, that implemented in one contract, that everybody kind of relies on as an ecosystem standard you don't have to rely on it but. If you do you kind of get compatibility across the entire universe for free, that's what I would hope would happen instead of everybody rolling their own. But like yes at the very worst case everybody rolls their own and. If you don't know how to roll your own for example like I didn't account for the repeated keys in this example working too fast you can get yourself in trouble and just to follow up on, that part of why I'm a big proponent of single key signatures and doing everything else is like you know secure multiple-party communication computation is. Because there's a lot less ways to blow yourself up on chain a single signature is easy to verify in fact my argument would generally be, that. If you want to write a good contract, that is really safe and easy to audit you should use the simplest

[31:00] authorization scheme possible, which is, that all right can I ask a follow-up question is, that I'm a little bit well i'll be honest I'm a little bit behind on this entire aspect of the interactions. If you're dealing with a case where people do use the simplest and safest approach. But you know suppose you're a smart contract author who's trying to be conservative and you don't want to do anything too elaborate and you're using this interface am I correct in reading this, that you are probably you're probably not going to have to include an awful lot of code in your contract to make this work right is, that correct, that the sort of the number of calls, that you have to make to host functions is not particularly huge you're talking about in the case of like a single 80 25 590 right, that's right

[32:00] yeah in, that case like it's very simple, that example is in the first one and like the code is basically like check the nonce hash your hasher you know message do a 80 25 5 19 post function call, that's pretty much it and everything just traps. If the wrong thing happens and you could probably write this in like five lines three host function calls or four host function calls pretty lightweight all told and it's pretty hard to escape all of like some of these parts no matter what you do like at some point. If you're going to do the authorization on-chain you probably need to do at least one 80 25 5 19 signature verification or ecdsa signature verification. So I it probably could be like a little lighter than this. But probably not significantly lighter than this well I guess this I'm just being my typical trying to shave things down approach

[33:00] this feels to me like even the minimal version is a blob of code, that will have to get stapled onto every single smart contract and they will all run you know even. If they all wind up being conservative and they'll take your advice and be conservative this is all in vm rather than extra vm there's no way for them to say fastpath may just do this conservative thing. So yes and no. Because like even, though I think, that everybody will be conservative I still think the ideal universe of everybody being conservative is them all using a single you know contract, that implements this all so, that you don't end up with the same code cloned everywhere you just have a cross-contract call you probably think, that's worse than it is from a performance perspective. But it doesn't end up with like 10 000 or a million copies of the same code everywhere on the blockchain. So it's the plus the second part of it, though is, that. If there is a lot of ecosystem adapt adoption around some kind of standardized signature verifier we could always

[34:00] deploy a native version of, that's super fast I see. So so. So you are I'm not I want to make sure I'm not promising you, that's ever going to happen no I'm not hearing your promise I'm just trying to understand what level of code reuse you're assuming is going to work and also. So you know calling a third party to do your authentication for you definitely gets us into the question, that is the other thing we're going to be discussing today, which is mutability of contracts and like you know versioning your dependencies like I think. If there's anything someone is going to not want to trust to a third party it's the authentication path unless they're 100 sure, that third party is you know immutable and like the code, that they audited the last time they read it I think another option is you know you can it could be a cross-contract call to this one contract, that's living

[35:00] in one place. So we're talking about reduced wasm size it could also just be the library code, that everybody's sharing. So people understand there is no mechanism for library sharing besides stapling this like including the code into the contract yeah it's all right. So obviously there's no space saving like all these contracts are going to have the same code within them. But addressing the concern of people implementing these things correctly. If everyone's using this common piece of library code, that has either been audited or people generally have more trust in. Then then. Then they don't really you don't really have to worry. So much about the mutability concern. Because they're choosing to build, that into their contract at build time there are some trade-offs there like

[36:00] I think it would generally be a thing, that would occur, that people would probably provide some of these like very standardized off functions like single key or you know based off of seller accounts or stuff in a library, that you can use but. If you want like a stateful system like lee you were talking about aliases and, that's why I keep coming back to this like the stateful system is only really useful. If your state lives in a centralized place, that people can rely on you know like it would be really annoying for me to have to go and set my alias in every single contract, that I use like I could. But it just, that just seems really irritating I think people would much prefer a system where. If that's an option there is some global contract for, that state limits basically I guess well I guess people could you could have a really simple contract it's just like an aliasing contract and you could have library code, that uses, that I don't know there's a lot of options here

[37:00] when yeah like it is to me super interesting like is, that. So like what how do we think about, that like. When you make. So this is like the smart wallet type of like situation where the smart wallet is kind of shielding or separating the whatever key you're using at, that time from your persistent ID on the network I wouldn't necessarily call it like a smart wallet it's kind of tangentially related I guess it would be what I'm really saying is like I sign using key x. But my public key always stays as y and I can change x to x prime or x triple prime. But my public key always stays y yeah. So for context I think the aliasing came about. Because we were talking about how do we replicate what exists on Stellar today in the smart world and what we have today with mo like we often talk about Stellar multisig. But

[38:00] actually the other component of Stellar's multisig, that we get is aliasing. Because you can have an account identifier. And then you can attach other keys to it. And so Stellar accounts provide these two concepts aliasing. And then multisig and I think you know there's been the concern address presented, that you know we shouldn't just implement the multi-sig, that exists on classic over on smart. Because there's a lot of trade-offs with doing, that. But the aliasing alone is like a feature, that I think, that's worth us exploring like what will, that look like. Because it allows people to do things like rotate their keys or have multiple devices or using the same address and. If I understand correctly John you're saying, that aliasing

[39:00] capability could actually just be a contract yeah, that's exactly what I'm saying any other questions on [CAP-52](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0052.md) I think. Because it's. So fresh we probably need a bit more time to get into the weeds John could you elaborate a little bit on how you see replay prevention happening. So I see in the CAP, that there's this announce, that concept exists yeah could you elaborate a little bit with how you see contracts would typically do, that yeah I'm happy to do, that. So this whole replay prevention thing

[40:00] gets kind of annoying in this proposal, that's one of like the big downsides of this approach, that I pointed out in [CAP-50](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0050.md). When I said like why we shouldn't do this, which is basically like every contract ends up implementing their own replay prevention. When wherever it's needed and this means, that like things get pretty annoying fast for example like on Stellar you like consider stuff center today you might like submit a transaction and you know you have replay prevention on it. Because the sequence number and you also have like you know a deadline effectively you know the max time and you know, that. If you get to, that point everything is done it can't execute or it has already executed. But like. If you want, that functionality here you also have to implement the deadlines in your contract and all these other things and everything just gets kind of annoying fast basically. Now again same kind of thing, that i

[41:00] was just talking about you could actually. Because like in this approach. Because everything is done by signed messages you can actually delegate all of this to like some other contract, that deals with it. So you can imagine implementing Stellar's times you know time downs and ledger bounds and all, that other stuff in a contract and reusing it. If you still want it or you can rebuild in your own contract as well. But but basically there's no generic nonce here and the CAP goes into a little bit of detail about like why you can't use the sequence number and they're like my original proposal here actually had an example where the sequence number was like the transaction sequence number was used as an us. But it had a couple like kind of annoying details about it specifically like such a contract is like really vulnerable or such a design is really vulnerable to what's it called confused deputy

[42:00] problems and. If you try to fix the confused deputy problems. Then it becomes impossible to use the sequence number as a replay prevention tool. So there's a lot of trade-offs here basically like I can imagine an argument where we just say like hey like people should be cognizant of their confused deputy problems etc and we make, that an option again I don't know. If I would personally feel good about, that. Because confused deputy problems are like a very difficult foot gun to deal with I think like they're an easy thing to overlook. So so I don't know. But basically yes every contract is building their own replay prevention or relying on it from somewhere else on chain got it I think one nice side effect of not exposing the transaction source account on sequence number two the contract is, that contracts are getting really set up for, that common relay

[43:00] pattern, that we do see in other ecosystems where people design their contracts so, that the message, that's getting signed to be used like the contract call, that's getting signed to be used on chain is independent of the participant who's actually submitting it and paying the fee and, that, that participant could be like a third party, that has you know is playing, that role of relaying making sure, that the transaction is on network. So in some ways it's nice it sort of sets up contracts to really work well with, that because. If a contract is written to use the source count you don't get like you would have to. Then modify the contract to make it work with a relay yeah definitely you mentioned this to me like I don't know a week ago or something and, that idea really kind of stuck in my head. When I was writing this. So I totally agree with you, that's a huge advantage of this design

[44:00] I think something, that i've heard I think maybe grading like raises a concern is. If we encourage people to write their own replaying mechanisms, which I don't think we can actually really get away from. So maybe it's not worth having this conversation but. If we encourage people to write their own replay mechanisms people may write replay mechanisms, that are really inefficient say is storing data on chain forever type of inefficient do you think there's anything, that we can provide, that'll like maybe some utilities, that we can provide in the SDK or even in the hose functions, that might help people write replay prevention mechanisms, that are more efficient, that you know use the ledger in a less aggressive way I haven't given, that too much thought honestly

[45:00] honestly. But like I mean my general kind of perspective on this is you know it costs money to use the blockchain and people will be incentivized to do things, that cost less money. So so basically. If there's a reason to do a really inefficient replay prevention mechanism. Because it makes the rest of your contract much simpler or maybe it's the only way to even do it. Then I think people will do, that. But in the absence of, that need I think people will favor the super simple mechanism, that's, that's cheap whether I can guarantee, that I don't know and whether we can provide some utilities I'm not really sure I mean like a really simple replay mechanism like a sequence number is basically like you have a map you look it up you check you increment, that's it could be hard to make it much simpler I mean like we could provide some like library functionality, that literally does, that exact thing. But the thing is. If you have an account, that already has per user data you would probably want to of like wrap the

[46:00] non-sin with the other per user data. And then the helper is not actually helpful in, that case so, that's kind of the main perspective there. But I have been thinking in general this is like a bit of an aside, that it would be really helpful. If the SDK provided some types like for example like there's the sc val map type, which is like a map in the sense of like a conventional map. But i've also been thinking like sometimes it's like you want to look at the data as in like I have a bunch of data stored in different ledger entries and be cool. If there was a like a map type, that did, that very easily instead of having to use like the contract put contract gap etc in [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md). So maybe there's some interplay between, that and a replay prevention mechanism, that we could learn from yeah I mean I think, that yeah all those things are going to like as we develop like even basic applications will factor you know like

[47:00] this type of basic functionality in some traits right, that people would just use I'm not actually too concerned about yeah people having to write it. Because we are going to write it I think for more like maybe like different type of like replay prevention like I think, that's kind of the nice thing about this proposal is, that I know in the past we discussed like you know potentially doing very different things like where you have like those like ephemeral type of you know like things, that only work in a specific time period right so, that you could in theory like replay in a specific window. But in some design it's actually acceptable. And then you end up with like much simpler client-side code. So so yeah

[48:00] and can I ask it don't question. But you have a function in here called knots of how does, that work what does, that do it's just a contract internal function I don't know. If the com comments emphasize, that they don't it's not a host function basically it just reads the data I just read the larger entry and find the nonce in, that would probably just be a single you know integer sort in a lever entry non-sub in those cases is user maintained data associated with an address yeah exactly contract maintain did I want to say. But yeah okay in general I mean I'm a broken record I don't want to waste too much time with this. But I'm I am extremely nervous about suggesting, that users roll their own authentication mechanisms I think this

[49:00] is just this is just asking for disaster. But but I understand, that we've been around this like a lot. So you don't need to convince me I mean providing the really simple authorization, that mechanism is the only one I would strongly favor, that approach this just feels like it's going to be a disaster you're going to have people who completely fail to. Because this data this code path. If you get it wrong is came over for everything and it's. So easy to get it almost right and your tests pass and you deploy it and you think everything's fine. And then it's not. So I'm I would love to not have users writing this code. But okay. So you know obviously I think we need to dispense the value in this proposal in the meantime we have 10 minutes

[50:00] remaining and i'd love to hand it over to Siddharth to talk a bit about the changes to the smart contract life cycle and potentially any remit open questions, that we need to answer yeah. So the most recent change was a small one about how the contract ID, which is. Now a hash is calculated and you can we can look at, that change it's pretty simple where the it's created from a transaction you hash source account a user provided salt and. If it's a contract created within another contract you hash the parent contract ID and assault provided by the contract I mean. If there are any questions there we can talk about it. But I think the more interesting thing are these two other points I want to bring up one is mutability, which is do we plan on adding like initially the CAP right. Now does not have mutable contracts. But the question is should we leave, that question open for the future

[51:00] or should we just say contracts will always be immutable and the second thing is we allow contracts to be removed. So the CAP has a host function to remove the contract code entry. So I think we can start with the immutability question right. If so. If we do allow mutable contracts in the future a big question was how do we deal with versioning and I was taking the approach, that let the contracts deal with it. So for contract a calls contract b and contract b is mutable contract may just trust, that just trust contracts bees creator or right. So I think grading had some issues with this graham you want to talk about this well I think they're just I keep coming back to the general sense I have, that

[52:00] cross-contract calls are something like dynamic linking or package dependencies in software in general right, that smart contractor software and this is a general software versioning problem and in general software has like natural tensions around versioning, that people frequently want to lock to particular versions. But they also frequently want to get the newest latest and there's a concept of newest latest, that is not compatible, that is often expressed in major version numbers or separate apis or separate names for things and I'm concerned, that we are not reproducing any of the infrastructure, that would be normal to support points on, that natural tension. So I think it is worth trying to provide some of the

[53:00] building blocks, that people are going to provide themselves anyway. Because it's bad enough to have like versioning it's worse. If there are multiple versions of the versioning system and you have to like opt into different versioning regimes depending on, which ecosystem you're adhering your contract to like I understand there's just there's this natural tendency in our conversations here to try to push everything to the ecosystem and let the ecosystem figure stuff out let them develop patterns in the smart contract space, that just solved the problem. However contract users would like them to but, that is not actually as much of a solution as I think it sounds, that really strongly introduces the possibility of totally incompatible regimes developing in parallel or inadequate regimes, that miss some important aspect of the design. Because they were cobbled together in a hurry. So I wouldn't mind us spending enough time to be able to provide the basics, which is like I want to pin to a version

[54:00] I want to pin to a major version and only get security updates or I want to follow you know any new features and additions, that people had including modifications upgrades whatever I feel like there's got to be a future where those are things, that someone's going to provide and it's our position to potentially furnish them maybe it's not but, that's what it feels like to me. But like at our layer like is it just having a way for to have like committable versus mutable contracts and like the versioning is metadata basically and it's up to. When you write a contract you know to decide how you want to use this metadata okay like. When you make a cross-contract call at the moment a cross-contract call only identifies a contract ID right and it does not say call this

[55:00] but give me version five or whatever like there's no version information in cross-country calling right now, which means we're essentially always dynamically linking to either immutable exactly the same thing or immutable whatever the person up to updated, that contract with the thing is, that I mean you have a bunch of things, that feel like. If we do, that at the kind of protocol layer we're kind of printing those like for example like. If you're talking about different versions right of a specific contract like you have what is like there are a bunch of questions around like you know you have a 1.0. But maybe the 1.1 is actually deployed by some somebody else right like it's not even the same author like how do you deal with those type of situations

[56:00] like who do you trust to be 1.1 I don't think we have the notion of oh like a of actual like or like you know the organization or whatever, that is the publisher right over have a contract yeah I agree we don't have any notion about, that I'm advocating for us to come up with a notion rather than saying but, that notion like I think it becomes I think in, that space I don't think you can necessarily come up with a one size fits all. Because it's not like in you know in like normal I mean they say like normal software like you have a company and they ship their thing and, that's kind of it like here and also like. If you depend on a specific version right as a my installer basically is going to just grab cause the os right to grab, that version

[57:00] that I depend on only in a blockchain type of situation. If that other version like the cost of keeping, that other version around is actually on the on whoever deployed, that contract so. If you say like oh I want to pin all version like everything is always pinned I think, that the implication there would be well okay the publisher is. Now has to keep around all versions of their contract, which is kind of you know weird okay. So is could you make a concrete proposal here are you talking about you would like all contracts to be mutable immutable not immutable. But mutable. So you can mutate them and. If I ever want to have a pinned

[58:00] version I vendor it is, that what you're saying not necessarily. So I think it's like yeah you could have always vendor of course I mean, that's a solution and I'm okay with, that I just I think we have to think through the scenario is what I'm saying and I think. If if what we're saying is we're not going to give any thought to the scenarios whatsoever. And then we're going to tell everyone to proxy every single call they make. Because that's the only, place they're going to have any ability to enforce policy I think we're kind of losing a good chance to shape the system I think it's more like what are like the things, that are like support like a, that you get with tooling, that you know we can provide versus things, that are actually baked at the protocol layer like I think, that. If you have a way to say. Because you know you'll also have the other problem right of

[59:00] contract discovery on the network like you know, which net, which contract do I trust you know versus the ones, that I partially trust versus the one I don't trust right there are some, that were basically you trust you try, that contract, that even. If it was mutable you trust, that they are not going to or you're actually fine with them modifying it in other situations you do want to pin. Because you have like a you know an actual I mean maybe it's a stability thing or whatever right there you have like other problems there. So so some of these data yeah like the mutability aspect I think, that's actually yeah, that's a property of the contract so, that you know basically can I even directly depend on this from my contract or is it potentially going to change under me. But then you have a different problem I think, that is kind of like. When you do you

[01:00:00] know like with you know. When you have your manifest files. When you're. When you decide, that you're going to pin your dependencies right in your own program, that decision is something where I think we have to just develop the right tooling. But it's not something, that actually the you know the network doesn't shouldn't have I think an opinion on you know like the this even the schema of how you pin things like I said I guess I'm saying, that the network has to provide whatever is necessary to support it. So we come up with what the network needs to support it and I'm not seeing, that developing in our conversations we talk about like bad things can happen. And then we throw up our hands and say obviously we can't solve it like no we have I think we don't fight this one I agree we just try

[01:01:00] to figure out what the pattern is. And then figure out what the network needs to actually support it right I think it's we need to have to kind of to solve those problems what I'm saying is, that we don't need to. Because like the things, that i've heard. So far they are in the context of a CAP we are saying this is going to be like we kind of forced a specific model at the protocol layer oh and I see a raised hand who is, that I think it needs to be in a cup like it needs to be in the design rationale for this cup like. If we're saying, that we're going to provide this limited set of functionality and you can go and do whatever you want we still need to provide in the design rationale this is how we expect it will solve this problem of versioning I mean I think like at least I can already see like there are some gaps here you know. If we say, that

[01:02:00] you're going to do all contracts are immutable. So with an eye and you're going to do versioning yourself data migration how's data migration going to work. Because like right. Now we have [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md) and contracts can't access other contracts data. So how is contract you know v1 going its data going to be migrated over to v2 and then. If we say, that you know v1 and v2 are gonna coexist at the same time you know how do contracts do, that seems rather complex yeah and especially yeah and actually like yeah in the context of [CAP-53](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-05.md) I agree, that it's also you would change your like. If your ID on the network is your contract like your you know the contract is your ID type of thing you don't want to be changing your ID like. If you upgrade, that contract

[01:03:00] we got a request to speak from the audience I know you don't know how to take them I'm not I don't think after right it's android question hello can you yep we can hear you great thanks appreciate the conversation and glad I was able to hop in I'm going to keep it short. Because it's going to be slightly off silo of what you're discussing I am a week away from dropping a tokenized community with a coin, that I'm bringing over from raleigh into solana and I want to develop on Stellar and i've looked up online there's some people, that you know help you punch I you know I'm just trying to understand based on what I know with moving forward I mean who can I connect with so, that i

[01:04:00] can understand the liquidity and how the Stellar network works and how I can make sure, that same liquidity I'm probably going to lose it. But how I can transfer over to the Stellar network and really make sure, that I look at all aspects of where I'm launching. So they don't have to wrap and do it differently in a month do you know what I'm saying yeah cassandra I appreciate the question this specific conversation is about a very like specific topic technical topic and you can go to the to one of the other channels support for example ask, that question and i'll be happy to help you there okay thank you I'm just looking for a personal contact I'm not bringing in the conversation I just I can't find anyone and i've left a couple messages in the chat. But I'm not sure who to talk to. So who is just speaking. And then i'll tag you okay. So we're over time and I think

[01:05:00] there's some really interesting discussions here, that we probably need to continue either on the Jump Cannon dev channel here on Discord or on the mailing list. So I think we should go with, that thank you all for joining and tuning in and have a great rest of your day you

</details>
