---
title: "Rent Payments and Bump Primitives"
description: "Discussion of Soroban state rent, archival storage, and configuration upgrades, covering rent mechanics, security tradeoffs, and approaches to managing large network configuration data."
authors:
  - dmytro-kozhevin
  - garand-tyson
  - graydon-hoare
  - justin-rice
  - nicolas-barry
  - siddharth-suresh
  - tomer-weller
  - tsachi-herman
tags: [soroban, CAP-46, CAP-46-9]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="45zgjaRBdW0" />

This session combined two major threads: how Soroban network configuration should scale beyond ledger-header limits, and how state rent and archival storage can bound ledger growth without breaking contract safety. The discussion emphasized that Soroban parameters (like metering and fees) are too large and flexible to live in the ledger header long term, motivating new upgrade mechanisms tied to dedicated ledger entries.

The second half focused on state rent and archival mechanics. To prevent unbounded ledger growth, Soroban introduces rent-backed contract data that expires and moves to an archive. This enables pruning unused state while preserving recoverability, but introduces subtle security challenges around replay protection, versioning, and key collisions that must be addressed at the storage-model level.

### Key Topics

- Limitations of storing Soroban configuration (metering, fees, limits) directly in the ledger header.
- Three approaches to scalable configuration upgrades, with strong interest in using Soroban ledger entries as upgrade inputs.
- Validators voting on hashes of configuration sets, not raw data, to avoid SCP bloat.
- Using Soroban and ledger entries to make configuration proposals on-chain and auditable.
- Motivation for state rent: bounding ledger growth and incentivizing cleanup of unused data.
- Archival storage model using Merkle proofs, with validators storing only archive roots.
- Security risks from archival state, including nonce replay and restoring outdated data.
- Three Soroban storage classes:
  - **Unique storage** for data that must never have multiple versions (e.g. nonces).
  - **Recreatable storage** for mergeable or replaceable data (e.g. balances).
  - **Temporary storage** for short-lived, non-recoverable data.
- Contract-level responsibility for resolving key collisions when restoring recreatable data.
- Role of preflight and RPC nodes in fetching archive proofs and preparing transactions.

### Resources

- [CAP-46: Soroban smart contracts framework](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046.md)
- [CAP-46-9: Scalable network configuration storage](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-09.md)

<details>
  <summary>Video Transcript</summary>

[00:00] So Darth do you want to give us a quick overview on the scalable configuration issue yeah I can do, that can you hear me yes I can hear you okay perfect. So yeah the issue at the moment is, that all current network related settings are stored in The Ledger header and this isn't a good place to store you know the Soroban settings. Because they can be quite large I think the current estimate for meeting or alone sorry for metering alone is. When kilobyte. So before we go through the approaches I'll mention, that they all assume the use of dedicated Ledger entries for storing settings as described in [CAP-46-9](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-09.md) and, that the settings will be created on protocol upgrades the only differences in these approaches are how the settings themselves are upgraded. So approach one, which is described in [CAP-46-9](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0046-09.md) this requires a mechanism for validators to introduce and distribute setting upgrades directly to the overlay the validators will. Then vote on the hash of the flooded settings

[01:00] approach 2 allows users to use Soroban to introduce possible setting upgrades into the system using arbitrary Ledger entries allowing validators to vote on one of these entries to overwrite an existing setting we use Soroban here. So accurate fees will be charged for the large for the larger operation and Ledger entries, that will be introduced there's still a question of how the user will interact with sore button we can add a new option to the invoke host function up or rely on a smart contract itself to create these entries approach three is similar to approach two except, that a new Stellar classic operation will be used to create the entries, that validators will vote to use to make this Opera to make sure this operation isn't spanned the idea here is to make sure the to make the operation withdraw a large amount of excellent from The Source account. But this idea needs some additional work to guarantee, that the operation always has a sufficient balance for the withdrawal once the operation has been flooded. Because you don't want the

[02:00] operation to get flooded and fail. Because the network will have already flooded the, that doesn't work for the operation. So yeah, that's the summary of the three approaches and I know their opinions on and I know their opinions on all these. So the yeah this is Homer do you have any opinions on what would you prefer Sid I have a question. So can you tell us a little bit more about what is stored in, that in these network configurations like concretely and why is it important to preserve these, that data. So for example from you know for

[03:00] metering we need we have different parameters on how specific things get charged. And so the structure will have you know say like you know a bunch of u64s, that indicate how like specific settings should be charged and, that is what sorbonne will use. When deciding how metering should be used. But we want these settings to be configurable, that's why it's not just hard-coded in Soroban the settings will be stored somewhere and, which and the value should be able to upgrade the settings whenever they want. If all the validators agree does, that make sense yes it just it's it doesn't sound like something, that would change frequently between two subsequent Legends right. But it's more something

[04:00] like yeah. When we'll go to upgrade to protocol Version Y. Then we need to make, that update am I cool we don't want. to tie the upgrading of the settings to protocol upgrades right for example right. Now we can update I believe like at the Base reserve. If we want to without updating the protocol right so, that we want this to work the same way and well I agree, that we won't update and well I agree, that we won't update the settings very often it I wouldn't be surprised. If we you know did it twice in this specific protocol version it's just generalizing what we're already doing a letter header to a much larger number of number like this it's in a potentially unbounded number of numbers, although it's not like enormous it's not thousands and thousands of entries. But it's probably tens or possibly as much as 100. So yeah does it make sense to do something

[05:00] in the like of having an optional field in the Ledger, that would have, that data and in any subsequent Ledger header just say hey in the in Ledger number XYZ we've last updated these parameters. So I mean, that's how I think we're describing is how like current upgrades currently work like you just you submit the upgrade to upgrade specific setting the issue here is, that the settings can be here is, that the settings can be quite large right. So it's not it doesn't like you know sending a one kilowatt XDR blob in the current mechanism wouldn't work it actually wouldn't work at all next in the with the current xcr. Because we have a limit of I think 128 bytes per upgrade but, which is why we're exploring like a more scalable solution

[06:00] Ed can I ask you a question about the other approaches, that are the other approaches, that are not approach number one here in approach number two for example is this is you sort of you've got two things, that are in play one of them is sort of propagating promulgating the proposed changes the set of options and one of them is a consensus vote. If people vote on two different config settings you know what happens in approach to like there's still a consensus conflict resolution problem the same as you would have in the first one where there's this arbitrary you know take the largest upgrade choice, that the consensus layer is doing in approach two what does a consensus player do. If there's two con two conflicting votes it would be the same as what it is today right like. If you. If two. If there's two different votes on say the Base reserve I don't actually know what conflict resolution is on the Base

[07:00] reserve right. Now my point is, that it is currently happening in the consensus layer and it approach too it sounds like you're trying to take it out of the consensus layer. So then oh I want to say, that all the approaches actually are the same in terms of voting like in any case what leaders would vote on the hash of the upgrade set of the config upgrade set and the reason for, that is, that well we don't want to blow the SCP values as well. Because you have plenty of them and even the update upgrades are rare event we don't really want to have huge spines during this forever. So voting always happens on the hash of the config upgrade set and the default mechanism for consensus is, that. If you don't agree this is upgrade evolved against it. And then there is not an upholds yes yeah. But so all of this happens very quickly during the voting round and

[08:00] I'm not concerned with the I object to this vote like that, that's not likely to happen very often unless someone just you know doesn't schedule their validator it's the combining thing. If validator a uses you know arms upgrade for upgrade set a and validator B for upgrade set a and validator B for B then. When someone hears those two or nominator here's those two or whatever they do a value combined right they do a SCP value combined yeah this is this a not combinable right like, that okay you need a vote and for the contract upgrades is a hash of the whole set of upgrades, that need to be upgraded atomically like. So any conflict just fails immediately as we don't try like in option one I guess it is possible to do something weird there or you can you know have some

[09:00] fuzzy matching of the upgrades you know you say like this operation should take thousand units of gas and I say it should take a thousand one unit of gas and maybe it doesn't matter in sand. But it kind of introduces too much unnecessary surface for potential issues and bugs and sure I'm just saying, that looking at the document, that I'm reading right here it says, that the CAP picks the largest upgrades, which is arbitrary and requires some sort of conflict resolution it may want to only vote for a specific hash so, that sounds to me like a conflict resolution mechanism. But I'm just trying to clarify whether, that's, that's real or whether it's yeah maybe, that's not one or yeah maybe, that's how it is. But I think in all approaches idea for voting is the same like revolt on the hash of the whole upgrade set and Valderas have two agree on this hash exactly. And then the question is normally how we distribute the premature of this hash instead I do have a question on kind

[10:00] of like the implementation burden like what is between the different approaches what is you know what's the low hanging fruit and what's what requires like additional mechanics I would. So I would say approach to should be the simplest correct me. If I'm wrong. But it shouldn't be hard to you know add a function Soroban to you know create these entries. And then you know. When the like any mechanism to interact with, that function should be simple either a new operation or the invo coast function up or I think using smart contract is a little more complex. Because I think there's some there are more details around, that. But I don't think like I think in my opinion approach to the simplest approach three yeah I think we have to be careful and to make sure, that it's not spammable

[11:00] an approach one requires work in the overlay in the consensus layer and is the reason approach to is the simplest. Because it used it reuses the metering mechanics, that we have in Soroban yeah, that's, that's the only reason we're putting it in Soroban itself. So what does, that I would say I mean approach one is not hard either in the series to protect care for, that and they both approaches I don't think there is too much work anyways this one problem is there for surabana too like we figured it out discussing this concerns for example you know by the transactions it install vasms they can be pretty big too and we don't want to flood them for free basically and I guess in this sense I like it's maybe not necessarily a very easy problem to solve. But we need to solve it anyways. So kind of option to just piggybacks on

[12:00] this file with options three I guess we need to change something in classic mechanisms to make appropriately. So yeah I kind of from drinks it probably too is the best it's a long time for the week and use the amount of work additional work required yeah could you speak to the complexity of the consensus change, that's described in verse one it says, the consensus changes it would be complex is, that like. Because it involves the item fetcher. Because we would have to have a second like the way we do DX pitching is, that it yeah I mean it's again I haven't traveled I haven't triven this point about consensus layer complexity it doesn't it's complex per se just literally reuses the TX set logic. But the except logic is arguably not very simple either yeah replicating yeah I mean on one hand it's trivial

[13:00] that, that we copy something, that exists. But we are copying something complex. So maybe it's really better to just use normal Ledger mechanisms to put this interest into lecture and yeah not bother about I didn't yet another flooding problem the one question I have about, that is, that the consensus module has to actually be able to judge. When it has a given hash right. If there's a vote for hash X it has to. Then ask The Ledger hey the hash X actually exist. Because I'm not going to vote on a config setting, that I don't have right like in The Ledger right. So this is just moving it from moving, that question from item fetcher over to a ledger inquiry and, that's a synchronous Ledger inquiry in the middle of consensus right yes okay I have no idea. If this is a big problem or not hopefully are we okay with, that I mean, that's kind of the risk of my mind. But maybe the maybe, that's okay I

[14:00] think, that's probably less pain yeah the thing, that is kind of annoying I mentioned, that in the dock with the approach, that relies on consensus is approach, that relies on consensus is, that we would have to secure the we would have to secure the upgrades in some way right. Now upgrades. Because they are small we don't actually they are not actually signed. So you know values in SCP they are actually signed so, that you. If some valid signed so, that you. If some valid data all goes Rogue you can basically blame you know who introduced, that the value, that is kind of spamming the validators. So you can basically decide okay I'm going to remove this validator from my Quorum set, that's kind of the idea with the signature upgrades are not part of this. Because it's today they are small so. If we

[15:00] start to make to have to and the for upgrades not being included in the sign payload is the there's an option for validator to remove the upgrade from the value. If they don't agree with it. So for you know so, that basically you can still close ledgers you know with transactions even. If there is no consensus on the actual upgrades, which yeah I mean we didn't have you know. So much of those problems you know until. Now there's like you know broad agreement on like for example. If you want to upgrade the network right to a specific protocol version there was no contention I think we'll have potentially more contention in the future as we think of network parameters, that maybe are impacting certain contracts right like as every time we change like fees I mean like

[16:00] the yeah metering schedule right like we're going to cause maybe certain contracts to become more expensive. Therefore there is maybe a higher chance of disagreement between very data. So I think this the chance of conflict is higher in you know post-organ post-organ and, that's why yeah I think we may have yeah multiplications where somebody there are going to say Hey you know I'm I don't agree with this thing. So I'm going to drop, that upgrade and. And then until the conflict is resolved you don't want to be in a situation where the network is not agreeing on basically anything right anyway. So so, that's kind of why you can drop those upgrades and right. Now they are not signed. So we would have to either make them signed

[17:00] if we want to make them bigger or just not you know keep them small and you know, that's kind of the option too hopefully, that makes sense to people. So I do have a question, that relates to the user experience of changing the configuration right. Now you know validators you know just have like Stellar Core commands to control this is the idea to maintain the same experience across all of these different approaches or do different approaches mandate different user experience I think I can answer, that I mean it's easier to arm an upgrade for a hash like it doesn't require any changes. Because hash is a small like. If you needed to distribute the whole upgrade set as in option one and, that's durable. But it's annoying like it's annoying to do this

[18:00] as a or command you need to maybe extend the command interface to take a file instead of you know just to get request or something like this. So approaches like two and following a SIM for instance. Because it doesn't require any exchanges like we say Hey you like you want to upgrade to this set of configs with hash X here is a link to the lectures, that actually contains this entry. So I think the distribution is kind of linear in this it's easier to arm these upgrades. But it introduces the transactions, that needs to be submitted yes someone needs to submit a transaction this is true yeah. So the US does change. So in all approaches right. Because in the approach two and three you need to you know submit this operation. But in approach one the initial set of upgrades needs to be distributed as well right

[19:00] I guess, that would be maybe another solid core command, that the validator would submit and then, that's how, that would be flooded right. But to be honest I do feel like approach one is an extension of the current experience we have, which is you know the validators will coordinate the validators always need to coordinate regardless outside of the network right. So you know they coordinate on a Channel right. Now you know Justin tells people hey. If you want to upgrade or to change this value you this is the command you use right. So maybe the mechanics are going to be a bit different we're going to say hey this is you know an XDR take a look at it at a Stellar lab or SDC or whatever and this is the hash vote on it. So it's still like you don't introduce like another step of you know submitting transactions to the network well only one person needs to submit the

[20:00] transaction it's not like value there was once you know the thing is populated you know it's basically like today yeah basically whoever initiates the upgrade should send the transaction, which I don't think is a huge View. Because it's a big thing they need to propagate any weightages foreign got it okay, that makes sense yeah by the way this closes a kind of a potential you know communication problem, that we have today right like today. When we for example. When you know we as SDF say Hey you know we are proposing to upgrade the network to you know version 19 right like we did last year people look at, that message is like it and it's not signed we don't publish, that with you know digital signatures or anything. So in theory you

[21:00] could imagine somebody faking you know us. And then getting people to vote for something else I think the chance of, that again increases as you increase the complexity of those things, that people are voting on and the benefit of the having the The Entity, that is kind of championing for a specific change is, that. Because this is actually an actual transaction on the network it's also signed by The Entity so, that's kind of you know one of the extra pre-free benefit from option two and three. So essentially a configuration change proposals become on-chain activity yeah and actually so, that's something I wanted to kind of maybe. Because we didn't talk about, that too much like in the doc we only kind of mention it, that option two like one of the things I think, that this opens the possibility for

[22:00] in the future to have a like a point the validators to like Ledger entries, that are controlled by those so, that you can form many validators there's well there is a concept, that we discussed before right, that is actually in a paper, that was published a few years ago about the notion of governing and non-governing validators and I think this for certain things this will this may actually become like more interesting you know in the context of yeah Dell's, that are not the tier one validators, that are kind of managing certain aspects certain parameters, that you know of the network. And then the validators

[23:00] and. Then the validators, that are interested in actively participating this of course you know they get to they would not just blindly vote for those things. But otherwise other validators just might do, that right for those do, that right for those like maybe not as coupled to network operators right or maybe even need like additional voting or something right and you want, that to happen on chain and you want, more transparency or something right got it. So yeah it does sound like there's some added benefits to approach too and it seems like from an implementation perspective it's fairly simple do we have any concern about you know it's not immediately obvious, that Stellar configuration changes should be proposed on Soroban it feels a bit like we're using just

[24:00] Soroban. Because it's there Soroban. Because it's there and you know I don't find it offensive. But but like can there be you know issues from it and things like we want to do like in pure Stellar, and now we're introducing them to the storybond I think this kind of ties into like subtopic I wanted to talk about is whether or not to be as separating sort of an operations into like different operations in the transaction. Because currently you have just one golden vocalist function, that does a bunch of stuff and this changes it would do even more stuff. So I'd say using Soroban is just an implementation detail here and nothing prevents wake up an option two from being a separate operation and also we may want to do this to all the operations anyway just to you know have a single flat hierarchy of things you can do to discover

[25:00] which basically would make it pretty opaque in terms of like how exactly transactions have been executed I think improves the X sound would I think improves the X sound would. So I can. So the derived question here is whether we want to split the invoke host operation into like multiple types of cerebon invoking operations yeah right like in this particular case like I feel like it's a race a strong case for making it a separate operation it was very explicitly namely containing internal assumptions, that you know people don't randomly submitted and for all the existing operations there is the same consideration I know people have any opinions on this would be accessible from the story would be accessible from the story about environment in addition to being

[26:00] accessible through the this dedicated operation no I mean the same about the current invoke host function transaction operations, that annoys me and, which is why I'm proposing this is, that we are not actually invoking a host function in the same way you can invoke it on yeah from the contract like they are Divergent they like the cost functions, that we invoke from Stellar Core are not the same course functions, that are more invoked from contracts. So the name doesn't make too much sense I mean even, though initial intent kind of made sense. But is there any implementation and requirements ended up they're not quite consistent and the convinced we should keep it this is in this cost functional you know we could just be a bit more explicit and just say the operations and the adapter lessons you are doing without involved in the Waterhouse at all yeah my opinion

[27:00] yeah I think the original the was to make calls original intent was to make calls uniform and I think we may be paying a fairly high price to try to make two entry paths to call look the same you know we're sort of orienting everything in order to make, that one path prior to use code well I think maybe like the this is more related to I saw there was like. If we have a dedicated host function for all this stuff I agree maybe we need to think about this. But like I'm not actually convinced, that this is the necessary to have anything first class for those network upgrades inside Soroban like I think we should be able to point to basically any saruban State should be considered valid for like as long as

[28:00] we can you know like I was saying in the dock like it's basically a byte you know a byte array, that we happen to be able to decode you know using. Because it's some XDR right, that we can understand, that the network level but, that in from a Solomon point of view I think it's just a byte array and. If we do it like, that there's nothing special about like any of, that stuff okay this is an important point, that I didn't understand. So from the approach number to your perspective we're not actually introducing changes to Soroban in order to facilitate this. So what is in the dock right. Now it talks about having special Ledger entries, that are like you know. So you need like a host function to basically manage to the special entries. But I'm not at you know. When we originally discussed, that I didn't think we would

[29:00] actually have dedicated legendaries for the for as input to the upgrade I thought, that at the end yes you know. When the. When upgrades are actually. When those network settings are actually active yes there are special entries on Ledger. But in terms of like how do we how do you get did they get fed into the upgrade function, that I don't think we need to have a dedicated entries you know with, that specific format as input would, that be at, that point in the dark was more to specify, that it's not a config setting entry I guess it wasn't you know it wasn't clear what it actually is. Because I wasn't expected out. But what Nico's saying is probably the best way to do this where we specify, that you know for an upgrade it'd be a byte array of a config setting entry or a byte array of a vector of config segment entries right

[30:00] I'm not sure I'm good at the point. Because they don't have an entry like sent to me it would just be I think with Nico's things it'd just be a contract data entry it's not a contract we don't have a contract. So it cannot be contract data you know you can no like is anybody like you know as part of this right like you can imagine, that's why I think like as. If you think of in the future you want to have some doubts doing, that like it's just a special case like. If I want to in the first iteration of this the way you would do it is you have a dummy contract you know, that has like State you know associated with it, that is just a single you know, that has a single value in it and, that allows you to basically you know as a user I can persist I can yes like kind of like a balance right

[31:00] for you know for token contract. But instead of a balance it's basically a binary blob. And then in the upgrade, that's how you get it on The Ledger all right. And then the upgrade just points to, that specific Ledger entry, that contains the binary problem of Interest. So what do you guys suggesting we would have a contract specific cost function no there's nothing specific to this it's just like yeah to in order to construct how would we vote and wait like it has to be special right like a similar proposal and option for opposite there needs to be a way like since we are voting on a hash or something you need a straightforward way of finding this hash in The Ledger yeah, that's why you can point to any arbitrary contract data right you're using the Ledger key

[32:00] a hash yeah you bought an electric key you would vote on a pair Ledger key hash probably right like maybe you can make it work with just a ledger key. But like, that I you know for. Now I think it's still a joke right maybe we need to yeah we need to maybe like sketch, that a little bit so, that I can. So basically you would vote on say I mean, that's quite inefficient right I guess the best you can do is contract 80 plus some data ID and accurate format actually I do. But so I think this has been super informative it sounds like there's a very strong bias towards approach number two here and there are some details, that we need to figure out like whether this is actually something, that's special cased in Soroban or it's just a general

[33:00] in Soroban or it's just a general ledger entry. And then the validators is part of the upgrade process just pointing at this Ledger team. So let's I think we can take this conversation offline about the details and. If needed we can bring these up in one of the next meetings we have around 20 minutes left. And so before we go on, that Siddharth do you think we have enough to keep prototyping here yeah we should be good okay. So we don't have a lot of time we have 20 minutes and Garand is here today to start talking about State expiration, which is a very big hairy topic. So garen's gonna give a very brief overview about this today. If we have time we're going to do Q&A I imagine, that we're going to be talking about these about this specific issue a few more times. So no big decisions will be made today during the stages

[34:00] yeah. So I guess we want the time in time. So I just wanted to kind of start out with the general motivation. And then talk about more about the interface. And then we'll leave the implementation details to probably future conversations. So essentially the issue we're trying to solve with archival state is this issue of unbounded Ledger State size at least right. Now it's not a classic the number of Ledger entries we have to store is growing and there's not a strong incentive for users to delete entries additionally a large amount of those entries, that exist on The Ledger are either outdated or just won't ever be used again for instance we have a lot of climbable balances, that are more spam-like entries, that aren't used very often or won't be used at all they're taking up this Ledger space and increasing larger bloat. And so for the healthy skill patterns of the network we want to be able to essentially CAP Ledger State size and not allow for this arbitrary growth and to do, that we want to essentially delete entries, that

[35:00] aren't being used. But somehow not delete and keep the entries, that are being used often. And so in order to do, that what we do is we want to implement this concept of rent. So essentially for all Soroban smart contract data entries the data entry will have a rent balance, which is some amount of XLM, that is reserved for, that entry to pay a rent fee. And then as, that entry lives on The Ledger it will have a rent fee deducted from, that rent balance. And then whenever, that entries rent balance goes to zero the entry is deleted from The Ledger answers. If it never existed before. Now this is obviously it opens up some issues right. So imagine you have a wallet or a balance, that stores a lot of tokens, that are valuable you wouldn't want this entry to be permanently deleted and lost just. Because you forgot to pay rent and forgot to up to rent balance. And so instead of even, though the entry is permanently deleted from The Bucket List what we do is we take, that entry and we send it to a special

[36:00] kind of node called an archiver node and this archiver node is essentially storing all of these entries and then. If a user. Then wants to use an entry, that has defaulted on rent. And then been sent to an archiver node. Then what they have to do is they have to go pay a fee. And then retrieve, that entry from the archive node. And then give, that entry back to the validators. And then once, that fee has been paid the validators will. Then take, that archive entry put it back on the bucket list. And then this entry can be used as. If it was never deleted. And so kind of the implementation details as to how this work is the archive is implemented in a Merkle tree-like structure. And then in order to restore an entry from the archive. And then put it back on the ledger. So it can be used again you have to provide a proof of inclusion, that this entry, that you say was archived actually does exist in the archive. And so this Merkle structure is very powerful. Because it means, that validators are able to check and make sure, that the entry you say is in the archive is actually legitimate

[37:00] but the validators don't need to store the archive all they need to do is store this root Merkel hash. And then they are able to validate proofs, that are generated from the archive. So kind of high level how this works is the validators themselves don't store any of the archive State and cannot produce these proofs. But the archive of the archive nodes do store the entire archive State and have enough information to produce these proofs. And so the archive nodes we kind of Envision to serve a similar purposes kind of Horizon where they are supporting the validators. But are not directly involved in consensus. And so from an interface perspective this has a couple of issues, that we kind of need to discuss especially. When it comes to security and so. Because the we want the validators to store as little State as this archive is possible they don't store any of the keys and they don't know what's in or not in the archive and so. If you can imagine an example say we have like this token

[38:00] contract right and you have a balance on, that token contract. But you don't use it. And So eventually the rent balance on, that entry will go to zero and, that entry will be deleted from The Ledger. And then stored in the archive. Now once you default on rent from the perspective of the validator it's as. If this entry has never existed before and. Because the validator doesn't actually store the archive it has no way of knowing, that this entry used to be in the bucket list or, that this entry is currently in the archive. And so this means, that. If say the smart contract was to create another balance with, that exact same address this would be allowed right. Because the archive or. Because the validator doesn't know, that this entry and this key exists in the archive it will create and generate new keys, which means, that we have this issue of key collisions where you can imagine. If this process was repeated several times it's possible to have an entry with multiple different versions of, that entry with the exact same key, that exists I'm simultaneously on The Ledger and also in the archive

[39:00] now for some types of data these key collisions aren't, that big of a deal right so. If you can imagine. If you have say a balance of some amount of token with your given key and there are multiple different balances you have. So say you have 10 XLM in an account and then, that 10xl imbalance gets archived. And then someone sends you another 5 XLM and eventually, that entry gets archived with your key there are two entries in the archive, that corresponds your balance one with 10 XLM one with 5 XLM. But this isn't, that big of a deal. Because both of them are valid right you can just restore the 5x on them balance spend, that 5x Alum. And then restore the 10 XLM spend, that no problem. Because both of those balances are valid even, though they have the same key. But for some data types, that's not the case right so. If you can imagine a smart contract implementation, that uses a nonce value. So let's say, that we have something like USDC, that uses a nonce to protect against double spends and make

[40:00] sure, that transactions can't be played say, that this nonce value is something like 10 a non-zero value. And then gets archived well. Now this token contract. When it needs to do something the token contract will see, that there does not exist in nonce. Because from the perspective of the validators the contract itself has no way of knowing what's in the archive. Because the contract is running on a validator and the validation store the archive. And so whenever the contract sees, that there doesn't exist a nonsense on The Ledger instead of going to the archive and restoring the nonce with a correct value of 10 what the contract will do is just create a new nonce with a value of zero. And so what you can imagine is say, that we have the correct notes value, which is 10 in the archive and this new zero nonce. Because this nonce is incorrect and is essentially shadowing a valid version of the nonce in the archive this allows malicious users to execute a replay attack. Because they can take a

[41:00] transaction, that says hey. If nonce is equal to zero this transaction is okay and even, though the correct non's value in the archive is 10 the knots value on The Ledger is zero. And so they can replay this transaction and maliciously use this nonce value. And so we have this challenging problem, that for certain types of values like nonsense there needs to make sure, that you only have a single version of, that entry between both the bucket list and the archive this is a problem. Because the validators don't store the archive. And so there's no way of checking. If a key exists or not directly from the archive so, that's issue number one this thing like nonsense where you want to make sure there's only one version of the entry, that exists. And then the second or the second issue, that comes up with security is something where you restore an outdated version of an entry. So similarly let's think of a token where you have some sort of kyc Entry right. And so say, that you have a kyc

[42:00] entry, that allows a user to spend their tokens and then, that kyc entry isn't touched for a. While and. So eventually it runs out rent balance and gets sent to the archive. Now a new kyc entry is generated on the bucket list. But this kyc entry revokes access to those funds. Now let's say, that this new kyc entry, that revokes access isn't used for a. While and it too falls into the archive. But a malicious user might do is go into the archive instead of restoring the most recent version, that revokes access instead restore the earlier version of, that kyc entry, that gives the user access to spend those funds. And so by restoring an out-of-date entry what you can do is essentially do a versioning attack where you take an out of date entry restored onto the bucket list. And then you're essentially able to pretend as. If it is newer than the kyc entry, that revoked access. And so essentially we have two issues we need to solve here first we need to make sure, that you can only restore the

[43:00] latest version of an entry. So you can't do this like kyc rollback attack and second for certain types of data we need to make sure, that there's only one version, that exists in the archive in the bucket list. Now we don't need, that guarantee of the uniqueness guarantee for all types of data for example the balance example I talked about earlier it's completely fine. If you have multiple different balances. But for things like nonsense you need to make sure you only have one. And so essentially. Because we have these two different requirements we expose two different types of storage at the Soroban interface level right. Now we're calling this unique storage and recreatable storage. Now essentially the differences here is, that unique storage guarantees, that whatever entry you have there's only ever one copy of. So in our non's example. If you're using unique storage and you say you know I use unique storage to create announce value and, that Knox value gets sent to the archive. If you try to recreate, that value the function will panic. Because it says hey this is unique storage and entry already

[44:00] exists in the archive. So I'm not going to let you to recreate, that under the hood how we do, that and again this is an implementation detail for later is we use a combination of proofs of inclusion and also proofs of exclusion. So for instance whenever you create a unique data entry for the first time you need to also provide a proof, that this entry has never existed before. And so this proof needs to become from the archive nodes. Because the validators don't store enough information to prove, that something never existed but. If you provide this proof to the validators they are able to check the proof and make sure it's legitimate and so. Because you have this extra step for Unique data and needing to prove, that nature never existed it's more expensive. Now the recreatable data doesn't have this guarantee. And so for a thing for entries like nonsense, that have security implementation implications you wouldn't want to use recreatable data. Because you could have multiple versions of your nonsense and you could recreate nonsense and have security issues. However you would want to use

[45:00] recreatable data for something like a balance, that doesn't have this issue. Because recreatable data is much cheaper. Because you don't need the security guarantees you don't need to provide all these proofs of exclusion and whenever you create a recreatable entry you don't need to prove, that's never existed before. And so essentially we tried to provide these two classes of data so, that users who need some sort of security guarantee can use the more expensive and slower unique data. But for entries, that don't require those strict guarantees you can use recreatable data. Now in addition to these two data types we also introduce a third storage type called temporary storage and what temporary storage is it kind of just does what's the name sounds like it's entries, that are meant to be temporary. And so essentially. If you have a temporary storage entry whenever its rent balance goes to zero instead of being sent to an archive it is permanently deleted and. Because you don't need to worry about sending this to the archive and there's not this archival cost the temporary entries are

[46:00] the least expensive storage type. Now you wouldn't want to use this for sensitive data such as balances. Because if a balance defaulted on rent it would. Then be lost forever. But for temporary entries such as. If you want to give another user authority to use your funds for say 100 days you could use a temporary entry, that automatically deletes itself or. If you had some sort of data, that is easily recreatable such as like a payment Channel you could also use temporary data for, that. Because whenever it was deleted you could just regenerate the exact same entry again. And so these are the three data types in the storage mediums we're trying to present to the end user. Now the issue is we have these security issues, that are generated from the archive interface and they can be very tricky to protect against for instance it's a very tricky to think about examples like kyc rollback and nonce attacks in the context of this archive especially since archival state is not only a new interface for Stellar. But a pretty new interface for blockchains in general and

[47:00] so we want to provide an interface and Define use cases as clearly as possible for each type of storage and try to make this as seamless as possible for the end developer and trying to abstract away as much of the complexity from the archival interface as possible. So I think this is a pretty tricky problem. But I think, that's the high level issue I want to talk about today is just the security issues, that arise from this archival state. And then the three types of storage, that we've tried to implement. So far. And so I think I'd be open to any questions or. If anyone has anything I'd like to talk a little bit more in depth about bro thanks for, that just a quick question you just you were talking about unique storage and having to supply proofs, that does, that mean, that in order to create a unique storage entry I need to have access to an archival node yeah. So essentially how, that works is out. When you actually from the

[48:00] perspective of the validator itself. So yes you do need to have access to a an archiver node to produce these. And so kind of what we're Imagining the interface would look like is, that RPC nodes kind of double as pre-flight nodes and archiver nodes. And so during the free PreFlight process. If you generate a unique entry or. If you want to access an entry, that's in the archive of the PreFlight note itself will carry archival data. And so the PreFlight note will be able to essentially kind of work similarly to the footprint. When it generates Footprints for a read write data accesses, that the pre-flight node will be able to retrieve the proofs, that are required for your transaction. And then give those Trend or give those proofs to the validator and so. Because the PreFlight node is generating and providing these proofs whenever you actually go to apply, that transaction on the validator the proofs are available. Because they've already been made in advance by the previous flight node

[49:00] and just a bit here for recreatable storage I don't need, that right yeah. So for recreable storage it's cheaper. Because you don't need to provide any proofs and there's no work to be done you can just. If a key doesn't exist on The Bucket List you can just generate, that new key without any checks or balances no problem well to make an amendment like it's not like nothing has to be done through a created both storage. Because if you want to benefit from the fact, that it's recreatable you need to write some codes, that allows you to nurse entries for example. If you have a token contract and your balances are creatable you probably want to provide a contract function, that allows users to recreate there is a balance given approve basically. So like both involve something to be done. But in case of unique storage it would need to be done every time you create a new entry. While for a tradeable

[50:00] it's only. If your entry has got archived and you actually want to restore it, which hopefully shouldn't be the case too frequently yeah kind of the difference is in unique storage. If something exists on the archive you must restore it in recreatable storage. If the entry exists in the archive you have the option of either restoring it or just. If the restore would be too expensive or you don't want to restore it for some reason just creating a new entry with the same key. But but just to be clear with regards to what Dima just said. If I do have an existing. If the Ledger entry has been recreated with the same key. When I try to restore it right. Now will it fail or will it like use some sort of like custom merge capability what's gonna happen yeah so. When it comes to recreatable data there's a function, that we Define called an archive latest version and

[51:00] essentially how this works is whenever you restore unique data it's automatically add to the bucket list immediately. Because unique or for Unique data. Because unique data is known to be unique. And so you know you won't have a key collision between the archive and the bucket list. So you can just automatically add to the bucket list for recreatable storage this isn't true there could be a key collision. And so whenever you restore a recreatable storage entry it's not immediately add to the bucket list. But instead this function restore latest archived entry just Returns the unarchived version right. And so in the doc. If you look at the last two pages there are two example functions on how to restore our recreatable data to example implementations, that contract might do. But essentially. Because of these key collisions it's the responsibility of the smart contract implementation to resolve these collisions. And so for instance what you can do is after you get the restore

[52:00] latest or the unarchived entry returned from this function you can check the bucket list and see. If there's a collision and then. If there's no Collision the smart contract can just write the key immediately no problem but. If there is a collision. Then what the smart contract might do is compare the values of the recently unarchived version and the bucket list version and pick one or discard the other or in the example of balances there could be some sort of merge operation defined by the smart contract and so. If you have two balances the unarchived behavior might be okay take the unarchived version sum the balances of the version, that's live on the bucket list and the recently unarchived version. And then just write the result, which is the sum of those two balances and so. Because there are different use cases depending on the contract, that's a contract implementation, that this contract developer itself might think must think of it and is there a I see you have a section on default implementation what should be the default

[53:00] implementation yes I think the default implementation is just. If there's no key Collision IE. If you want to restore something and there's no key on the bucket list. Then you restore it and add to the bucket list. If there is a collision just panic and fail. Because you don't want it to be a no up. Because the an archive function does delete the entry from the archive right. Because you can't unarchive and entry twice from the archive once you've unarchived it once it's gone forever. And so you won't want to lose entries. If you can't. If there's a key collision. And so I think the safest option, that preserves all the data is for the default just. If there's a collision just panic. And then you either have to wait for the entry, that's on the bucket list to be archived or delete the entry yourself on the bucket list. And then the default unarchive will work

[54:00] okay I imagine there are a lot of questions unfortunately we are at time. So this discussion is going to carry on feel free to ask questions on live chat Garen. If you can hang out there and ask any questions that'll be great and yeah thank you all this has been a very productive hour and we'll see you all next week

</details>
