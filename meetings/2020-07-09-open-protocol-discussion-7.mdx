---
title: "Open Protocol Discussion (7/9/20)"
authors:
  - nicolas-barry
  - david-mazieres
  - jed-mccaleb
  - jonathan-jove
  - leigh-mcculloch
  - orbitlens
  - eric-saunders
  - tomer-weller
  - tom-quisel
  - justin-rice
tags: [legacy]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="8i7MCaEC5uE" />

- okay we're live alright everyone welcome to the open protocol discussion a biweekly meeting in which we review and plan for upcoming changes then provements do the stuff call we've missed a few meetings because people have been out of town but our plan is to keep live-streaming these so that the world at large anyone who's watching can see what we're working on and all the wonderful thoughts that you have about I don't like stellar better these meetings usually focus on reviewing court advancement proposals aka caps which suggests new features and improvements to the protocol and at the moment we're focused on protocol 14 on the upcoming protocol which will likely include two big caps cap 23 two part payments and cap 33 sponsored reserves today we'll cover a few remaining questions about those we'll also talk a bit about the downstream implications of those caps and the preparations underway to make sure horizon and the stellar SDKs are ready for protocol 14 um so we published some pre reads for this meeting there in the event description and this discussion will make up a lot more sense if you've done them so do those if you haven't um there's also just a rough outline of the agenda on the left of the screen um and there's also a list of participants in the event description and for this meeting instead of introducing everyone I'll just let anyone watching Google the names to find out who's who if that's not working in the future we can take some time to introduce ourselves but let's start there seems simplest so diving in quick status update cap 23 and cap 33 are in final common period they've been there a little longer than usual there were a few lingering questions discussion was a bit slow because people are way but barring any blocker is raised in today's discussion we will be night them with the coveted status of accepted so first let's talk about those cap 23 to part payments cap 23 creates a new letter entry called a claimable balance the new operations that allow you to create and claim a claim of a balance the goal is to separate the sending and receiving with a payment so you can send a payment to an account that isn't already prepared to receive it it looks like there are a couple of issues here and I will turn it over to John to talk about the first one sure so this this is actually this originates from a from a question that David brought up I guess a few weeks ago which has kind of been lingering but basically the question is like right now we're using dislike predicates tree structure which is kind of novel in the stellar world you know in other places we use like linear threshold combinations and stuff like that but here we have this predicate tree and David's main concern about this predicate tree is that you know XDR doesn't have any built-in mechanisms to prevent arbitrary recursion and so anybody who has an extra parser first of all like as a matter of principle you should be making sure that you're not gonna get infinite recursion but you could imagine a world in which you have some structure which is like sitting on the edge of the recursion depth limit for different nodes on different architectures and whatever and maybe some nodes choose that it's acceptable in some node trees it's unacceptable and then you end up in some bad undefined state depending on potentially a fork in a really bad case depending on basically just how everybody's configured but they're different XDR and so the question is like how should we update this predicate tree structure and David had this proposal to basically turn it into a into disjunctive normal form but I've heard some opposition from myself from Nicola and from somebody else that I can't remember right now who the third person is that this disjunctive normal form idea is not really that great because like complicated expressions end up becoming like exponentially large that's kind of not ideal so we could either decide that we don't think this is a problem and we're not gonna be anywhere close to the recursion depth limit because like probably realistically a transaction even with this predicate tree structure has recursion depth maybe like 10 or 12 and if your system doesn't support like 12 recursions like that's a problem hold on that the idea is that a bad person could submit it to purposely submit a transaction that like pushes the recursion depth right but that would be invalid no matter what they're automatically invalid if the depth is greater than four so like if you submit because we have logic in there too we have to be exactly actually there's a laterally invalid if any of the predicate trees have depth greater than four so like it might you know if your XD our compiler doesn't generate code that handles recursion like deep recursions well it might crash your node but like if that's the case your XD are set up it's garbage and you have a security vulnerability regardless of whether we've done this or not well if we pick a number like four we can actually admit it's ugly but we can actually express that in any XD are pretty good tree one two three and four is like four different types you know leaf one two and three or something I mean but so that much is how like do we really actually have use cases that involve all of this like the thing is you already have the disjunction because you can have multiple claimants right and so so the question is why not do something that has just a conjunction you know in in the claim predicate and then you know we can add something more if it turns out that's not useful enough or at least come up with some actual like realistic use case that actually requires like you know like four deep nesting of like hands and wars so so the four wasn't chosen arbitrarily the four has this like very magic property of making a claim of like a claim over balanced entry the same size as a or rather the claimable predicate tree the same size as a data field so basically like this bore was basically saying like if you're gonna pay reserved to get a data field you might as well pay a reserved to get the same amount of space and claim a little balance but so that's where the magic floor comes from but regardless of that I mean like for me it's not just about whether like everything I think sensible then you could express it you could express in disjunction format like I don't see any reason why you couldn't do this but the issue is like if we then subsequently add the conjunctive stuff then we're no better off than we are now like we'd be strictly worse in fact because we'd now have this the disjunction thing which was your proposal where basically if you get disjunction if you're the same claim into multiple times that's currently not permitted at all claimants are unique in this in the current proposal but then you'd also be able to build these trees and then you've like kind of violated the like one way to do it rule so like that's kind of my main like basically if we think we could ever end up in a world where DNF is not good then we shouldn't start in a world where DNF is the standard because we'd be no better off in the long run so that's kind of my main take on it one other idea I'll pitched you and I'm gonna credit this one too great in it's not my idea but graden mentioned that we could actually to avoid the infinite recursion thing XD ROP KO phi the predicate tree and like we can for example make the xt are opaque version of the predicate tree like 64 bytes right now and if it's bigger than 64 bytes you're dead like that's not gonna work and then later if we need to get bigger trees for example for hash Cree images or something we just increase that number but that provides like a default bound automatically I don't know I don't know how much anybody cares about this at the end of the day like we could definitely do the hard coding thing there's a lot of different options and I just like I don't know how much it matters to spend time trying to decide like do people care a lot are people really concerned about this recursion thing what is the what is the use case that's driving the the more complex like pretty good cheese well the main reason for having the predicate trees is just that it's a lot like it's a lot more powerful than having Linear's like signer weights basically like what we have on accounts I like there are very simple situations where you would want an or for example or maybe even an and or an or for example like you might want a situation where it's like from now until this time I can do something and then from some future time until some other future time I again have the right to do something and now you've automatically got this tree structure implicitly so but no but you can already have this because you already have multiple claimants right so claimants are unique right now they don't support repetition but we could do repetition sorry but oh you're saying even the subunits of the claimants if you have a conjunction of me before Tuesday and me after Thursday you're saying those are the same claimant so the same claimant has appeared multiple pi correct and that would be invalid so yes I don't see why would you prohibit that like I can actually see a number of reasons why you might want to do that because like maybe you want to maybe you want to be able to be claimed by like a two out of three configuration and so like you know I should be able to sign on two of the claimants for example so having repeated claimants actually seems like a very useful thing unlike this this alternating and in or four times I'm not I'm not sure I understand why the repeating would be useful like other than in this disjunctive normal form at world like why what usage could possibly come from that it's strictly less powerful than the tree like anything you can do with the tree you could also do with the claimants like repeated claimants but the opposite anyway I want to come back to like then I don't want to spend time about this if nobody's concerned about this recursion issue like my stamps on the recursion issue is that if the recursion issue comes into play you already have a security vulnerability because your XDR compiler doesn't handle recursion well or your XDR library doesn't handle conversion look Oh recursion well so not but the question is like do we define a depth specifically for this data structure or we just expect to conform to whatever your XD our parser knows how to do this one has a big depth limit there are two paths to it yeah there are two paths straight right there from a security standpoint you want to be able like you shouldn't never use in you know unbounded num you know amount of memory if you get untrusted data on the wire right and then there is the second part that is a protocol and fourth thing that is what is the maximum depth and it's fall so they in that case if that's really what we're gonna do I would argue against the the maximum depth because because that's hard to implement right so people get it wrong if we just have a maximum do we have a maximum transaction size because then because the thing is like and maximum transaction size would at least be easy to enforce where did I I think I'd rather I'm not go that route because like right now like John said like the four as it's kind of a nice property where we don't have to deal with variable based reserve requirement only as soon as we say we have we bound this to the size of the transaction now you're you're you have to come up with an onion model so this for how much to charge for return because now like they're all going to be potentially a lot more predicates that you can put Kristin operations can get very big so what's your how big is your like a bad payment is you know can get is very big so how big is your implementation of the of the recursion depth John how many lines I suppose for us Siddarth did it so I don't have it in my head right now but I my guess is it's probably like on the order of 20 that's how much I think it would take me to implement it and and how many places just invoke that function in the code because it seems like we're gonna have to check this in a bunch of different places right everything is just one yeah it is very eight because they're immutable so it's just like in the validity path you check like hey like how deep is this cool you're good I mean and how so we need like a like basically every we need a version in the SDK and then we need to like check that the two are equivalent right it seems like any time is it difference like any time an SDK can be convinced to create a transaction of stellar core we'll consider are incorrect or invalid that's it that's it that's exploitable right because I'll convince you that hey we're gonna do this thing where you can like get a claim a bunch of money or whatever and then it turns out that you can't because stellar core rejects the transaction so I really I don't like this idea but like yeah III don't think we should never do it like they're clearly times that's appropriate but it just it doesn't seem justified here to add like an extra validity role that is not like this is not a good argument I think because like in general like you have an expression and you're saying oh like I want to look at the expression and I expect it to just pass in call but like really like if this expression is bogus like it doesn't matter like you know it's going to be invited later right so maybe the reasons we should make it as easy as possible for SDKs and other software in the stellar ecosystem to replicate the validity rules of stellar transactions and obviously sometimes we need to add extra logic but given that we haven't cited a single use case that it wouldn't be satisfied by by a a simpler approach here it just it doesn't seem justified but again I mean I've said my piece I'm not I don't object strongly enough that I'm overruling this I did both to put this to FCP so you guys can heed my advice or ignore it I guess how would you feel about the like the hard-coded thing the you know least one to three thing better much better because like that would be easy to do and it wouldn't really impact the implementation at all because all I would do in the implementation is just parse it once the real like the leaf four way or you know depth for way and then I would just parse it again as an infinitely recursos structure once I check that it's fine and you know and then you can just like you know XDR start you know SDKs and whatever could just parse it as the for depth version and it would just work out of the box yeah we would not even use the the one with the labels in call yeah we might not even use the one with the labels in core exactly that's a good point because we know that we're secure against arbitrary recursion so no no but the whole point is we want the logic in core to exactly match the logic in SDKs right so so the well I mean we're paying the sophistication is full maximum depth fall right so it would be the same spec and they'll be giving a canned version to educator I'm just for I'm very skeptical that this the four is even necessary why don't we start with two and then if we need for we can do it yeah we could start with two two is fine but I mean like two hasn't helped you a ton in the sense that like two has exactly the same constraints is full right like you'd still need to check the depths and everything well no because two is like much less unwieldy to do with like a leaf in an internal rate like now it's sort of it's not that bad to have two different types for it also you would need well I guess like do you mean - is in like Route one - because no I I meant just like I just meant when you have an and or an or it's an andron or of something that doesn't include andin or of a leaf that's a lot more restrictive but again like what are the use cases here this feels to me like were we're building a mechanism without an idea of the use case for the mechanism right and you can always go you can always increase the depth right that's not oh yeah that's that's a third site you could always increase you know on the other hand you know it may turn out that the the main bottleneck here is that it's like it may be like very unintuitive to have like and an award like complicated boolean formulas for this stuff right like I think a simple one level like a disjunction of conjunctions is something that you know it - it really makes sense like okay there's like three conditions on which we can do this it's like this and this are true or this is true but once you start going to more than two deaths it starts becoming like fairly unintuitive like it's what you you know expect to you know feed into your Sat solver because you've generated from whatever but but it's no longer gonna correspond to sort of simple human expectations so do we actually expect that we're gonna be having compilers generate these like really complicated predicates and that we're just gonna trust these compilers to do something sensible or do we just like if we have a use case for that I'm okay with it but until we do I'd say why not just defer having that complexity I mean I'm not opposed to going to - in the short term and making it super simple and you know we can make it higher in the future when we need it like would that be difficult if we started shallow and decided that it needed to be deeper would it be difficult you know it'd be very easy I think that I have an idea how to implement this without the recursion maybe we can just allow the predicate to be an array of predicates and allow duplicate claimants in the conditions so with these two preconditions we can implement both and and our cases because duplicate duplicate claimants for example two or more claimants with the same account ID with the same destination will allow to implement our cases and and making the predicate an array of predicates educates it makes they were all soon easier it's intuitive it's like it makes sense it is fairly expressive clearly there's a lot of different opinions here so let's let's take this I think we've like we've talked out some of the details okay take it to the mailing list and try to come up with what's the right thing to do like the beautiful thing is like none of these changes are going to be very material to the implementation so we can plug in whichever one we kind of think is the right one but I don't want to take the entire time of this meeting talking about this one thing okay so the plan then is to make these suggestions on the mailing list evaluate them in synchronously and choose one for the implementation soon okay that's what we'll do then email after this meeting and kicking that off cool um then I or is there anything else that we need to cover on cap 23 or it's that sort of procedure forward with that I think we could probably take if people want to talk about the other thing about like changing this before-and-after notion to having a not we can do that but we can probably take that to the mailing list oops they're all kind of bundled up is one kind of topic so unless anybody really wants to talk about that right now I'd say let's take it to the mailing list great think it's the mailing list look out for that in your inbox everyone um get excited so then we'll move on to cap thirty-three sponsored reserves this proposal allows an entity to cover the reserve for accounts controlled by other parties without giving those parties control of the reserve that extends account entries and ledger entries so they were pertinent information about sponsorships it creates new operations to initiate and terminate sponsorship and to update sponsorship information for existing ledger entries the goal is to allow asset issuers and while it's never user reserves basically one issue came up which is that there was some incompatibility with step 30 which is a stellar ecosystem proposal that defines an API to allow users to regain access to a stellar account after they've lost their private key without providing any third-party control of that account and so I think these there's been some investigation of how to deal with potential incompatibilities and I'll turn it over to you yeah I can give just a quick description of what the incompatibility is and then we've got a short-term fix at the SAP level and then John will talk about a potential way we might want to think about this at the protocol level later on as well so the incompatibility is that Sept 30 the server that implements Sept 30 because of signing transactions for multiple accounts it has this rule to find intercept that says that the server should only sign a transaction if the operations and the source Canon the source count on their transaction and the operations is for the the account that's registered the account for the signing for and the latest version the cap 33 has things like adding signers including other operations that have to be signed by the sponsor account so because this transaction that adds a signer that's sponsored it has multiple accounts signing it as have 30 server would reject this transaction so the short term fixed that we're thinking of for at the SAP level is just to alter that rule so that a Sept 30 server implementation can choose to allow the sauce account of operations to include some limited set of accounts that are knows that you won't be signing transactions for and that should allow asked I'd say the sponsor to that list of limited limited sub and would allow those transactions to be signed and John also has an idea of how to deal with this in another way at the protocol level do you want to go you wanna shut up yeah I wasn't sure you were done talking didn't want to interrupt you so this this harks back to a super-duper old issue from actually opened by Jeremy Rubin like out of more than two years ago it's protocol issued number 93 if anybody's interested but basically Germany's observation was that signing transactions in the world of stellar is very susceptible to the confused deputy problem and the basic idea behind the confused deputy problem for people listening is basically like or at least how it applies in the world of stellar is suppose you have two accounts both of which that can be signed with a single with a single sign or a single private key and then you look at some transaction and you and somebody asks you to sign it for one of these two accounts but what you don't realize is that it actually contains transact operations for the other account as well and you've now signed it for both accounts and it's good to go and now you might have authorized something that you did not mean to authorize there are other manifestations of this as well for example like it could be a more complicated world where like you have multiple keys and somebody asks you to sign for a certain set of keys and those keys combine to sign for some third account that you didn't realize you were signing for so at the time Jeremy opened this issue and asked a question which was something like John will know the best way to implement this that wasn't knowing but at the time there was no way to implement that was this that wasn't annoying because there was no way to change signatures but since David's cap 19 which got like subsumed into cap 15 and introduced the new transaction envelopes we actually have the power to change signatures in a very clean way and so the gist of my proposal is basically to add a new kind of transaction envelope which we'll call like for the sake of this discussion transaction envelope no confused deputy but that's a terrible name we wouldn't actually call it that I don't know I'm no good at names anyway somebody else would probably think of it and basically what we would this transaction envelope it would take the normal transaction but it would require a different type of signature and a different type of signature payload for that matter so transact signatures for an existing transaction like a train action v1 don't work here but these signatures would basically have a new bit field wrapped into the signature payload saying which operations you're signing for and also whether you're signing for the source account and then when you provide the decorated signature you provide that bit field again and since the signature is over that bit field you know that you've gotten the right data when you verify the signature like if you try to verify the signature it contains that big field and so you know whether you have it or not and then you just apply the signature weight to the operations and potentially the source account that matched that bit field so this would be like a pretty clean approach and it would allow you to sign for like every single thing in a transaction anymore it's pretty neat it'd be some work to do and it would require overhauling a lot of the stellar core signature processing stuff but I don't think it'd be like breakingly challenge challenging excuse me but to be clear the idea would be to try to proceed with cap 33 now and work on these changes later yes exactly I mean like from my perspective as long as there's a way to solve the problem and we're recognizing that like the problem is beyond the scope of cap 33 it's kind of like we could always choose to do this if people are encountering this problem all the time whereas like if we had no proposed solution at all then that's a lot scarier right I mean I suppose we can also see how many people use some 30 and how the set level change affects people and whether it's not the problem legitimate right yeah I think I think that the reason that this incompatibility and compatibility is important is that people who use cap 33 other people we may use accept 30 and vice versa and as comparing 20 years both but yeah I think we there's no reason to change that cap 33 because of this problem I think what John is saying is right this this is an underlying issue that we should address on the time and we do have a way to move forward with this up with sub-30 we have 33 together anyway alright it looks like there's one other issue here about changing sponsorship checks the truth sorry I was trying to remember what the what I from applying to the validity yeah so right now the way that the cap 33 is written the way that I've been implementing it like we we would accept transactions that have like for example like bad bad sandwich egg but these transactions are 100% guaranteed to fail at the time of apply you don't need like normally when we do validity checks we're doing them on a single operation level but like the reason for that is normally just because operations have side effects and we can't necessarily tell what they are but for all of these sponsorship operations like for beginning and ending in the sandwich King like you don't need to do anything to note like I can look at a transaction and tell you if it's properly sandwiched without applying any of the intermediate operations and so the question is whether we should allow these transactions to make it to apply time and then fail at that point for being improperly sandwiched or whether we should just say like no your transaction stupid and it's invalid doing that would probably require some new machinery but like it's not conceptually hard at all it's just new stuff to build whereas doing it at apply time is very easy which is why I took that approach but it has the disadvantage of allowing people to spend money on stuff that and also spend time on the network with stuff that's like obviously not going to work so I don't know people have a strong feeling about this I know I've discussed it with Nikolai before but I don't know whether he what I don't really know which way he wants to go with that really cold it's worth I would have thought we would check this kind of thing in the SDKs as well even possibly in horizon I mean actually thought about it but as you say it seems trivial to check I mean like yeah like for me it seems like it would be a good candidate to have this as the validation step because it you know I'm trying to its a usability thing I think that said you know if if we if we think that some bitching is more like remote in kind of a niche thing maybe it's not worth it I mean either way is fine to me it's it's more like yeah trade-off of were classes because we can always change that later on to so it's not like that's exactly is gonna say that we always have the power to change this later I also don't think it makes a huge difference to usability because somebody's running these types of transactions is probably gonna be running a lot of these transactions so they're gonna find out really early in development like they're not kind of hopefully I hope I think I'm likely then they kind of run like 10,000 of these transactions and then see them or fail they're probably going to run one during testing see a fail fix the code and then move on then there is a strong point this is like a kind of like an enterprise tool not really an individual user tool yeah and if it is a user tool it's probably not gonna be at a scale that is okay so it seems like the answer is don't worry about that great are there any other are there any other sort of comments questions or suggestions about Kathy actually not as I mentioned earlier ambit worried about the locked phones clawback logic without it this proposal seems incomplete and sponsor the sponsor sponsoring entity has no means to recover its phones from government sponsored accounts we agreed to sync on this later so just with the record I think it's important to articulate this towards the timeline for this feature like this year the desired feature list is fully parked but next year will definitely introduce some way to deal with such situations because companies that are willing to sponsor reserves for their client accounts must have confidence that they will be able to claim the walked funds in the future I think it's important but if you if you have some ideas about how to do this I mean like I would definitely welcome a proposal about how to do this I definitely think that be a be valuable I haven't I haven't really spent any time thinking about how to do this other than the fact that it's like probably possible with the current design albeit like probably not that easy but yeah if you have a like definitely like submitted proposal I'll definitely look at it so I spent like a ton of time thinking about this I have lots of context on it sure [Music] okay great um I mean I feel like even though there's it seems like there's still a little bit to be resolved on uncap 23:33 it's feeling like it's in a pretty good spot and obviously these are things that were intending to include in protocol 14 and already the horizon team and people that work on SDKs have started to think about what that means and so I know Eric put together a working group to to sort of go through some of the issues figure out some decisions and I think he's going to give us a summary of where they're at yeah I don't have a same person except to say that we had a lot of discussions we went through and tried to think about use cases and then we tried to think about how this stuff would actually work and we came up with problems and the caps and we went back and modified the cap and then we came back again we went round asleep a few times it was really really useful I've linked our discussion doc in our agenda that you can look at to see kind of the gory details an FAQ that's coming out of this which will be helpful for the ecosystem I think and the outcome now I think is that we have a spec that's pretty solid so there are two issues in the go mono repo 2787 and 2788 that you can have a look up that really described the end points that we expect in horizon and the work that we expect to do in the SDKs and anything that's remaining that's unclear I'm kind of satisfied is implementation that all problems how do you put things in what tables that kind of stuff so I'm pretty happy with it the only other thing to mention I'm just going to mention this briefly I know they didn't really want me to there is a naming question in the cap 33 whether we should rename these operations there's something more clear because one of the things that we ran into in this discussion was that we kept getting confused about what certain operations actually meant and if we're confused then everybody else is going to be confused this is relevant to horizon because we have to name our SDK functions and our own points according to what we decide the actual operations are so there's a mailing list discussion about that I'm hoping we'll get an agreement on it I like the new names I think we should keep them but we'll need that in order to actually make the final implementation what are the new names and one your news sure yeah I don't have an issue talk about the sponsoring future as I think so the sponsoring future reserves confirm and clear sponsorship and then update sponsorship they were the original and the new proposed names are begin sponsoring futures as hen's sponsoring future reserves so it's really clear that that's a sandwich and then the third operation renamed from update sponsorship to revoke sponsorship because that third operation is signed by the person letting go of a sponsorship or letting go of paying the reserve for the account yeah I agree with all three of those particularly the third one yeah that seems nice and clear I mean there's a lot of there's a lot of letters and some of those names for super clear we're gonna hit our 80 character limit yeah on the lines of the HDR file after like one word so as far as you can see right now from your point of view and no surprise database migrations it looks good to me yeah I'm pretty happy with it cool any any other questions about sort of prep for protocol 14 downstream or about whatever just I guess it's worth mentioning as well that I expect us to begin implementation of this in our next sprint which will be in about a week in a house type unless something comes up to dissuade us or ask us to spend more time so those like pending decisions about cap 23 think they have they have an impact on that timetable that's significant that's more exciting Eric hopefully I'll have something decently functional for you guys to test off up by then yeah when you can get me a cool build ok going that looks like there's one final small issue which is started as issue 622 change clothes time semantics I think it's now cap 34 or which is in the draft you know you want to just summarize that real quick yeah like the cap is not yet in a good transform it's really rough so I'll just focus on describing at a high level what its cap it's about which is actually yeah we had like a bunch of ideas going around in the issue itself and this is the annoyance in the current protocol so this is not impacting the SDKs or horizon it's more in fact if the only people that are getting impacted are going to be impacted in a positive way which are people that write transactions that in the context of smart contracts that have expiration times and basically closing a net case that nobody is probably aware of when the right those smart contracts that today a transaction can fail drink on Central Time basically consuming the sequence number even though the time bones that was specified would make this transaction invalid so basically like the typical scenario is when it is smart contracts is you have mutually exclusive transactions on a time bound one is for example only valid before a certain date and the other is that it only after and then those are like your two branches of execution basically and here what we have is it's possible that it's a the first one which is only valid before a certain time would be accepted for consensus then only you would find it to fail during consensus because the first time that was picked during consensus is actually incompatible with that conference actually this is very annoying because now if you have this one transaction failing in your branch that means like normally people what they do is they change transactions with with hashes right and now that particular path basically becomes possible because you consume the sequence number but she didn't actually execute that one transaction so that smart contract is gonna be in a broken state so it's it's a very edge it so it's a really an edge case and because it's an edge case yeah we want to get rid of it anyway so the fix for it that that we came up with with David was - during Constance is basically this pick as close time the one that is associated with the transaction set so normally when nodes nominate a value in general so the value is made of transaction set close time and upgrades right now what we do is that when we get at the end of nomination we combine all those values into some more interesting value that is translate a transaction set that is one of the transaction sets that made that made it to the end of nomination and then a closed time that is the biggest closed time so instead what we're going to do is going to preserve the opportunity of transaction set to close time and by preserving the affinity what we can do is actually change the validity criteria for a transaction set I mean up for the pair transaction set close time to only allow transactions that are going to not be expired with the that given transaction set so basically it's pushing the burden through the nominated value it sums up like the actual the actual change is that and then and then will will kit will basically not even include those transactions that would expire between two Ledger's in the transaction set start the gist of the update change so it's not it's not a big change but it potentially will make it can be a life-changing for those people that may hit that Berg do we have actual use cases that are running into this so we so the thing that we see those transactions fail today on the ledger so people are running into this it has definitely happened before it happens we don't know in the context that people know what that we don't know yeah that we don't know and we we don't even know for sure like contracts that you didn't even you know that didn't happen yet right like that's the problem with those things that they are pre signs we don't know yet what so fact here's my my concern so I agree this is a problem and I think there are a number of ways to get fix this problem including this which seems like a plausible way another way would be the the kind of signed operations proposal that I made just on the mailing list but the problem is that all of these things they eliminate some äj-- and introduce other edge cases and so I'm just a little apprehensive about doing it sort of in the abstract because so for example this new proposal would definitely would facility Ned case where like you know you've got a payment channel and you know you submitted something and it like expired and like you know you would have submitted something else otherwise and now you're sequence numbers are messed up on the other hand under the current proposal there's a danger that if someone is doing like atomic cross chain swaps that it would work today and with this proposal if there's a network outage you could actually lose money right because no no no like this is fixing a niche case where you're guaranteed that the transaction states that it will not exclude transactions that would succeed so here's here's so ok so the fact so either I'm misunderstanding it or or you're misunderstanding me either way it's in its it's an indication that this is an edge case which is so imagine that what we're trying to do is we're going to swap lumens for Bitcoin right and so what I need to do is I'm gonna claim my lumens and in order of my to claim my lumens I'm gonna use a hash X signature and by disclosing that someone's gonna be able to like claim their Bitcoin right and I have to do my thing by certain deadline and then the Bitcoin person has an extra hour to like take the preimage that they've found in the stellar transaction and use out the claim the Bitcoin on the Bitcoin network right so under this new proposal if some if the stellar network goes down for an hour it's possible for an operation to actually execute even though it was supposed to execute at most one hour ago right and so you know again so this will require Dawson the network are getting a little bit lucky with with the time of a of an outage but if it's if you're trading enough Bitcoin for lumens that's something you'd be concerned with so again it's not I'm not saying it's a bad idea but I'm saying it causes potentially other problems that people now I mean not that we're sending what you're describing because if I don't already exists David yeah this problem already exists this I we messaged Justin about this like a very similar problem this morning like two hours ago like right now imagine this like super terrible thing happened imagine that you submitted a bunch of transactions you know like I submitted some transactions Nicola David Lee Orbitz Justin to like we also made a bunch of transactions and like Eric also submits a transaction but Eric's name starts with E and that means that he's malicious in conventional you know crypto stories and Eric knows of some zero-day that allows you to crash the network at externalize time and so what happens is like Eric's transaction gets into the ledger all of our transactions get into the ledger and now the network is dead like it crashes during you know ledger closed and nobody externalizes as a consequence everybody crashes now like when you restart the network it's gonna crash again because you are committed to this transaction set so the only thing you can do is update the software blah blah let's say this takes like 36 to 48 hours probably to get everybody back up and run it would be like I think a reasonable guess maybe even a conservative guess but like you should not consider any transaction expired on the stellar network until you've seen a close time past that time once you've seen a close time past that time you can say that that transaction is expired but until then you cannot because no matter what model we use like it's possible that the difference between like the clock close time when the like ledger actually externalizes and when the like a long time like the like the stellar header close time could be like arbitrarily far apart so this problem exists today like if you get into the situation like that transaction might execute until that ledger closed timestamps I think this is a different problem so this is also a problem but the the problem the problem I was concerned with is that a transaction with an older timestamp might not get disclosed until long after that timestamp right and if and if the contents of that transaction unlock something on a different blockchain then then that's a problem can you give some more background on this I'm not sure I understand how that would sure like so okay so basically we propose a block with a particular time right and that time was an hour ago because and in my case you don't need a software fault it could be like some botnet attack stellar or something right so it takes us an hour to like recover from this thing and now we're going to execute transactions that were like an hour old because like it's still the biggest block that's been nominated right even though at this point like everybody should be nominating everything but it could be that the block an hour ago was like the biggest blog nobody's heard of neuro transactions right so even though other people should be nominating like higher timestamps we you know well how it will execute like an old timestamp and so people might learn about a transaction with the timestamp of 11:00 a.m.
- at like 12:00 p.m.

Justin Rice restarted the public protocol calls and focused on two pillars for Protocol 14: CAP-23 (claimable balances) and CAP-33 (sponsored reserves). CAP-23’s sticking point was the predicate-tree shape—how deep should `and/or` nesting go, how do we bound recursion in XDR, and should we allow duplicate claimants—so the group agreed to hash out an implementation-friendly limit on the mailing list.

CAP-33 conversations centered on downstream effects (SEP-30, Horizon, SDKs) and naming the new operations so wallet devs understand the sandwich flow. The call closed with cap 34 (close-time semantics), sequence-number gaps, and how to coordinate Horizon/SDK work as part of the upgrade checklist.

Key discussion threads:

- CAP-23 predicate trees: recursion limits, disjunctive-normal-form proposal, and whether to enforce a fixed depth for claim predicates.
- CAP-33 sponsored reserves: renaming operations (`begin/end/revoke`), making SEP-30 compatible, and outlining the Horizon/SDK work.
- Protocol 14 readiness: tracking downstream tasks in GitHub issues, building FG/FAQ docs, and ensuring the ecosystem knows about the upgrade plan.
- Follow-up discussions slated for the dev mailing list (predicate changes, close-time semantics, sequence-number ideas).

okay we're live alright everyone welcome to the open protocol discussion a biweekly meeting in which we review and plan for upcoming changes then provements do the stuff call we've missed a few meetings because people have been out of town but our plan is to keep live-streaming these so that the world at large anyone who's watching can see what we're working on and all the wonderful thoughts that you have about I don't like stellar better these meetings
