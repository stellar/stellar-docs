---
title: "Stellar Developer Meeting"
authors: [carsten-jacobsen, orbitlens]
tags: [developer]
---

import YouTube from "@site/src/components/YouTube";

## Part 1

<YouTube ID="QrLA66qU9N8" />

The dev team behind Reflector and Refractor presents their tools for decentralized finance and transaction management on Stellar. Reflector delivers on-chain price feeds to enhance DeFi protocols, while Refractor provides a pending transaction storage and multisig aggregator for coordinated signing. Theyâ€™ll cover how these products support trustless financial interactions and improve smart contract execution on Stellar.

Visit their website here: https://reflector.network

<details>
  <summary>Video Transcript</summary>

[00:00] Okay hello everyone welcome to this week's Stellar Developer Meeting and today I have a couple of guests. So let me bring them onto this stage and first of all thank you for joining please go ahead and introduce yourself I think you are well known in the community at least from your names and your projects. But please thank you for inviting us people know me as OrbitLens I am the lead of the team behind StellarExpert albo Reflector and some other ecosystem projects and today we are here with RF the one and the only RF yeah. So I'm Raph and I've been doing some business development work with Reflector for a bit here helping

[01:00] doing some Outreach finding some debts would be a good fit and this works really well with our other project I do kind of marketing for seller projects through Lumen Loop. So just kind of just a big Outreach guy and. If you want to know what's going on in the community come to me. But it's just really great to be here and be able to talk about two really good products and kind of jumping into it a little bit. But we're going to be talking about Reflector and refractor both things, that are incredibly useful for devs looking to build decentralized way on Stell great yeah take it away I'm looking forward to this [Laughter] [Laughter] presentation. So Reflector is a decentralized price F article for the Stell DeFi smart contracts and it supports you know Financial derivatives lending borrowing protocols algorithmic stablecoins Asset Management contracts and you know there's just anything, that you want to do with price feeds you can do it

[02:00] so why are oracles important. So price manipulation is probably one of the most common vectors of attacks. When you're working on DeFi almost every single week we hear about successful exploits. So the rule of sum is. When you write DeFi contracts you have to you write DeFi contracts top oracles and you got to make sure, that you you know you're choosing these price feeds wisely. So again you're only as good as your Oracle. When you're kind of sourcing your data from somewhere else and kind of the really cool thing about Reflector is, that it's it's really built on some known brands in the ecosystems it's a def decentralized consensus, that we they're agreeing on these price price feeds. So it's controlled by a multisig of reputable organizations on Stellar and it really helps guarantee, that reliability fault tolerance and making sure, that those price feeds are updated and secure Reflector kind of relies on, that Quorum of nodes and you can kind of see it in, that picture on the dock there and it's

[03:00] connected by seller core validators next slide there. So we kind of combined two different Oracle approaches on chain feeds and subscriptions you can kind of think of it as kind of like the like we just have the price feeds rest rest the free. And then subscriptions, which is kind of like a pro level of of of these price feeds what's really cool about Reflector is, that at its base it's free like all, that data anybody can plug into you don't really need to pay for it but. If you do there's some really cool things, that you can you can kind of set for yourself some automations and some some Communications some notifications for, that could be useful. When you're developing. So onchain price feeds are free for everyone the interface allows for a set 40 compatibility standard all the price feed data is stored in the contract itself in a fixed time frame I think free is interval of every five minutes. So the data is readily available for all consumer contracts the use Oracle quoted price results through directly in

[04:00] the same function it's efficient it's optimized low fees and it greatly reduces the number of contract calls. So again we're trying to make sure, that it's efficient for your DeFi apps to kind of plug into offchain Data or classic data subscriptions on the other hand provide kind of like a more fine grain data access with kind of user defined customable customizable triggers with up to one minute sampling, which again unlocks a lot of use cases with, that one minute sampling this service provides kind of access to a much wider range of available price feeds private web hooks invocations in combination with onchain execution proofs. And then it guarantees again stability and security. So highlights the Oracle contract provides a kind kind of comprehensible interface again for step 40 the ecosystem standard and on top of, that kind of offer like automatic cross price calculations utility functions tww AP range approximations last. But

[05:00] not least onchain data is again like we said it's completely free. So kind of go ahead and use it I feel like doing Onre is such a sorry we're doing Outreach is kind of a weird thing. Because we're trying to sell devs on a free product go use it it's there we hope this way you know more you know sbon Contra contracts can build kind of safe and reliable infrastructure and kind of, that main goal being we want you to use a system, that's safe and, that's kind of why these nodes, that we we've kind of we work with are incredibly important, that we want to make sure, that you know you like these or these these node providers are securing everyone's DeFi apps. So they're just kind of incredible importance with you know making sure, that consensus mechanism and, that decentralized nature of re Oracle kind of services those DeFi UPS okay. So and, that kind of brings us into the Dow. So you know it is a it's a cluster it's governed governed by

[06:00] these organizations and they they kind of they serve these and maintain these Reflector server nodes and they participate in this cluster consensus. So again. When you want to get an asset listed this price feed listed you really have to U make a proposal to the dell and submit with the tokens same with adding different kind of price sources again you got to go to Dell and the Dell make the decisions none of it is done by one entity and everyone needs to agree on it and again, that kind of helps secure, that you know all these organizations are key players in making decisions and, that brings us to the Token again the token is what kind of go makes us able to participate in the Dow cluster operators receive actually tokens for participating in the consensus mechanisms and they provide like computational resources to aggregate validate certify. So again it makes them to pay them in token

[07:00] correspondingly like these tokens are correspondingly acred tokens represent the equivalent of computational resources contributed by by each party these tokens are. Then used by the cluster for governance token and subcription services. So the xrf tokens can be used in cluster governance voting on new price feeds or new cluster members members invitations and the again serve as the backbone for the sub subscriptions and they're required to create C create custom subscriptions and. And then pay for the upkeep of these subscriptions yeah and here we have a few prerecorded videos with the interfaces. Because we have a lot of them I think I will start with the no admin dashboard interface, which is the interface, that displays current configuration for cluster nodes there are public Keys and we. So addresses

[08:00] for each server below you can see stats for each note other sections contain the list of direct assets for every orle contract and configuration changes history logs Etc life price feed, which is produced by the Oracle is available on The Ledger and on our website collector. network here you can see update history for every Oracle supported by the cluster for both pabn net and test net ler updates are stored on the L itself with regular five minute intervals and immediately after, that available for all consumer contracts, that utilize our price fees and sorry and the next one is the

[09:00] interface of subscriptions subscriptions is the more advanced solution, which is a kind of a Swiss arment knife for any developer who decides to build any kind of Oracle based Solution on Stellar or some other chains. So subscriptions is a service for user defined triggers invoked automatically once the price deviation reaches a certain threshold every Oracle not independently Eves subscription condition once a minute and consumers receive web hook notifications on triggered price changes let's see how it works in the web interface first we need to select data source for base and quote symbols the quoted value is calculated as quot price divided

[10:00] by base price there are two options first option is aggregated price from exchanges and price tokens from Stellar public exchange for instance here we select arbitrum token from exchanges. Now the base scker we can utilize the same data source or qu prices for a pair from two different sources this provides the ability to build a personalized Oracle on assets from different feeds and different different asset classes the cluster will calculate the prices automatically here we select yxm and Trigger threshold here it controls the Target price deviation amplitude this example it means, that web hook and point will be notified every time the quoted price goes up or down by

[11:00] 2% com compared to the previous dispatched value Reflector also periodically sends herbit messages with current quote price to let the Upstream service know, that subscription is still active to activate the subscription we need to pay for it with xerf tokens and they also required to pay the upkeep balance gets changed gets charg on the daily basis it is worth noting, that the daily fee depends on the quote complexity and herbe interval here we can use web hook demo website to show how it works we'll just copy paste in the web hook URL and, that's basically it the account, that created the subscription will become its owner we need to confirm the transaction and it will be

[12:00] it will automatically create the subscription. So basically all all the setup everything is done in the web interface interface here here this description page we can see all key parameters for each of the account subscription as well as remining up keep balance subscription consolation removes it and reclaims reming tockets and here on this interface you can see the nod notifications, that come from the the cluster there was a lot of similarly identical post requests yet they have different signatures each Reflector cluster node sends notifications to Consumers to provide redundant and

[13:00] reliable proof of delivery as we can see here every incoming request contains information about the subscription quoted price and update time stamp. So what's next we actually developing the protocol and at the moment we're currently working on adding new price feeds for foreign exchange rates and commodities website improvements is another important task for us we need some more quote samples better documentation and developer support to make the integration for new CER as smooth as possible. Because sometimes R needs to go here and there here and back again to explain something and we need to basically explain some very basic things about

[14:00] oracles and our particular Oracle model and of course the next huge milestone for us is going cross chain we want to expand our subscription service Beyond Stellar. While the Forum and do smart contract logic will be working on Stellar network network. Because essentially subscription subscription subscription service is chain agnostic and U any service any chain or even without any blockchain can receive these price notifications they just need to set up their subscription VI the web hook interface, that's basically it about Reflector we definitely want you to come to our website to join our Discord

[15:00] and to talk to DS talk to other people and check what you can do with oracles. Because Reflector is the ultimate price reference for Star defe I think one of the big things too is you can see it out in the ecosystem already like you can see it in blend like there's really cool protocols, that are starting to build on to Reflector's. So again go see what's out there you know. If you see what you like maybe you're want to play around with Reflector and once you're ready go check out the Discord or you know reach out to one of us and we can point you in the right right place yeah totally we can answer some questions now. If you have any or we can jump yeah. So anyone anyone, that has any questions feel free to ask I saws he was asking about the QR code, that at the end of the Pres, that's great but. If anyone anyone else has

[16:00] questions we still have a few minutes. But I would like to ask a couple of questions I'm thinking what what made you start build this. Because this is not just a small pricing price feed Oracle this is pretty comprehensive I mean everything from the security to have it managed by da how how did you get started on working on this and did it it start well project you know we had a conversation with to at the Meridian about two years ago and it was like what what do you want to build on seran well we have Stell expert we are experts in data processing. So Oracle seems like an interesting thing to build. But well right. Now maybe I'd reconsider

[17:00] this decision. Because it was a pretty bum perod and like I am super grateful to all our Dow members who joined us in this Challenge and task and supported us. Because they are actually the ones who runs the who run the n and protect the protocols on chain currently we have more than 15 million dollars protected by our oracles and it's like def fact Oracle solution for Stellar network I am grateful to everyone who supported us with with this great I think we have a couple of questions mat has a really good one about the

[18:00] hackathon yeah yeah, that's a pretty good one. So we we we have we've had two Dev competitions up-to-date just to kind of help us highlight how the oracles can be used and kind of get devs playing with it we had best Oracle contracts we had Lena, which is like a trustless loan platform, that won one of the the prizes we had Pula Labs, that won a trusted portfolio manager, that helps kind of rebalance bance your portfolio automatically, which is kind of cool. And then we had kale kalepail, that did his Reflector predict like betting on prices through an oracle. So those were the three winning ones we had a bunch of walkthroughs and and text writeups as well we had some you know some people, that built some Reflector clients some boiler plates notification Bots again there's just kind of a lot of stuff there and it kind of. If you're interested in Reflector and seeing how it can be used going to the Discord and seeing the submission submission

[19:00] sections is a really cool like list of different different examples great yeah I have seen. So I've been to a couple of hackathons. Now and I've seen seen the Reflector being used and I know you mentioned kpl I know we use it in general SDF. When we build build cool stuff fun stuff. When we experiment. So but it is I have seen, that both slender and blend have used it too is, that something you are driving something you are seeking out or did they come to you and say this looks like a really neat service well it started with u this this first meeting with st and people. When sban was still in bet and not even on mayet yet. So actually we partnered with some key players at the

[20:00] time okay great could we maybe just kind of I think we're running out of time. But let's jump into maybe do some just a little touch of refractor. Because it is I will do pretty quick yeah. So refractor is a pendant transaction storage and multi aggregator for Stellar network it's a developer Focus service in the first place. But anyone can use it to store trans ctions and G signatures required to match sign thresold, that's the service, that been around since 2021 and like many active Dow organizations are using it like Aquarius Reflector yel blocks at some point. So it's a very handy toolkit for D Administration join castol accounts secure scroll server Services treasury

[21:00] Management Services like any any kind of service, that utilize Stellar multi why you might need it It's tricky to check what inside the transaction what are the operations requir signatures and something like this. When you just get some random Exar encoded transactions and someone asks you to sign it G signatures requires sending a transactions via Messengers orail or some other channel, which is like not. So convenient let's say it. If two or more people sign simultaneously one of the signature can be lost lost. If someone signs the wrong transaction everyone needs to start from fresh not enough signatures to M the threshold

[22:00] and transaction will be discarded by the network too many signatures it will also be discarded by the network it also requires coordination. So someone needs to oversee the process like contol the signatur submit some transaction submit the assigned transaction to the network Etc. So we've been there and, that's, that's actually a painful experience like once you start using multiseq at scale it's it's problematic U refractor allows you to basically upload your transaction XD web form just copy paste transaction exr check selected network click save and, that's it optionally you can choose Auto submit to the network option, which which will allow the system to

[23:00] automatically submit transaction to the network once it gets enough signatures for it and after creating you can simply share the link with all the signers and, that's it, that's how the transaction looks right and we can see here, that it's much easier to understand what operations are inside what will be the execution result what are the probably probable consequences and Etc pinted transaction page also displays all transaction details current signing status and automatically detected validity period and the key part of it signatures Reflector can automatically detect

[24:00] all possible transaction signers their weights and total transaction threshold it will show who else can sign and it basically prevents the situation. When too many s signatures were added and transaction again be becomes becomes valid here for example you can see like real world examples from Real World signature with different thresholds how it looks in the process of signing and how it looks. When it's fully signed web interface supports multiple wallets. So there is an option to sign the transaction Visa wallet of your choice or import assigned ex directly. So in case. If you have air gab device or something like this you

[25:00] can just copy paste XDR there and Reflector will combine valid signatures discard duplicates and remove all inapplicable signatures. So it basically makes the process much more streamlined it's free service it's free for businesses and individuals without limitations we have some some rate limiting. But you can use it for free it's been around since 2021 it is supported by our team it receives some regular updates. So it's been pretty reliable servers it is Battle tested as I told it was employed by different DS and it has open IP. So you can open II you can integrate it directly into into your application or service

[26:00] and best of all probably it's open source everyone can check what's inside or run on instance and work with it. So it's pretty handy tool anyone who will be managing Dow some kind of multi setup account you just need to check it and maybe it will be right for you, that's, that's I think it's it's very interesting and and I think multis is not necessarily something, that's easy to do. So so having having a tool like this I think, that is super interesting I think actually multii maybe maybe we should do a a separate presentation on, that or separate talk about, that. Because you guys definitely have some experience with, that. So yes. So maybe I think, that could be an interesting topic for for future presentation. But for. Now we

[27:00] unfortunately out of time I need to wrap it up thank you both of you for joining it was super interesting to to learn more about about what you're working on and yeah can just encourage everyone, that needs pricing feeds to start using it I mean it's it's super easy and it's it's free and it's something, that allows you to scale with your application as well into into maybe a subscription based model. So thank you for joining thanks for having us okay talk to you later thanks everyone by bye

</details>

## Part 2

<YouTube ID="lzxv5ij6TLw" />

In this protocol meeting we discuss two Core Advancement Proposals - Dima will be presenting CAP-0063 (Parallelism-friendly Transaction Scheduling), and Siddharth will be presenting CAP-0067 (Unified Asset Events).

Here are some resources to read up on:

- [CAP-0063](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0063.md)

- [CAP-0063 Discussion](https://github.com/stellar/stellar-protocol/discussions/1602)

- [CAP-0067](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md)

- [CAP-0067 Discussion](https://github.com/stellar/stellar-protocol/discussions/1553)

<details>
  <summary>Video Transcript</summary>

[00:00] Hello everyone welcome to this week's protocol meeting we have a couple CAPs to discuss today and I'll start by presenting CAP 67 I'll share the link here. So this CAP is about emitting events in Stellar in the same format, that we see in sorine for Stellar assets so, that a user can use the stream of events track balances and this CAP covers three separate High higher level changes they're listed in the abstract of the CAP and I'll first go over the the two smaller changes, that make some semantic fixes to the seller asset contract events for compatibility with set 41, which is the token interface Set. So the first is, that we're going to stop emitting the admin on the mint and clawback events the admin is an implementation detail of the the sa and does not belong in the event the admin doesn't even exist in sub 41 as a concept

[01:00] the second change is, that we're going to update an edge case with regards to transfers to and from an issuer the sa was written in a way, that matched classic payment semantics where a transfer from an issuer Min the asset and a transfer to an issuer burns the asset the issue is, that in these two scenarios we still emit a transfer event instead of the appropriate Mint or burn event. So this CAP just proposes fixing, that to Mint the the mint the correct Mint or burn event instead of the transfer prevent. So I'll pause there real quick. If there are any questions about those two changes before I move on to the the bigger change at hand all right. Then I'll move on to the third and most significant change of this CAP, which is emitting an event in the same format as the Stellar asset contract for any asset movement and this means, that a a Stellar operation, that results in the movement of an asset can emit

[02:00] a transfer mint burn or clawback event and we're also adding a fee event to represent the fee paid by by The Source count. So the the CAP specifies what what the events for each operation will look like. But I'll just mention some interesting points. So a trade, that occurs through either through an offer operation or path payment operation will result in two events one for each side of the trade and the two parties on both events will be the source account of the operation and the owner of the offer and it's and these these events can be transfer events. But they can also be instead be a mint or burn. If the issuer is involved we're also adding new SD address types to be able to represent the claimable balances and liquidity pools as from or two addresses in these events and this allows us to represent movements to and from these entries

[03:00] yeah. So silence asked can you explain removing the admin from the sa mint and Clack events. So currently on the mint and Clack events of the sa not not in set 41 the admin is one of the topics of the event. But this is this isn't necessary and the concept of an admin is not required for a token exactly yeah it's not we're not removing the admin from the sa just it just says the topic we're removing it from the the topic of the event all right. So yeah we're also in addition to those two new address types for clal balances and liquidity pools we're also adding another SC address type for MX accounts so, that these new events can pass along the MX information, that the MX account is used in the operation or transaction it's also another thing to

[04:00] sh screens. So I'm I'm actually not presenting this in the order, that it like it shows up in the CAP this is just some points I thought were interesting but. If you look at CAP 67 these points should be listed there at a with more detail. Then yeah. So I wanted to mention, that these new events will not be Into The Ledger. Therefore will not be part of the protocol and we also will emit the emit these events from Genesis. If you replay from Genesis and something to keep in mind here is, that there was a bug in a very old protocol version where XLM was minted and burned and we will admit events, that will allow you to reconcile those Balan changes and finally we will be reworking the format of TX meta with the transaction meta V4, that generalizes the events and separates them by operation

[05:00] previously the events were for formatted in a way where was very specific to Soroban and the fact, that Soroban only has for a single sbon transaction there's only one operation. But the transaction meta V4 generalizes, that and the plan is to provide a config flag, that enables the emission of this new meta format okay. So let me look at some of these questions. So will this resolve the issue with a locked issuer sa having an admin Sil can do things the issuer can't lock no. So I think what you're asking is. If you change the admin and you lock the issuer the ad okay ignore, that yeah they the admin can still do whatever it wants

[06:00] to to what else does back filling mean, that replaying with these new events enabled will emit meta4 for all ledgers rather than just post 23 yeah, that is the goal and we and, that should work I we we probably have some stuff to work through there but. If we want to admit these events from the beginning. Then it has to be meta V4 see all right. So MX addresses and memo. So yeah one one of the issues is. So the CAP addresses the M partially by forwarding the M addresses into the events. But there is another question about memos, that I'll link this discussion

[07:00] topic topic from from Nico it's relevant and at a high level this this amounts to considering a solution where you can group an arbitrary memo with a group of trans events this will allow you to you know solve some issues we've had with in the past with regards to the design of memos but, that. But yeah I don't know. If Lee. If you want to discuss this a little bit more, that's something this concept is something we still have to discuss yeah good yeah. So yeah NCO may want to he he this is sort of I feel

[08:00] like his idea. So he may want to speak to this as well. But there you know we have the separate we have another CAP CAP 64, that we're not actually discussing today, that's doing some things with MOS it's touching MOS and where they show up where they will show up in tooling and you know making it possible possible to do a soran transaction with a memo and have, that safely included in the signature so, that's CAP 64 and something, that niik was bringing up with this CAP is, that you know right. Now we're in this moment where we're making some significant changes to events or we're doing a lot of work around events and doesn't make sense you know we have to sort of we we sort of have to get close to Memos. Because we've got this issue with moer dresses and you know right. Now CAP 67 in a very non-controversial way is just

[09:00] suggesting, that these events. Now might contain addresses, that are MX addresses. So we're just going to pass it through so. If you do a payment operation, that's got a destination of a Max address, that address field in the San event could be a MX address it could be an M address instead of a g address or a c address and I think, that is like very not noncontroversial it's sort of like straight line what we I guess we would sort of expect the CAP to do in the situation. But there is this we have this history where you know once upon a time there was memos. Then there were M addresses right. Now the ecosystem is pretty there's you know both get used in the ecosystem. So both getting used by exchanges and but. When you look at the data tells a somewhat confusing story. Because this is memo field there's this MOX address they can be in the destination you know technically sources can also be

[10:00] addresses and nik's Nik was sort of suggest I don't know. If this was actually a proposal. But he was suggesting like do we do something more here do we make these events in this moment more opinionated about what the memo for this event is and you know do we do something like. If there's a transaction memo this event has a memo is or. If there's a destination of a m address do we pull the memo out of the address and we say well actually, that's the memo for this event to some degree like the ecosystem has to do this somewhere and you know right. Now it has to do this Downstream ecosystem tooling has to make a decision about where these events come from and in the future like you know right. Now memo memos there's like MX addresses are only a classic thing memos only occur at the transaction level in CAP 64. But in the future like how do you attach

[11:00] memos to like other groupings or transfers. If there's you know transfers being bundled together yeah sorry I'm not sure I'm actually doing a great job of summarizing this there's like a lot yeah yeah it's definitely something, that still needs to be thought through. But I guess the question is should we explore yeah addressing, that issue in CAP 67 it sounds like we should at least at a high level I think we should and I think what nio just posted in the chat is a really succinct way of for the why we should consider it. Now and, that is, that you know CAP 64 is forcing the memo to be exposed in down TR systems for San events. And so it makes sense since this CAP is very focused on, that consistency between classic and Soroban and fixing it across those two dimensions of you know San

[12:00] versus classic sack versus set 41 it makes sense to discuss. Now yeah makes sense okay orbit asked this question why did we decide to extend the meta with the vents. If these events can be commuted dynamically from the existing meta seems like redundant data in the meta and this is something we actually discussed internally I believe George actually mentioned the same single source of Tru truth rather than risking bugs Downstream renting the wheel and dynamic compute also Lee do you have I say you mut yourself do you want to address this yeah I can speak to this the a large part of this is, that you know. When we look at the tooling, that we have today. So we have a Horizon, which takes a lot of What's in the mattera today and creates this data model for people to consume this new data model for people consume. And then St RPC was designed with a different

[13:00] approach where the Stellar IPC really exposes the raw network data and tries to introduce as little as possible in terms of its own data model on top of, that and one of the reasons, that Stell IPC was designed, that way is, that you can go to you know you can take contract events, that have come out of star RPC and they look the same way as. If you go and take meta out of core or. If you use G Galaxy to you know generate a data Lake of meta like the events look the same way in all these different places except. If you go to Horizon you know Horizon has its own data model and it has, that effects data model, which is you know very different. So the idea between the ibr like how like why bring these events to Classic and the the the Stellar asset contract and the CL operations is to create this like one

[14:00] unified view, that appears everywhere, that looks the same everywhere, that that the data occurs okay I hopefully, that answers, that question but. If not let us know I think Lee you mentioned another thing about another question about asset names on the the topic of the events right. Because the currently go ahead maybe before we move on it does sound like orbit has some things I'm Orit do you want to speak to yeah hello everyone just wanted to

[15:00] add my two Sense on this we've been successfully reconstructing everything from transaction meta XDR on the client side basically our inje pipeline in Stell expert is built on top of the library, that we built specifically for this purpose and we had a conversation with the Horizon team before like several years ago I think where I proposed to actually remove all this excessive data tables like effects and everything like this. Because all of it can be reconstructed on the client side of course there are very specific issues like the memo case lay covered before, that

[16:00] but maybe we can just like conduct some research on it and maybe the Parson Library we already have and, which have been tested for years and, which works with classic and San maybe it's enough. Because on the client side people can just use it directly in JavaScript to parse the response they receive from DC DC Horizon. Now doesn't support transaction meta. So it's probably case. When we cannot use it directly. But at least from RPC it has zero problems is

[17:00] Parson looks like George has something to say. But I can't bring him up on stage there oh okay yeah I actually didn't have anything to say I think it's a I think or makes a good point. But I do think, that a unified event stream is better for like your average dApp developer even, though we can do all the indexing on the Fly it's probably more catered towards like infrastructure providers like StellarExpert and Horizon. So this is still going to have value for people who are just interacting with RPC directly I mean we've been using it to actually display transactions in Stellar Expert be wallet

[18:00] refractor refractor everywhere I'd say it's a universal approach, that can be used everywhere I'm not insisting on it it just seems to me, that transaction meta will be even larger after, that and it might have some duplicated data. Because of of this can you hear me well yeah I can hear you yeah okay great. So I have a couple thoughts on this first. If the meta size increase is concerned I think I don't know. If we made it explicit anywhere. But I think this should be optional for the most part part at least I don't see a good reason for not making Z

[19:00] so I don't think and the same goes actually for The Ledger changes ideally you would be able to pick one or another depending on what is your Downstream Downstream processing whatever suits you. Then regarding the library I wanted to point out, that well a not wa JavaScript is not the only language out there. But also there is like some Nuance to like there is very Nuance difference between Ledger divs and events. Because events tell you what exactly has happened during the transaction whereas weder deeps tell you what was the state after the transaction and I think it matters more for soran. But I don't know it might have some interesting implications for classic I don't know. If they're interesting to anyone. But just, that the format of the event like. If you're processing Samson you

[20:00] might be interested in processing Samson as a stream of specifically events right and not a stream of some data types, that depend on the operations like can see from the library correct me. If only them missing something there. But this don't seem to me like this stand and the idea here is, that you know. If I wanted to track just the movement of the balance and I not particularly interested in the exact semantics of each and every store operation what I could do is I could just ingest the event stream and track the balance for a given account, which I think may be interesting to some consumers. So yeah this are just some thoughts on what why buer doing this at all I think the argument regarding

[21:00] the event stream is pretty solid. Because this way the server in this case RPC handles the streaming itself and indexing and cses and Etc. So instead of working with transactions and Parson transactions it can like really rely on these events it's a solid argument it point is, that I'm not proposing to use JavaScript everywhere maybe we can like Port this to rust or other languages and use it inside RPC inside other applications instead of adding the data we can like do it on the fly in RPC itself right regenerate this events. Because

[22:00] basically all described events, that what we do with u with the transaction M XDR, that's something we already do. So maybe it's just one of the options instead of extending the XDR itself again. Because to me it looks already pretty pretty large my subjective opinion on this. But again. If you could opt out of imian events would, that solve your issue with the size. Because it's I think besides the size concern I feel like the suggestion of let's put Comm implementation somewhere we just

[23:00] suggesting well why don't we put this common implementation right into ore I don't think it makes a huge ideological difference and I think it does make something easier to maintain and to ensure, that they works correctly for b yeah especially. If we are doing protocal changes. So yeah and just for the say I totally agree and yeah. So I don't know. If you have any strong arguments. But I feel like you're more or less in the same page here in terms of standardization. Because it I guess just in of putting it into cor kind of make sense yeah the standardization it sits inside The Narrative of everything being a contract. So I think like, that's what's beautiful about this is, that you know

[24:00] today we have you know classic operations. And then we have everything, that happens on Soroban and the result of those things looks pretty different. But as a result of this work we're moving towards this future where actually everything, that just happens on classic looks exactly like a contract, that's executing and you know all assets actually do have a contract, that's reserved for them they have the Stellar asset asset contract. And so the events are just going to look like everything, that happens with assets no matter. If it's classic or anything like, that it just looks like a contract, which is also much more similar to other ecosystems other like blockchain ecosystems as well. So there's, that familiarity for everybody about how this works versus say Ethereum and other things yeah and I I'll make sure to clarify in the the CAP, that nothing's being removed from meta and this will be optional, that's a good point I mentioned the config flag for

[25:00] transaction met V4. But I I'll go into, that in a little more detail are there any other questions I didn't quite catch. If we have made any final decision on MOS like what we talked about like 10 minutes ago wa wait what is the final stages. Because I maybe I've lost it yeah. So we're for in the context of this CAP we are the we're forwarding the M account information right into the the addresses. But for memos we still need to explore what Nico mentioned earlier like. So there's nothing finalized there. But we are going to go look into, that right soal decision okay I was just kind of missed it okay yeah

[26:00] no yeah I'm not sure. If we want to discuss, that more. Because it's not actually nobody's actually written like a a formal this is how we're how we would propose an alternative for it to work I mean I could say something right now, that I think aligns with what niik was talking about. But I don't know. If I think it'd be valuable. If we actually get something written down. Then present it in the future meeting. Because I know we we have 30 minutes left and dimma has a couple CAPs to present, that's probably more valuable right great all right go ahead okay yeah great yeah I don't actually mind spending more time on the events. If necessary but. If you're done we can go to to app

[27:00] that's six hisory. So this is basically a CAP, that mostly changes the transaction set. But it does. So in a pretty interesting way. So as you may or may not know Al a hustle you need to go through with a Footprints inur transaction is done for a good reason and this reason is being able to run the transactions in parallel. Because if you know your footprint we know, that you don't have a data dependency between two transactions and you actually can run them in parallel without worrying about any any synchronization. So given the footprints in theory today what you could do is you could right your own version of core application logic

[28:00] that takes the transactions and partitions them somehow into threads, that R par from each other. But the issue with, that is, that there is no good boundary on how much time is it supposed to take for example. If you take 10 transactions without data dependencies you could run them in 10 shads and they would for example take 10 milliseconds after synchronization or you could have the same 10 transactions. But all of them dependent on some W your entries are being updated and thus you cannot schedule them into 10 different threads you would run them in the single thread fres and in the end you will spend not 10 milliseconds. But 100 milliseconds to apply all transactions. So basically there is since protocol doesn't do anything currently about pration there is no good way to

[29:00] schedule the transactions in such a way, that it takes some bounded and expected well well there is still an upper bound on the runtime. But this upper bound rise wildly and it's not good. If Ledger may close both within 100 Mills or within like two seconds for example, which is why we are doing this CAP fix3, which solves exactly, that problem, which is given a set of transactions come up with a transaction set data structure, that guarantees certain time for appliances transactions is a c of course of, that time being time in model constructions and not of course well time CL time. Because we kind of cannot tell it beforehand without tring the contracts. But we hope, that our cost models are

[30:00] good enough and another cave being, that you actually should have the fish is sufficient amount of physical course to support multi stading. Because well again. If you have just one core obviously it doesn't matter. If you run 10 shreds there some here so. If you have just a single core and you wanted to apply transactions in 10 threats obviously you'll not get any performance gain. Because well transactions are purely CPU bound bound so, that's the motivation for this CAP and the way it works is, that we Define a new structure for the transaction set, that instead of having a linear array of

[31:00] transactions defines two levels of group transactions together and the first level is called stages and I will talk about why stages and what does it need for any moment. And then every stage consists of multiple clusters of transactions and within the cluster transactions generally may have data dependencies. So it is expect Ed in this cluster to have data dependent transactions, that need to run run sequentially. However there are no data dependencies between the Clusters themselves and thus since every cluster is guaranteed to be independent of every as cluster what you could do is you could take every cluster and put it into a separate physical thread and thus you can apply the whole stage in parall is as many

[32:00] threats as you have rers rers also I probably forgot to clarify what we consider data dependency it is. When a single transaction has an entry in a retrade footprint. And then another transaction has the same entry in either readon or retrade footprint me, that one transaction modifies the entry another transaction as read. So modifies it in either case we need to sequence them. Because if you don't do, that. Then we will get non deterministic results for any these transactions. Now why do we have this additional level of transaction grouping in stages the reason is, that. If we just try to naively say hey let's just par the put the transactions in separate threads put them transaction set and, that's it. Then you may run into some

[33:00] issues with C traffic patterns for example example imagine an oracle contract for example right and imagine it updates and entry some key. And then there are bunch of transactions, that want to read the value from the contract from the ccal contract and, that key and thus we have data dependency on a single entry. But we have only a single rate of the entry. But we have a lot of rats of the exact same entry. So what we can do with this stages is, that you can put right into one stage. And then all reads into different stages stages and basically this introduces just a few barriers of execution into the transaction execution schedule. But it

[34:00] allows kind of efficiently work around this traffic patterns and given smart enough maybe not even smart enough algra for actually coming up with this data structure you can deal with a surprising amount of conflicts all at once without introducing any special scheduling and synchronization procedure. So basically just have a few barriers in between the transaction applications and this allows resolving a lot of the conflicts at the same time before driving deeper I go read, that for a moment and there any questions far okay the new adjust approach okay. So this app itself just changes

[35:00] the structure of the transaction sets it doesn't have any particular changes around how we, which transactions do we pick at all. But what I can say, that the protocol specification remains such, that that transaction ification order is randomized and I think in case of M manipulations nothing changes from the current approach. Because for any Arbitrage you're probably going to have the data dependency and data dependent transactions are applied sequentially and all we shuffle all the stuff, that is being applied sequentially. So the transaction order is still for to predict and yeah I don't think it's

[36:00] any easier to manipulate than it is currently, which is to say it's not impossible. But just spaming Ledger is a arbitr transaction for example example yeah in terms of the system requirements again the cup doesn't Define them I will will talk about this in a moment. But of course yes. If we want to run transactions in parallel. Then well you need to have certain number of course present on the validator, which is I guess a tradeoff with the like, that's Horizontal scaling the notot use better Hardware as. Then cannot find more

[37:00] transaction M resarch maybe maybe I think I already have the section on shoing the transactions. So may be efficient maybe not yeah okay I see question regarding Z and let me actually go a bit deeper regarding how exactly things are specified first thing is, that Moro does introduce a new network configuration setting for the maximum number of f per stage, which roughly well not roughly. But basically maps to the EXP Ed number of course, that your

[38:00] that the network is willing to throw it. When ledgers. If you have less course. Then it may take for you longer than expected to pric transactions you have more cores or at least, that many you should be good and as all the network configuration settings this setting will be Modified by validator vot. So you know some validators say, that we don't have, that many cores they will not vote for this and ultimately network can decide. If requirements for to rers are too expensive or too hard to come this come up this and also to make it clear. When we upgrade to protocal 23 this setting will be set to just one. So any parallelism allowed as network

[39:00] settings. So you know, that to enable any parallelism network quote to happen Okay. Then to the structure okay I already talked through the specification of the data structure the phas fee and search pricing will work in the same way as it works now, which is there will be a single base fee for the transactions it is 100 Stoops. If there is no search price in and. If there is SAR pricing, which means like. If there were transactions, that you couldn't include in The Ledger. But this, that were in the mle we take the fee of the cheapest transaction, that we do include and use it as a base fees, that everyone has to pay

[40:00] okay, and now again to the apply order first there is still a canonical apply order, that is defined in CAP. So you still can apply all the transactions sequentially and arrive at the same result as you would. If you were. If you applying them in parallel and similar to what we do. Now we shuffle the things before applying them first we shuffle every cluster using the hash of the transaction set and hash of every transaction. Then the for the Clusters we don't need to shuffle them. But by the transaction

[41:00] hashes. And then we shuffle the stages as well. So yeah like. So transactions are in different stages one may happen before or after another one one, that's kind of what to do for Shing and I think it's not significantly different from our St yeah I don't think it does anything specific to again it doesn't changes status quo for M on the large scale like I think the best thing you can do is still just Spam the network is heritor transactions it is the issue we have. Now and it

[42:00] persists like the goal of this CAP is not to prevent these types of attacks. But it does not make it any easier to guarantee any particular execution order between two transactions. So yeah I don't see like. If you see something specific like maybe you know maybe we can up Asing later. But I yeah I don't think there is any change to to the execution orders, that is relevant for for right okay so, that's a kind of simpler part of the CAP. So again multiple threads to apply transactions in parallel oh the most important thing, that I forgot to mention is, that

[43:00] we limit the number of sequential instructions across all the stages, which means, that for every stage we look at the cluster, that takes the longest time in terms of model instructions. And then we sum up this numbers across all the stages. And then we come up with some number of sequential modeled instructions, which are hopefully approximating the real runtime for applian the transaction set given and physical threats for example. If the Ledger limit currently is 500 milon instructions with this CAP for example. If you set Ledger Max dependent TX clusters to for example eight meaning you need eight physical threads and we keep the limited 500 million instructions we expect the weder application time be roughly a factor of 500 million virtual instructions, which we still on to few hundred milliseconds

[44:00] currently our cost models models yeah so, that's kind of the simpler part of the CAP the trickiest thing is the GTL update semantic change. So all this data dependency stuff is very good and nice. But one thing, that we again consciously did. When La in San is we allow extending the dtl of the entries even. If the entries are in read only footprint and the reason for, that is, that well we expect TTL updates to be really prevalent and. If we treated them as rights there is a big chance we wouldn't be able to AR anything at all. Because things would be just just Clump together with all this clusters of

[45:00] for us contract, that happen to update totl on the same entes, which is why we decided, that we actually can reconcile the r changes after running all the transactions without risking introducing any nondeterminism and this TTL updates someand exchange is doing exactly, that changes how we update ttls in such a way, that changes to TTL done by transactions, that only touch the readon entries are not observable until someone actually writes the entry or until we have finished applying all the transactions, which is good enough to be able to actually run all the transactions, that have the same ke only the run them in parallel and still right have the proper TTL

[46:00] value and the algorithm proposed is pretty leny. But the gist of it is, that. If two transactions update to TL on the same entry they both will see the initial state of the TTL of the tantry and will be charged the respective fee for set. So for example. If transaction a has a key in with only footprint and updates TL of some say XLM contract rate by 200 ledgers increases TTL by 200 ledgers. And then transaction B increases TTL of exm contract by 100 ledgers well the transaction a will pay for extension by 200 ledgers and transaction B will pay for the D extension by 100 Ledges. But

[47:00] what we will do in background is we will correct this TTL changes and only apply the maximum change out of, that to the ler out of this two to the ler, which means, that the exm contract will be in the end extended by 200 lers and it does introduce bit of annoyance into how mattera should be processed, which is unfortunate. But I couldn't think of a way, that is both compatible with prism and doesn't require any changes in met ingestion and this change is, that basically. When ingesting the changes in TLS from the mattera you should never decrease TT this is only thing you need to do

[48:00] it is documented in the CAP and you'll probably make another announcement. But yeah we kind of get around doing something special for the map and this seems like the minimum possible change. So TTL like, that you record in the back end and never go down. But the TTL, That You observe in the is correct DET change for the mattera is correct in the context of a given transaction meaning as I mentioned before, that will charge the fee based on the changes, that you observe in The The Meta Meta and this is kind of the ju of it and U right yeah I can talk a little bit about candid generation. And then move on to question CAPs don't typically provide

[49:00] a way of how to actually build a transaction sets, which makes sense it doesn't need to be a part of the protocol as long as it is valid it doesn't matter how it has been built. But will'll likely implement the simple gitty algorithm described in the CAP and the idea behind it is just to pack the transactions into the stages R. While uring we are utilizing the stes properly by being packing from time to time and the efficiency of being packing heris sixs is surprisingly good. So in the end I think we can I added some benchmarks and we can both P the leeders pretty efficiently like uze most of the instructions and we can also deal with a decent amount of

[50:00] conflicts like. If you have some relatively sparse clusters of conflicts, that will not cause any depredation in lure utilization at all and in order to have any real degradation un need like some super inter interdependent transaction for, which I don't expect to be happening in the wild. So I think this should be good enough at least initially and. If we ever observe there are issues with, that ways to deal with, that both at the mle level and the U wayer of building the transaction sets I think, that's all I wanted to talk about just to present the CAP, and now I will most through the chat and read some questions

[51:00] yeah question from niik about the meta changes for TTL TTL M net diffs for ttls not emit one per transaction well we still need to emit TT per transaction I think. Because they are an effect of the transaction this is how we charge P and I I'm not sure it is correct to just omit them and whether we want to update meta I have

[52:00] considered this I I'm not sure it is necessarily a better solution. So basically the alternative to what is proposed in this app is well we could add yet another field to the mattera, that will explicitly output the TTL every entry facted by the wer the final for it after applying every transaction in The Ledger I like, that's potentially a significant amount of duplication unless we also kind of remove the changes for transaction I'm not sure. If it is a good idea to remove the changes per transaction. Because then well meta just has some entries drop, that transaction has actually affected, which makes tricky to debu transactions for example makes to understand your fees. If you ever wanted to run the computation logic

[53:00] comp. So I'm sure we can omit, that Pro transaction changes. But then I'm not sure. If duplicating this data is necessarily making things much easier to just not decrease in TTL with your track on on it it so, that's my opinion and I mean you either need special Logic for merges or you need special Logic Logic for this new types of TT entries and you also need to kind of sometimes ignore, that for transaction changes I'm honestly not convinced. If it is a good idea and. If you wanted it possible to only ingest the T changes after the LED. Then so, that you don't even need to process per transaction changes at all

[54:00] then basically we duplicate every TTL change into, which is again probably not very Fe you don't know n. If you want to talk more about this or we can continue offline okay I don't see any more questions and we actually have just three minutes left so. If anyone has any questions please feel free to comment on the ad discussions read it is yeah I guess the to meta change is really the most controversial thing about it everything else

[55:00] is really uncontroversial I feel. Because though we are just allowing to include more transactions in Ledger and we still shuffle them. So no changes for any. So yeah yeah and one final thing I'll say thanks thanks for presenting Dima We we forgot to talk about back to the unified events a one topic about how the sa events have the asset name of the topics. But SE 41 doesn't specify having putting the asset name in the topic. So lee is about to start a thread about this. So we can discuss it there yeah yeah Le just posted the thread in the chat yeah, that's all we have for today thank you for joining

</details>
