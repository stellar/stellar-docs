---
title: "Stellar Developer Meeting"
authors: [carsten-jacobsen, leigh-mcculloch]
tags: [developer]
---

import YouTube from "@site/src/components/YouTube";

## Part 1

<YouTube ID="hfl1n5BKbs4" />

Blend, a universal liquidity protocol build on Stellar/Soroban, is preparing to release V2 and launched the Blend V2 Audit + Certora Formal Verification competition this past Monday. There are $125K in USDC up for grabs, and the team are joining this weekâ€™s meeting to talk about the competition and walk through some code. Join to learn more about Blend and the competition!

Links:

- [Blend](https://www.blend.capital)

- [Competition](https://code4rena.com/audits/2025-02-blend-v2-audit-certora-formal-verification)

<details>
  <summary>Video Transcript</summary>

[00:00] Hello everyone and welcome to this week's Stellar Developer Meeting today we have the team from blend joining us for a code walkr in support of their exciting news security Initiative for the Stell ecosystem I'm Carson as usual from the SDF Dev team and and Alex will join us from script 3 and amen from Satur to set the stage here blend is getting ready to launch their blend V2 and in preparation for, that they're opening up, that code and for groundbreaking groundbreaking $125,000 in USDC competition focused on securing lend V2 through both a competitive auditing and a formal verification this is the first ever open competition competitive audit and formal verification competition within the

[01:00] Stellar ecosystem. So it's an amazing opportunity for Stellar devs to try their hand on competing to find bugs and write formal verification rules I just get more familiar with blend in general. So I'll post a link to how to sign up for the competition started this Monday. So it is open. But let me. Now introduce Alex hello thanks for having me welcome Alex do you just let me know. When I am ready to get running go ahead you are all right perfect your screen all right thank you so yeah I am Alex you might see me online as mots2 and I'm the lead developer at script 3 and we developed blend, which is a universal liquidity protocol primitive just a quick overview what we're going to run through we might go a wee bit fast. Because there's a lot of stuff to cover. But I'll give you guys a quick for those who aren't familiar just a quick

[02:00] breakdown of what blend is how the actual contracts all work together. And then I'll run into the code and kind of the main thing I'll try and focus on for those doing the auto competition is really going to be how we test blend and specifically. If there is some kind of bug or thing you found within blend V2 kind of some tips and tricks on how you it might be a bit easier to prove, that something is in fact wrong. So yeah blend is a universal liquidity protocol primitive and what does, that mean basically blend is a tool, that allows you know companies people who are crypto natives really any entity, that wants a lending Market to actually go ahead and have one. So lending markets the crypto are just places where people can land in BIO assets from each other and one thing, that blend does is blend basically comes with an automatic back stop module, which acts as Insurance bad debt protection and also scam protection and and these

[03:00] things these are funded by anyone and kind of the way this all works together is the pool will give some portion of the interest it generates out to the back stop module such, that it can provide, that that layer of insurance for the protocol, that's just a really brief intro as to what blend does I want to touch on first actually how the contracts all work together since been looking at this from a security perspective knowing how the main components of blend interact is pretty vitol. So kind of the really like Cornerstone part of blend is the emitter contract this doesn't have very many responsibilities it's core one is going to roughly Define the admin protocol token in this case is blend secondly it's responsible for defining the back stop module. is like I said earlier the contract, that basically holds all of the insurance funds for the blend pools insurance

[04:00] in this case are going to be blend USDC LP tokens. So there is a common AMM it's for those familiar with EVM it's basically balancer. But it's just an LP token for blend USDC its main responsibility is it actually controls the state of the pool so. If you have a lending market and there's a back stop module actually has the ability to turn the pool on and off. So for example. If something happens to a pool or it gets exploited some Oracle goes wrong the back stop module actually has the ability to disable borrowing for the pool secondly it's also the first less capital in the event of any bad debt and those, that might not know bad debt is basically the case where a user has liabilities and they have no collateral to cover it. So the protocol has to take a loss on, that debt secondly it also manages the blend emissions. So the protocol basically starting from the emitter blend tokens

[05:00] will get sent to the back stop module and it's the back stop module's responsibility to send it to all of the pools within the blend ecosystem within the backstop module it also defines a pool Factory this thing drops pretty simple it's a contract, that contains a pool wasum and for any pool, that's created this is the thing, that validates, that this is in fact a blend pool and it is the exact contract, that we expect to be deployed. And then the meat and potatoes of the protocol is going to be the actual pool this is kind of the sort of a part of the whole protocol it is the place where people can go and lend and borrow reserves. So there's tons of different ways you can set things up. But this is kind of the place where all the lending and borrowing occurs and kind of where most user interaction will end up taking place. So I know, that was really fast

[06:00] but hopefully is this is being recorded right Caren I think it is I'll take, that as a yes y it's recorded perfect. So yeah we can go probably go back and watch, that over. If anything was missed and I'm also available on Discord. If you want to ask further questions. But we will jump to the actual codes since, that's probably what a lot of you all are interested in. So I'll give a quick breakdown of how contracts actually exist in soron. And then we can start trying to dig through some of how this how our codes actually tested. So for those, that aren't familiar The Way We have set up our soron repository is you'll notice, that all of the core contracts, that I talked about earlier are located within their own folder. So for example all the code for the back stops within the back stop folder pool within the pool pool Factory Etc

[07:00] Etc each contract has a contract. RS file this is really the entry point and interaction point for the contract. So you'll you you'll notice, that in soran there are these macros contract contract client these are really the things, that defined how users actually can call into the contract. So anything, that's public facing is part of this trait and will be implemented for the back stop contract stru this is also the location where where we probably have all of the most user-facing documentation for each of the functions so. If you're ever curious about what's available on a contract what the arguments mean this file will be the place where they're documented. So yeah and there's also a contract implementation macro. So this will be the place where a a Constructor is defined. So this is basically called. When the contract's first deployed and can never be invoked again and there's also the

[08:00] back stop trait implementation for the contract and this is where all of our code is basically getting defined. So everything in here is exposed for anyone on the blockchain to interact with the way we structured things is we generally do all of the authentication, that's required to use the contract here. And then we actually perform all of the logic in a second helper function so, that we can better unit test the code as a whole. So you'll notice. If you're ever looking for authentication things you you'll notice, that all of the O actually takes place here. If you are from the EVM world o how Soroban does o is probably one of the largest differences between the two blockchains. So it is worth looking into it's it's incredibly well documented on sorond Sor bonds documentation. So I would highly recommend reading through how, that works. So you and

[09:00] then you'll also notice, that for example. If I go to where we actually Define the logic this is pretty common throughout all of our contracts. So we we we'll basically try and wrap all all of the logic, that's external to authentication and stuff within a function and we actually go through in write unit tests specifically for those functions. So this is a pretty good place to look. If you think something might be incorrect or wrong or a bug exists somewhere this is probably one of the easiest places to start testing, that and attempting to find whether or not something's wrong and you you'll often find, that this is like kind of where most of our unit tests actually occur secondly one thing, that we do include within this blend contracts repository is there's mock contracts and there's also a test Suites folder. So one thing, that's really nice about interacting with soron is as I'm using tests you'll notice I have rust analyzer installed on

[10:00] my fsco. But you you you have the ability to step through debug so. If you're ever curious about what actual values are within a function you're you can use all the rust internal tools to step through and actually debug it. If you're like most developers in the world you're just going to add a whole bunch of print lines. And then run the test, that also works fantastic too. But it's it's really nice for example. If you do think something's wrong make sure to use the rust tools and use use all the debugging tools to your advantage we've been able to find a lot of things internally using those those strategies. But one really important thing I wanted to touch on especially for L contracts is we have this giant test Suites folder we use a lot of this for all of the integration testing. So for example there is a test fixture this basically just contains a whole it's it's basically going to deploy from a fresh environment a whole bunch of contracts and things, that exist

[11:00] to basically support a pretty standard blend pool. So you'll notice it's going to make tokens it adds a whole bunch of users it deploys all the contracts out for you and you can basically. Then start from a really good starting point to actually write an integration level test one quick thing to note here is, that. If you ever see an our test the user Bombadil I'm a bit of a Lord of the Rings nerd. So he is the admin and ruler of everything. So he has the power to basically mint tokens at will he's kind of the admin the super key for roughly all of our tests as well. But how this is actually implemented you'll actually notice. If you look back at some of our older audits we've written integration tests after after basically fixing issues, that have been found one of these was an inflation attack, that I believe C Tor found in the first audit. But for example this kind of walks you through a really good way

[12:00] if you think something does exist, that might have an issue writing one of these level tests is a really really good way to actually prove whether or not it exists since this kind of is what at at this level of test this really is the place where you're going to have the same level of interaction, that someone using the blockchain would. Then yeah maybe the last thing to note before I switch over to the fev VA here is you'll notice on the test test fixture. When you cre the test picture there's a true false just. So it's clear you have the ability. When running rust integration tests to actually deploy the contracts and use WMS so. If you want to use the actual wasm files, that are being deployed, that's great we do, that in plenty of tests but. If you do, that rust itself won't actually be able to debug the code within the wasin packages so. If you're expecting some of the prit lines or line by line de debugging to

[13:00] work you will need to deploy all of the contracts as rust crates, that way you can use all the unit testing tools. So yeah, that's probably all I'm going to touch on here again within test Suites and test this is tons of really good examples on setting up all different kinds of environments. If you want to go through and attempt to prove, that something is incorrect lastly I'm going to also run through the Fe Vault contract this this is basically a add-on contract, that's aimed at wallets, that will allow users to specify either a fixed rate of return or excuse me a CAP rate of return. So for example. If a wallet wanted to offer say 10% on USDC deposits using blend they can deploy one of these Fe volts. If the blend pools returning more than 10% they would keep, that extra

[14:00] say it's returning 15% they would keep 5% 10% would go to the users or they can just take a fixed interest take off of all of the interest generated for deposits through this Fe volt. So it it works pretty similar similarly to any er ERC 4626 function or 4626 Vault token. So those, that are familiar or I've looked at those those kinds of protocols in the past this should feel somewhat familiar to you I did want to call out one thing you'll also notice some of the implementation patterns we use in blend also come over here. So the only thing, that's maybe a bit different is this contracts a bit simpler. So you you'll notice we've kind of there's probably one less layer of abstraction there for you to have to deal with most things are documented pretty well in code but. If you find anything please please feel free to call it out you'll notice, that in the test

[15:00] here it's set up a bit differently. So for example this is a very long test. But we we we mainly are testing the fee VA through integration testing since the contct is a bit similar simpler and it relies heavily on the blend contracts themselves. So you'll notice, that we we support a blend rust crate, that includes all of the blend contracts themselves this has been updated for the exact hash, that was pushed for the Auto competition. So you'll notice, that I believe yeah the version is here. But this comes with a very similar deployment script as the one we use within the blend contracts repository for their integration test. So let's see. If yeah there's a blend flixer deploy function, that will actually go ahead and create all of the

[16:00] blend contracts needed to interact with a very similar setup as to what blend view1 main it is so. If you do run into anything with the F VA this is kind of a great place to look to go ahead and try and prove, that something INF fact is missing or has a bug. So yeah, that's probably all I wanted to touch on I wanted to make sure I left a little bit of time for Sor to go through the formal verification portion of the auto competition. But I'll probably leave it there unless there's anything else you want me to touch on Carson great let's let's wait and see. If there's any questions at the end. But thank you for the presentation I'm going to invite Shandon Armen on the stage welcome both of you you hi Armen are you there okay yep hello you have my screen

[17:00] so this is the contest repo. So trra you can let me know yeah we should walk over sure yeah you can share and I'll I'll just go over it. So as Alex mentioned in his presentation before you know we are doing this audit and formal verification contest for blend and. While the audit itself you can you know all all of the prog the code bases in scope for the verification part we have a smaller scope. Because the the protocol is there's a lot of code. And so we decided, that it would be more tractable and manageable. If we tried to focus on one particular part and in particular we are looking at the back stop crate so. If you go up a little bit in this repository. So for what it's worth this is the repository where all the code is. So you can clone this repository. And then you can check out all the different crates and

[18:00] in particular the one, that we are talking about is the back stop as you can see Armen has it here and I want to show the Sora specific stuff. So first of all the the the properties, that you're going to be writing are going to be in the seras specs directory where Armen is right. Now I think I have a couple of little. So well. Now can you go maybe look at some of the other files there's a few, that I provided just for an example yeah. So here's a few examples of things, that you just to give you an idea for what these look like. And so you can notice here, that there is this thing called cvlr uncore assert in all the rules. And so the things, that are so, that's one thing I want to talk about a little bit so. If you look at line number five this is the we we call it Cavalier. So cavaliere is the specification language, that we've built it's just a rust Library. So you can import the library. And then you can use the various

[19:00] assertions and there, which are just macros, that we've defined. So assume assert and satisfy I think are probably the most common ones and the ones, that are I think we recommend using the most. If you need something more specific. Then you can always reach out and we can help you figure out how to do, that the other thing worth noting here is can you can Arman can you go down to the the cargo Tomo file at the bottom yes. So we are also relying on rust features. So for example here you can see on line 17 there's this feature called called Cur and the nice thing about this this mechanism is it actually lets you use this feature to selectively compile the code. So for example you can. If you go back Armen to let's

[20:00] see let's go in the back stop yeah and maybe in the withdrawal maybe go to deposit actually okay perfect so. If you look at the top of this file you'll see, that we are using this feature called sora and essentially what this is saying is, that. When the feature is enabled and I just showed you before, that in the cargo TL it is enabled. Then use this mock implementation of the token client and otherwise you can use the whatever default one, that blend was using and this is just for verification only right so. When you're compiling your code for other purposes. Then you shouldn't be using this but. When you're doing verification this is helpful. Because a lot of the times we want to modularize things. And so you may not need to include all the code for the token. When you're really just verifying the code for the back stop right so, that's an interesting feature, that I think you'll probably use quite a bit and you know always feel free to reach out on

[21:00] Discord. If you have any questions the other thing I also want to highlight is the con files. So on the left you will see, that there is this directory called conss and here we have provided some setup and some skeleton for some basic conss. So let's look at the one yeah the withdraw one is a good one. So what's happening here is we have this file, that has a a build script this is already provided you probably should not have to modify this at all it's just a python script, that does a bunch of stuff, that is necessary for making sure, that the reports look nice you know you have all the files uploaded there it calls the right build instruction in this case we're using this file called this build system called just. But other than you should not have to modify, that more or less. Then there's a few verification specific things. So for example there's this thing called we have this slide called optimistic Loop we we don't need to worry

[22:00] about what, that is I would leave, that there there's also this precise bitwise Ops flag, which is set to True what this is saying is, that. When you're doing the verification use bit vectors you may or may not want it it's you can. If you're facing like slow running times I recommend taking it out and seeing what happens and again. If you run into any issues. When you take it out you can reach out and we'll try to help you. And then really the main thing you will have to modify is this rules field. So whatever rules you write you just have to add them to this file. And then you just run this San with this configuration file I one thing I would recommend. So it is very common for users to run the prover with many ruls and, that is what you will do in the end but. When you're debugging and. When you're just you know trying to test things out it is often helpful to just

[23:00] run one rule at a time. So you know, which rule is problematic or you know, which has a Vu problem or all kinds of stuff right. So definitely keep, that in mind. When you're just getting started and you know writing these as as I can tell you there's a lot of excitement already on Discord lots of people have been asking lots of questions really good questions. So it's already really nice to see the the engagement from the the community so, that's, that's really cool. So I think let me just think what else I would like to show actually yes can you Armen. If you click on the documentation page, that you have open. So yeah here we have some documentation for using Sunbeam. So there's the installation guide, which I think you probably don't need to worry too much about. If you. If you have installed ctra CLI you should be good to go. So I don't think you need anything else. So the user guide is the main thing you know we have some examples of you know what the

[24:00] different components are. So for example here you know you see this like hash rule thing on top of these functions. So really what's Happening Here is you're just writing rust functions right. So Cura is like interpreting those functions separately especially for verification. But from your point of view you're just writing rust functions with special macros and special annotation so, that's one of the annotations is this rule attribute and I already mentioned the CVL assume assert macros from the Cavalier spec Library another thing, that is very useful is this notion of non-determinism. So oftentimes what happens is. When you're doing verification you might want to sort of summarize some piece of code. Because you know maybe, that's not super relevant for the property you're verifying and all you care about is, that that function returns some

[25:00] non-deterministic value right it could be a u64 it could be i1 128 whatever it could be a user defined type all kinds of Stu. So this nonb gives you a mechanism to do, that. So Cavalier actually already implements a bunch of non-ets for various primitive types blend has a lot of userdefined structs right. So for example there's the Q4 Q4 withdrawal struct you might want to implement, that trade for this this truck right. So you can do it this way. So right like as there's an example here. So essentially all you do is you know you assign non- debt to all the fields of the struct and you're good to go. So this is something, that's really helpful for for verification we often have to do it and I'm imagining other people will have to do this as well we have some very basic examples in the tutorial. So we have a Sunbeam tutorial over over here it gives an example of the token

[26:00] contract so. If you've never done verification I think this is a good way to start you can just do the exercises here and you know just try to understand like what what's really happening. When you run the tool. And so so on and other than, that. If you go back I think I already explained the scripts the build script stuff. If you scroll down a little bit more Armen yeah I think they these are already covered say. So I think the main other thing I would recommend looking at is what happens. When you do run Sunbeam. So as I mentioned before and it's actually also listed in the readme for the the repo all you do is you just run Cur San proofer and you pass this Con file and you you run it and all you get is a link to a run right and this link is going to look something like this right. So here you can see well. So here there is no violation. So you're fine. But

[27:00] if there is a violation you can see a call Trace the call Trace is not super easy to read so. If you are stuck with problems with the call Trace please post messages on Discord and we'll try to help you as much as possible you can also see the code right. So for example here on the left you have this files you can see all the files, that you used to run verification all the properties, that you ran all the code, that you changed or properties you wrote everything is here. So and I would always encourage you to. If you are having any problems you can just send me a link to a run. And then I can take a look at the run I can reproduce the Run locally and and help you debug your problems problems other than, that I'm not thinking what else would be useful I think about, that's, that might be all I have like oh actually I would like to mention one other thing. So we already saw the feature CRA

[28:00] feature of mechanism where you can selectively decide what functions to include. When you're compiling we also saw the non-dead macro another thing, that you may have to do is summarize various functions. So this is something, that you know for example here the way the best way to do this is the following right. So in the C specs directory you there is a directory called summaries and in there you can. So the mod is, that's where you just expose all the summaries, that you have written. So here there's you know this file called emissions and in emissions it's possible, that depending on the property you're proving you might have to summarize right. So for example you can say, that I don't really care what emissions does as long as it's returning a Nonet terministic value or you can, that well it should be a nondeterministic value. But maybe it should be in some range. So this kind of stuff right like it really depends on

[29:00] your understanding of protocol and like what you think is necessary for whoever is calling this function to know about this function Behavior right. So this is actually often very helpful. So you can put all your summaries here. And then wherever you're calling the function instead of calling the original function you just call this function and it would be really good. If you actually annotate it with this. Because then you know you're not really changing the semantics of the original program great I think we need to wrap it up here. But thank you so much for for this presentation I think this was very helpful for everyone who's going to participate in the competition to get to walk through both of the code and and the verification here. So thank you all for joining I didn't see any questions in the chat. But yeah feel free to reach out also in Discord. If there's anything I'll post the link again in Discord for the competition. So good luck to everyone

[30:00] the competition is running until March 17th. So still have plenty of time to to dig into it thank you everyone for joining thank you Shandra and Alex for for joining here us thank you everyone thanks everybody

</details>

## Part 2

<YouTube ID="9oCKAhbFzUY" />

In this protocol meeting Dima's and Leigh's prototypes for dealing with memos/muxed accounts in CAP-0067 are discussed, as well as updates to CAP-0066 (Soroban In-memory Read Resource).

Links:

- [CAP-0066](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0066.md)

- [CAP-0066 Discussion](https://github.com/stellar/stellar-protocol/discussions/1585)

- [CAP-0067](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0067.md)

- [CAP-0067 Discussions](https://github.com/stellar/stellar-protocol/discussions/1553#discussioncomment-12306846) and [CAP-0067 Discussions](https://github.com/stellar/stellar-protocol/discussions/1553#discussioncomment-12309408)

<details>
  <summary>Video Transcript</summary>

[00:00] Protocol 23 we are introducing the hot archive bucket list where we will actually actually store archived entries in a separate database and what this allows us to do is to store all live sorb on state in memory as well as store andan module caches in memory and we can do this safely. Because the rent system is St archival. So what we were talking about today is an update, that we made since the last time we talked particularly about the way, that we calculate the what today is called the target bucket list size. And. So in Protocol 22 whenever you pay rent or write an entry to The Ledger this fee is variable based on the current size of the bucket list, which is just the size of the you know network's database on disk and essentially we have a Target size and. If this Target size is exceeded. Then the cost of right and R bums increase

[01:00] Very rapidly and essentially this allows us to have a soft CAP on the current size of the bucket list or the the network database from a fee perspective. So you can technically still write but'll just be very expensive to do. So. So people will stop writing. And. Then eviction and styal will do a back pressure to overtime reduce the size of the bucket list. Now the thing is previous previous to Protocol 23 this was all measured in database size. Now there were two issues with this the first one being, that this is a sorond based fee and most of the sorond state or most of the core database wasn't soron state classic State significantly dominated the size. And. So we had this weird system where even, though Soroban took up very little space in the bucket list it was being charged fees whenever classic entries

[02:00] Would make changes. So, that's kind of issue number one, that you have classic influencing the cost of Soroban State. Now the second issue is, that with Protocol 23 we. Now have a bunch of storage or all the Soroban state, that we actually store in memory instead of on disk. And. So it would our kind of goal with this setting is to make sure, that the protocol can CAP the maximum amount of memory, that Val to use it at a given time and. If we were to continue to use just the disk based metric this is no longer able to actually CAP the amount of memory, that you need to store. Now, that we cach everything in memory. And. So the update to CAP 66 is, that we change the bucketless target size byes to soron LIF State Target size byes. And. So what this means is, that instead of using the entire size of the database to calculate rent fees and write fees we only use the

[03:00] Size of the live Soroban state, that we actually have to store in memory. Now there's a couple of details here particularly I want to talk about contract code. And so. When storing. So the plan is to store all contract data and all TTL entries in memory. And. Then to store instantiated contract code in memory. Now contract data and contract TTL the thing, that we store is the same size as the entry on disk. So for instance like. If you have a contract data on diss, that's The Ledger entry is you know 64 bytes. Then we store 64 bytes in memory. So there's a one to one ratio for data and TTL for contract code this is not the case. Because of the module instantiation cach changes or the instantiated module cache we are not actually storing just the contract code bytes in memory. But we're storing an instantiated model and this model can be up to 40 times the size of the on disk

[04:00] Wasm bites in the worst case. And. So what we're doing instead is, that for contract code in particular instead of using the size of the contract code entry for fees, which is what we' been doing up to this point you're going to instead use the size of the memory given by the instantiated contract code module. And. So in the worst case this is a 40 times increase. But in the average case this is only a 10 to 15 15 times increase over the size of the contract code as it's calculated today. And so, that's change number one is, that essentially. Because we need to account for the in memory size of state. Now instead of the on disk size we need to make this adjustment in the contract code size calculation. Now the other adjustment we're making is with r fees. So for rent fees essentially you are renting under you know Protocol 23 you are renting space in the in

[05:00] Memory cach of the network, that's essentially what rent is doing. Now. And. So we the rent fee will remain unchange how we still have like this target live State size. And. Then the rent fee will you know increase very rapidly. If the of Life State increases this for right fees doesn't really make sense to have the right fee a function of the current amount of Life State. Because we're to using memory based bounds instead of dis based bounds in the Life State we a dynamic wrry doesn't make sense. And. So what we're proposing is, that we still need to have a right fee. But you are not it doesn't need to be based on the current size of the bucket list rather it just needs to be based on the computational cost of doing a right. While you're applying the transaction. And. So we're making a

[06:00] Change, that right fees are. Now a flat fee per BTE or per kilobyte rather. And. So instead of having like this curve the rent fees will still have, that curve. But the right fees there'll be a network config setting, which is fee per 1 kilobyte R. And. Then this will be applied based on the size of the entry you're writing and this will be applied to both Soroban entries you write as well as classic entries you write. So, that's, that's the primary changes essentially we are changing the way we calculate contract wasm size respect to fees rent fee is still variable based on the amount of Life state right fees are. Now flapped any questions or or concerns

[07:00] Morgan's question about the the cheap storage we don't it's not. So much, that the storage is cheap. And. Then non-e it's, that before you hit the target the storage is reasonable reasonably priced. And. Then past the target the storage is like very very ridiculously priced. So the idea is, that you know we still even before you hit the target we still charge fees and the intention of this fees is to dissuade people who aren't using like who aren't legitimate apps using the space. But essentially the reason, that we have this target is, that we don't want a Dos angle where an attacker could just write you know gigabytes and gigabytes of state, that we all out store memory. And. Then oom kill them nodes. And. So I don't think the distinction is cheap versus non- cheap it's you know reasonable versus like ridiculous pricing, that we only you

[08:00] Know resort to in kind of the to protect ourselves from malicious attack see any numbers for the threshold I think exact numbers are TBD for reference we're are. So currently the we use bucket list total size. And. So I think it's like 12 and a half gigs or something like, that the target something in, that range. But. Because we're switching to only meter the life Soroban State it'll probably be something on the scale of hundreds of megabytes just. Because today the total sore on

[09:00] State size is like 40 or 50 megabytes. So, that still gives us you know lots of growing room. But is a very small you know value requirement as far as memory. But again you know we we still need to think about these numbers a bit more as it's kind of you know on the range of what we're looking at yeah to add to, that since vasm kind of takes more space. Now than it used to take in the database yes we need to re-evaluate this in memory State size including the module cache and I don't think they have done this yet. So. But the it is I don't think it will be significantly bigger than tens or hundreds of megabytes and threshold to be set I think quite High compared to, that probably it can easily be a few times higher, that you

[10:00] Know we didn't get, that much dat in a year I don't think like. If you said it to 2x 3x of the current state I don't think we'll run out of space anytime anytime soon yeah and to just to make it clear for for Morgan's question like we like the purpose of these limits is not to you know limit good users. So you know. If we see a dApp, that has 100K daily active users I mean, that's not going to happen overnight there's going to be a ramp up. But we would definitely probably you know introduce the slip SL SLP to raise those limit. So the intention is not to you know reduce actual good good usage of network

[11:00] Yeah and I guess Nico pointed out a good point, that I forgot to mention is, that. Because we are switching from on disk metering of contract code to in memory metering the size of the current instantiation model is protocol dependent. And. So for instance. If in the future we change to a like a g instead of interpreter. Then the size of the instantiated model will increase. And. So I didn't mention this. But well for fees we are for the rent fee in particular we are using the inmemory size of the contract code. But for limits and for the right fee we using the on disk size and we have to do this. Because for instance like we have like a a maximum contract size config setting and. If you can assume, that you know a contract today has the maximum size we wouldn't want want to break, that contract in a future

[12:00] Protocol upgrade. If the inmemory size just happen to change. And. So yeah I think, that's, that's a good point I forgot to to mention is, that only the rent fee is dependent on the contract in memory size and, that means, that like a and, that's and the target sorond life State size is also calculated using the inmemory size of contract code. But as far as write fees and transaction WR limits and contract code size limits those are still determined by the wasum and we have to do, that. Because only the actual on disk you know wasm size is consistent protocol to protocol actually. So my comment was actually a little bit different like. Because I had a chat with grer about this and it sounds like the current P request we have

[13:00] For CAP 65 is actually caching per host basically. So like. If you have like two host like core can be configured right with multiple host to support multiple protocol versions and right. Now we keep in memory the version of the WM basically like the the pre-processed version per host. So it it gets basically like. If you have like two H in memory is going to have two versions of the wesm cache per contract and, that's kind of a to make it easier to to to reason about. But at the same time, that means, that the overhead in memory is actually dependent on how core is is compiled yeah I think this is I agree there something to look out for. But I think this is primarily an implementation detail at this point. Because I think in the yeah no it's totally is it's just like

[14:00] That's why I said there needs to be an SLP to to kind of discuss, that. Because the your memory limit basically, that you you pick is has to take into account the fact, that you have basically a overhead of two or 3x for you know for the resident wiom, that maybe is not as trivial as you know. If you look at calibration. While like. If you're asking one host how much memory you know are you using right. Now, that's actually not the truth you you need to to basically lie through the network settings, that you know you actually are are going to count was them in memory let's say with a 3X you know a multiplier on top of what's actually using per per host does, that make sense it's basically the yeah the calibration settings yeah yeah I think

[15:00] Longterm like a like. So longterm. When we actually have like large amounts of States say like in the gigabytes. Then I think we can probably avoid this issue by doing something clever like you know like. If we are armed for an upgrade. Then we can like you know like do the compiling for the new protocol version in the background laely. And. Then serialize it such, that we don't have like the double memory overhead. But. But I agree as like as far as like short-term and medium-term plan goes I mean I think like the state size is still small enough, that like we can set limits like with these assumptions it' be be fine. But yeah I agree, that we should have these these this like 3x factor in mind cool I guess. If we don't have any other questions or comments about this we can I'll hand it off to the next

[16:00] CAP right thanks Garen for presenting this and yes the next thing we have on our agenda today is CAP 6 to7 and specifically the issues, that concern handling of transaction memers and maxed accounts in the event not sure. If CAP is even an appropriate link to share right. Now. Because most of the revant stuff is captured in the the discussions to the CAP and I think the the point of contention during the previous meeting was how exactly can we

[17:00] Enable maxed accounts in soran and after spiking a few options considering the fact, that it is likely for pretty much any custom token to be interested in being listed in centralized exchanges and as being compatible with custo Vols, that centralized exchange may use for multiplexing multiplexing it seems like this is something, that most of the tokens will want and I think this is one of the key requirements, that kind of made the whole discussion pretty hard to converge on. But since kind of came to an agreement, that this is probably a reasonable feature to have and contracts will need to deal with Max addresses addresses anyways the current preference is to

[18:00] Go with an option of add in simulus support of Max addresses to the SDK and what tokens extend well not extend. But update the transfer function interface to be able to accept Mark addresses I don't know you do want to add something we di into details you asked to speak yeah I think maybe it's just worth calling out for people who listening of the conversation last week I think an assumption we were making to be really clear it was an assumption, that this needed to be an extension and I think you know to has sort of pointed out, that it's safe to assume, that pretty much every token would be interested in being listed in an exchange you know. If if, that was. to happen. And. So there's

[19:00] Little reason to make this an extension where it's sort of targeting a very small number of tokens like really just sort of all tokens should should be able to handle the case where they're given a Max address or a memo. And. So much of the complexity I think in some of the spikes sort of disappears once we don't try to make this an extension right. So to summarize like what we are currently going to do with a protocol is, that we will add a new object type to soran specifically for handling maxed addresses max accounts currently But I'm actually not sure. If you want to do contracts or not. But we could we wanted to and the interesting thing about it is, that since it's in the

[20:00] Protocol well the primary reason for why we are doing this work is for the tokens to be able to support this basically any protocol will be able to use them. If they want to. So. If custodial Solutions are necessary some other protocols basically the solutions for single address, that can have multiple sub virtual sub accounts they will be able to implement this and I think this is one of the benefits of this it hasn't been discussed before even, though like we don't have limited obvious use case. But know folks may come up with something I definitely recall having recall seen some discussions here on Discord regarding virtual accounts for multiplexing support

[21:00] so yeah we we had this new new object type and also at the SDK level we make it. So the regular address type and this new Multiplex address type are wrapped in the same SDK type, which means, that for example. If contract just operates on the addresses and it doesn't know exit your token started except in multiplexed addresses for the transfer as will break. So for example. If contract a calls a token contract and passes a regular address non Max address to it things will keep working even the token contract updates to this new proposed feature, which is to allow transfers to have multiplexed destinations and sources and this is what what has

[22:00] Been spiked and this seems to kind of work and in terms of complexity on the token side it doesn't seem like there is too much it's few additional lines of code to convert from multiple addresses back to normal addresses, that can do all the interesting things, that addresses can do, that's kind of a hell level thing I am not sure. If they need to go too deep into details right. Now I guess the conclusion from the discussions for is really, that seems like we have a way of making this work in a non disruption non-disruptive fashion meaning, that the existing contracts will not be broken and it is also possible for the clients to discover. If a contract reports multiplexed addresses at all or

[23:00] Not, which may be relevant in some context right. So I think this part is more or less clear part is still unclear and I wanted to talk about a bit more is how exactly are we going to represent the multiplex destinations and the events and the option of just putting them into topic is problematic. Because it would break index. If they don't do anything special this Multiplex addresses break in a sense, that they will have too many virtual destinations, that they really shouldn't be caring about. So in the link I posted yeah thanks Le for posting other discussions yeah in the discussion the da post ly has listed different

[24:00] Approaches approaches to actually how to handle the events with mlex destinations and I think we haven't reached the full agreement on this. But again it seems like from my own preference and I see Alex has commented on this this seems like what we could do is you could just converge yeah I say want to talk. But I guess the current preference is to converge all the possible memo, that we currently have support in the transaction converge everything into the single Multiplex address data structure. So, that for the classic transactions you will be able to generate Multiplex destination based on the transaction memo even of this transaction mem is non ID and for s b

[25:00] Use cases you only support ID MERS to kind of ruce the potential amount of confusion I know Alex. If you want to okay Alex doesn't want to talk I don't know. If you want to yeah no I just you know my comment was basically my preference just as one point of view on like, which one we should use I guess I do have a question. Now thinking about it like would this cause any breakage to how people parse M addresses. If we were to expand M addresses to actually cover all types of memos well it depends on where exactly we put it in the sketch I've posted above the above in the same discussion I'm actually only extending SC address

[26:00] Address type, which means, that classic Max account data structure will stay as is and the reason to keep it as is is, that it's kind of over the place in the protocol like a lot of transactions have maxed accounts as sources or destinations and yes there is really no good reason to kind of retroactively Plum all this memos into the classic Max account type and yeah this m will just remain in address and it will be just yet another special SC address as the remainder of with CAP 67 we introduced like chable balance address and liquidity po C address and both can only appear in the context of this unified events sources of or destinations of payments and this is basically the Third special address kind

[27:00] That only appears in unified events coming from classic I cannot say this will definitely not cause any breakage. Because well we are kind of in order to achieve this we need to not well kind of break right the token events by converting the data field from a single integer to a map there is definitely this breakage and you know. If someone there something to address they may or may not be broken. So PR definitely some cost to it. But it's not really specific to this proposal of adding this this maxed address with memo whatever new type. Because we kind of already have have this issue. So yeah I hope the breaker scope is really minimal especially. If like we support this iny libraries and whoever

[28:00] Is using string key library to just convert to and from a c address they will just need to update the version and hopefully everything just works yeah George you turn stage yeah yeah I just wanted to add, that yeah I 100% agree, that we should minimize or we should try not to make any changes to the actual mxed account XDR address or like the M string key format. Because there's deep assumptions in a lot of places Downstream both in platform products and Beyond in the ecosystem about the M string key specifically having the integer as the ID. So yeah. If we can isolate, that to SC address, that sounds great right, that is option four right or

[29:00] Option yeah four right yeah or it's it's more like it's not option three I think option three is the only one, that modifies the string key yeah I think option two is the closest yeah like using M wherever possible. But not trying to shoehorn text and hash into an M address as long as we're talking about the stry representation not some other representation yes yeah I think, that's right yeah I understand, that concern I personally. When I look at these four options and I think there's more than these four options. So I think you know. If people have other ideas like please post them to this thread these are just the first four, that came to Wine the attractive thing about the first option and the third option is, that it's very consistent with what you see going into a transaction for I think for

[30:00] exchanges at least or for legacy users. So. If somebody puts in a memo, that's a string it comes out as being labeled this is the here's the string in in exact like it looks exactly the same. So. If somebody's using developer tooling they see they decode a transaction they see this transaction had a particular memo they decode the event they can see the memo right there like they look exactly the same. And. Then the same thing. If you know. If the mammo is an integer it looks exactly the same and. Because we need to decode the M into a G for the topics I don't think it's such a stretch for option one to to like break it apart and for even in the MX case the integer to come out at least there's some consistency there the concern I have with the two case is, that. Because it's sort of a little bit less consistent you can create a transaction with a Max

[31:00] Address and you get an m on the other side, which makes a lot of sense. But. Then you create a transaction with a g in a memo and some memos result in an m and some memos don't result in an m. So yeah this might just be maybe this is an edge case we shouldn't worry too much about. But just, that inconsistency seems surprising to me I think it will be surprising to somebody just to clarify real quick isn't it normally the case, that. When people use the ID for differentiation on like Omni bus or custodial accounts, that they would use the ID form like text and hash don't seem like they would apply necessarily there. So it might be like an edge case, that you know they're attaching it as a piece of text, that is supposed to like display something. So putting, that into a

[32:00] Memo would be it's not intended to differentiate yeah we should I mean we should use the big query data set just to validate just to make sure we got these numbers exactly right. But. When I've looked at this in the past I have seen plenty of text usage actually. But a lot of the text usages exchanges placing numbers into text form. And. Then using the text the memo text I see. So kind of a misuse of the text form yeah I think Jake has just commented on, that I think just our comments about what most exchanges are doing we just need to validate, that. Because I know like one exchange, that actually does use MOX counts well they support both moxed and non-med for the non-med they use text. And. Then for the moxed they're obviously using the ID, which is interesting

[33:00] I think one of the advantages of option three, which modifies the M string key format. So, that it can contain more information information is, that it sort of pushes everything towards, that MOX address format I don't know. If that would really change adoption of it probably probably not. But it does create like a single unified View and the data, that comes out the other end I'd be interested in Simon. If he's here on like you know what the perspective is from like a data consumer perspective like a future data consumer perspective assuming, that the network will always have more users tomorrow than it does today yeah like, which of these approaches makes the most sense

[34:00] For future users and future consumers hello I would actually say for I guess from like a big data or like olab perspective I don't think any of these would cause a problem for scalability yeah I'm not sure. If that's helpful or not. But I think the amount of users, that unless it gets into I guess like the billions of something range, which I guess is the like possible won't be an issue for like the next decade or. So I don't think this would be a problem from like an olap perspective

[35:00] What about from the perspective of your a data engineer trying to represent this data. And. So you're looking at a lot of the data has just a single value for this destination. And. Then in some cases oh this value is actually a combination of multiple fields or or something like, that does, that make it more or less difficult. When integrating into other systems or would, that be like a concern or something, that could be like a foot gun like something, that'd be easy to make a mistake with I think for our use case I don't really have a concern with, that I think the concern would be more for whoever is ingesting from like RPC or creating like some Horizon like end point. So I don't know. If I'm the best to speak on, that

[36:00] Yeah from from Horizon. So from Horizon's perspective. Because we already support M addresses in their current form it would be like the format itself would break right. Because you would need a way to distinguish what kind of memo it is. Now. And. So anybody who's relying on the current string key format would know longer be open be able to use the same string key format right any existing you know queries or lookups or parsing routines anyone who's trying to find stuff in Hubble right they would have to reformulate their M address as far as I understand it I mean it depends on how it gets formatted in the XDR I guess or how the shinky definition defines it. But yeah it seems like this would cause a lot of pain for Downstream. If we extended it to be more than just strictly integer

[37:00] IDs I think it can. When I had to look at I did a quick Spike having a look at what would need a change about the M string key and it's 100% extendable without breaking existing string keys. So without confusion. So like existing string Keys would continue to decode to the exact same value and for any existing decoder, that follows the back any new string key wouldn't overlap with any existing string key. So, that every new string key, that didn't follow a different format would definitely fail like say like. When I say new I mean like the text the return and the hash typ of MOT types. So I think we definitely could do, that. But without breaking existing systems. But you know whether, that's the right way still to do, that is another question you know based

[38:00] On on there will be systems, that probably won't upgrade to the new format and they might assume, that there's nothing to change. And. Then they become broken yeah I don't know I'm interested to hear like Nico you were just talking about exchangers will crack open the M address for case three. So, that's strictly worse as a use case could you unpack, that a little bit yeah I mean it's basically like you know in in the in their inje system what they are looking at is deposits to their hot wallet. And. Then they. And. Then the memo, that's how they you know ingest the data like I don't see like why they would want to basically track like M addresses as the kind of the the deposit key for for you know separate from the hot wet wet basically yeah, that's a good point also for for exchanges, that use Define

[39:00] Those identifiers like per customer too like you're you are really interested in, that identifier whether it be an ID or a string I think what I hear you saying is exchangers actually don't really care about the M address after, that import other than using it as an input. So, that users can just enter, that one value. When they're ingesting syst doesn't really am I guess you can still argue, that as a someone who's we're basically deciding what is going to be P published as an event. But you can always go back and forth between the M or the address plus memo right and you can abtract, that away in the SDK or even like as an indexer was actually ingesting this data and converting and serving at. However you want right. So does it really matter I think the point about breakage, though is very important right CU I feel like you first

[40:00] Said it wouldn't cause any breakage. But for anyone who is parsing currently M current M addresses and they don't upgrade their implementation to the new thing. If they see an address, that is using the extended version they would probably break right yeah in, that case they're not going to be able to decode it. Because it's it'll be a different format yeah I think it's really some confusion. Because I think we might be reading this proposals differently I do not think any proposal assumes didn't shink keiss anywhere in the events yeah, that's actually exactly what I was about to ask like what. When we say from M and to M right in the transfer topic from G and 2G is an address yes basically basically in my mind from G2 g means, that from SC address account ed25 whatever to C address account and

[41:00] From M to m means address maxed account, which I have introduced in my Spike rate and address Max account variant has the max account XD payload U, which is existing Max account, which it doesn't even have to to be frankly it could be just you know Ed plus ID there is no additional level of neration and the same goes for this new memo account from z x j point it can be just a variant of a c address, that contains G well ed25 key and memo, which is just the transaction memo tapee for example. So this key discussion is rather like how how do we

[42:00] Like like. If we want to convert to string key or not. But you know the event structure does not change I think well I think there is an impact here. So yes string keys are not part of the protocol and they're not part of the XDR. But they are part of the developer experience and how we structure this will impact it to a degree. So you know we you I agree, that you know we can bundle these two pieces of information the address the actual address and the the memo separately or together in different structures the same structures. But we do need to make a decision in the event. If it's a top like. If it's. If if this event the data section is going to become a map is one of these fields going to be something, that we expect to be a single unit or we going to expect them to be separate units. And. So and, that affects how it's going to render in things like the developer Tooling in the RPC Json API. Because

[43:00] That's yeah like, that's where we actually do map things to string keys and they either map or they don't map well. If if we make them like separate fields in the data nothing, that we built today for developer tooling is going to be able to map those things to a string key we want them to be a string key. If we put them in a single field. Then we have, that option. But, that may or may not be a good idea and we're also talking about redundant data too. So does it you know. If the G is in the topic does it really make sense to spend another 32 bytes and actually keep, that g in the data as well, that's unclear to me right yeah yeah I just want to point out, that like basically for the exchanges I don't think any option is specifically like bad or inefficient. Because like still have a somewhere in the event. So they you

[44:00] Never need to like par this out of strink key as for the overhead yeah I think it's a valid concern yeah I think like number one I think niik sort of pointed out, that that was an option, that was is like low risk of doing the wrong thing with like option one is. So simple you know, that everything's pulled apart you've got everything in the event the really only downside of option one where everything's broken apart is the developer experience isn't quite the same. So. When you're looking at a human readable version of an event you're going to see this memo and a g address you're not going to see, that M, that was in the original. But this already happens everywhere you know. When you go to `stellar.expert` or other explorers and you drop in an M address it immediately drops you into a page about the G address. So I don't actually think we need to have complete consistency with madress goes in madress

[45:00] Comes out yeah yeah I guess I just generally say I'm generally for option one I think it's simple the messages smaller smaller yeah yeah I think this may be fine and also like for the consistency part like at least like. If you can have M addresses in the contract invocations things are immediately much less confusion. Because you know. If you wanted to check your your transaction like there is a very good chance, that block Explorer will correctly say, that you actually have performed the transfer to an M address and you know not some pair, which was my concern for for the input. But for the events I agree it may be not as important and events are generally much less human reable. So maybe yeah it's not worth to optimize for exactly the same range range as rendering as we would have expected

[46:00] From the signature of the function. So yeah I think option one is probably fine okay. So it sounds like we're leaning towards option one no one's opposed to, that if. So we can move on to the next topic yeah I'm good all right the next thing I think we should discuss is what to do about the TX memo. If we should admit, that as a separate event. Because you know for example. If you're replaying. If if an exchange is replaying using Classic Events and someone sent a you know a payment to coinbase with TX memo and the M and a

[47:00] M account we don't have space in the current events as there as they defined to include both like both the txl and the MX information. So an option, that Dio mentioned is we could just emit a system event similar to how we CAP 67 emits a fee event and, that event will just contain the the the TX memo. If one existed anyone have any thoughts on, that I mean I you're saying sorry sorry like. So you're saying like you would not evit emit a m or whatever like know this destination destination memo event. If you have a transaction memo no. So. When I'm saying is like on a payment. If you use a MOX account you will have a des station memo in the transfer event right. But. If that transaction also had a TX memo set what do you do yeah. But. But

[48:00] First of all like I'd like to understand what happens. If you only have a transaction memo and not a Max destination yeah. So the CAP currently, that says, that you there's like an order of Precedence. So you would pull the TX memo and put it into the event. But. If you emitted the TX memo separately it would probably make sense to ignore ignore, that order of Precedence and have the consumer pull the information, that they want right no, that's mistake yeah. Because we saw with Horizon right like the this model where you let clients kind of decide we I think this is the opportunity to actually fully specify this precedence order. So, that you don't have ambiguity right. Because I'm pretty sure I mean I don't know I don't want to put you know exchanges in in you know in trouble. But I imagine, that the This president's order is not

[49:00] Actually respected. Because it's actually fairly complicated no what well let's let's I step back for a second is is the ability to re for an exchange to replay from Genesis using events a requirement. Because. If it is. Then you you can't make assumptions about. If they care like you know certain exchanges may care about the TX Momo and some may care about the M information right I think this is a new event stream. So we get to do what makes sense. If I think yes you don't want to lose the information, that there was a TX memo yeah at the same time I do think, that the transfer events should contain the you know what we think is the I mean as as per specific as per the specification of transfer events what we believe is the the right you know destination memo in this case yeah and I think on, that point it

[50:00] Doesn't I'm not actually sure. If reap like we can't we change this this is a new event Stream. So the replay can use the new semantics. While the previous versions of meta and the transactions still describe the old semantics yeah the the events can you know do whatever we can be specified. However we want them to be maybe I wasn't clear like like what I was initially saying is, that the the TX memo would be in this new event and the transfer event would only have any it only have MOX account information and and I think ni you're saying, that in some some cases we should push the TX memo into the transfer event as well I think for yeah like. If we are going forward like it's more like yeah what do we want the those events to look like for Classic Events I think we want them to properly represent the

[51:00] Destination and. Therefore you need to see the transfer event with the proper destination plus memo like you know like option one let's say right even. If the memo is actually at the trans has been specified at the transac only at the transaction layer okay. So. If the account wasn't moxed. Then for the the memo into the transfer event and the M basically indistinguishable. If you have like only the transaction level memo only the destination you know M account on either transaction or operation right those three cases should all end up with the right U you know transfer event with the added memo information as far as like the you know do we need a an event for the memo I don't know what the use cases for, that are. So

[52:00] Yeah like like people you can already get, that by by pulling the transaction itself and you know cracking it open basically. If you want. If you really, that. But I agree, that that you can do, that that. But like didn't didn't we want consumers to not have to do, that I guess it depends on. If that's a requirement or not we care about in the context of. If I think the context of the CAP was transfers in the context of transfers this sound seems to be out of scope okay sorry I'm I'm not totally tracking with

[53:00] With what we've just said. So Nico is saying, that the, that that the emitting an event purely for the TX memo like a separate event is out of scope in in terms in in the context of transfers I'm not sure about this spe I'm specifically thinking about the the replay case for exchanges cuz I you know. If you what what what is what happens. If you have a transaction of the TX memo and MX MX destination account. Because without the TX memo event you can only show one right. But in, that case the thing is, that. If people have like complex logic they will have to actually derive it from the transaction they have to know exactly okay did we, which combination of things right where they into I think, that's why I'm saying like the the transaction memo is not sufficient. If you. If you're get. If you're starting to pull, that thread like you have to do transaction

[54:00] Memo two second on the transac I mean destination oh actually no this one I guess the override can only happen on the two. So it there's only one place. But. If it's the source, that's where you have like the transaction Source or the operation Source right, that are on yeah yeah the source does make this more complicated this is well I'm actually not sure. If the source makes it, though much more complicated does it isn't isn't the complexity, that an exchange might be reading the TX memo today and actually ignoring the M the MOX address and just using the G part of the max address isn't, that the complex part yeah well. So, that's the, that's scenario, that I like this event might help you solve maybe right. Because I'm sure, that I'm guessing, that happens today like. If I send a an a payment to coinbase with the MOX account set and a TX memo like they. If they they probably just I'm guessing they treat it as. If the account

[55:00] Wasn't mxed right no, that's why we don't know. Because it's yeah under specified okay I guess for this we'll like to me, that yeah like the we this discussion is more like I see the historical stuff as more like as a best effort. Because really like exchanges in particular are they really I mean do you really want people to to be reinges you know from Genesis or something like they already ingested already reconcile all these data in their back end yeah, that's a good point okay there's also the additional ambiguity of. If someone's sending to a m address and has a memo at the transaction level you shouldn't necessarily assume, that that transaction memo is a specification of a user on the destination side right it could just be

[56:00] Some free text just for the purposes of describing the transaction yeah the point of this event was just to you know show all the information and have the consumers deal with it. But it sounds like we we we don't need to deal with this right. Now also we're out of time the last thing we were going to discuss we we can maybe do this offline is, that we don't we no longer need CAP 64. But I don't know Deo. If you want to say anything about, that yeah I guess the consensus of the discussions has been, that yeah CAP 64 is not needed and yeah this is just an official announcement, that we Are CL on it. Because MOS will be implemented differently yeah, that's it all right. Then we're out of time for today thank thank you for joining and thanks everyone for participating.

</details>
