---
title: "Stellar Developer Meeting"
authors:
  - carsten-jacobsen
  - bastian-koh
  - silence
  - siddharth-suresh
  - garand-tyson
tags: [developer]
---

import YouTube from "@site/src/components/YouTube";

## Part 1

<YouTube ID="izkwfSLc4dQ" />

In this week's meeting, Hoops Finance founders Bastian and Tim talk about the progress they have made, design considerations, and the future of Hoops Finance.

<details>
  <summary>Video Transcript</summary>

[00:00] Welcome everyone this is this week's Stellar Developer Meeting, and this today it's going to be a little bit different schedule than usual we will start with a presentation here from Hoops Finance. And then about half an hour from now we will switch over to Discord where the court team will give a CAP proposal presentation. But yeah welcome, that's Tim you guys are are well known from from our Discord and and I'm I'm super excited about hearing what progress You' have made with hoop Finance. So please take it away thank you so much Carston we are super excited to be presenting in the dev meeting to try to get some eyes on our project and well in our new update we we did the entire UI from the scratch to improve

[01:00] user experience and to make it easier for developers and for retail users in the future to start using our application. So this is our landing page at the moment we are working as well on launching a new one. But the goal here is to make human readable risk analytics for blockchain what's the idea of risk analytics is, that we do think, that liquidity pools are very scatter and they have an untapped potential and I believe Stellar really has, that potential hidden away and we can really unlock it. If we provide we clear and transparent data on these liquidity pools and different offerings right. So let's let's go to the app and see what we have here we just push an update hopefully it doesn't doesn't break there you go. So this is our new UI right. Now we have two main Pages, which is the pools page and the tokens

[02:00] page in the pools page what we're doing is aggregating different automatic market makers from the ecosystem projects, that came from the Stellar Community Fund in this case we have Soros swap Aquarius blend and Phoenix we're still working to add a blend into your UI. But it's working through the API this is something, that should be out quite soon. But through our UI in the pools page you're going to be able to see these rankings right where we are ranking what the best APR pairs we have the best volume and best stablecoins obviously this is data from our indexer. So we got to use it with caution still. But we are working on creating a validation system to ensure, that always the data, that we're showing is accurate and we're trying to do, that with other projects in the ecosystem. So we can always compare and ensure, that everything is is correct. But for example here in the table we can see different liquidity pulls from the

[03:00] ecosystem and important data, that would help retail investors and and really anyone, that wants to participate in DeFi have an idea of how they work and and why would they want to participate in a liquid pool over another for example here we have the native and aqua, which is XLM and aqua pool right we have our volume chart and we have some overview on this like 24 hour volume value locked fees whatnot we we have a proprietary risk score, that we we generate from different data points as well as different analysis from from social media from the internet, that gives us an idea of how risky a pool is there's many factors, that affect the risk score and and we are continuously improving on this. But hopefully this would be essential for us to be able to launch in a nearby future what we want

[04:00] to do smart savings account or or automate automated liquidity management right in in tokens it's very similar we have top tokens by volume right by liquidity and our top stablecoins and we can see information about these and we can go straight to the Explorer to check these tokens. So check the contract of these tokens and information on on them few things here and there, that we got to fix of course. But but basically this dashboard is going to provide the Stellar user and and really any user in the future with a One-Stop shop to save to invest and to analyze their Investments long term, which is really exciting another thing, that we' been working on is actually creating an authentication flow, which I think is very important. So we can get our users logged in and we can provide them with API Keys, which we were just working on right. Now so crossing my

[05:00] fingers everything is good but. If you sign up with Discord for example we use Discord a lot instellar we can create this account and as you can see we get a profile where we can go to our developer account and create an API key, which we're going to call it Stellar or live and. When we create this API key we get it over here we can copy it And then we can start querying our API to get all this information maybe in future projects, that people in the audience is working on right possibly in the seller Community Fund possibly from other projects from other chains right the idea again is, that all this data is transparent and available not just through our dashboards. But also through our API looks a little bit like this it's pretty well documented for you you to start playing around with it

[06:00] at this moment I believe you do not need an API key to play with the API. But but it's it's always good, that you create an account and get your own key early. But basically you have here a playground to try out to get different metrics from different protocols for example Aqua over here is going to give us a response right it's going to tell us the pairs, that are found in the Aquarius protocol right through IDs through their fees LP token supplies and whatnot and within this this is very early. But we are working on our AI endpoint, which is quite exciting, which is how can we talk to these metrics in a like a human would right and have this AI basically transform all this information in something, that is more pable like asking hey what's the best APR pool today and the I tell us just by quering the API and give us

[07:00] that information in in a way, that is understandable obviously this is still on the works and we're making sure, that the information provided is always correct and is always coming directly from our API and, that the AI doesn't hallucinate right. But I think this is really exciting we can see it being implemented in other protocol just to inject on, that hallucination those are end points to be used by large language models and not generated by a large language model absolutely. So so yeah it would be easy for us to to integrate this in just in other protocols or or even in wallets for example right in the Stellar ecosystem there are many wallets. So to have these little assistant, that can ask this metrics very very quickly and just give you an answer, that is understandable I think could create quite a disruptive change. If if. If I May

[08:00] but. But yeah I I'll leave the floor to to Tim to silence you all know him I to explain a little bit more the technical details of what was been worked on and yeah take it away Tim Tim you're mut you're muted oh wonderful, that's good can you hear me. Now right yes fashion was talking a bit about the risk metrics. If you look at the get statistics endpoint you can see like the various actual metrics and weights, that we make the score from I plan to show those on the front end in various ways right. Now it's just basically the aggregated scores. But we offer a bunch of different scoring

[09:00] methods for different pools there's also a way to generate those for tokens right. Now we only offer it for pools publicly. But we plan to add the token statistics soon you mentioned, that the API doesn't require an API key right now, that's correct. However it would require a key. If you were like interacting from a different origin. So like. If you wanted to use it on the client side of your website also Keys would be required for us probably in about a week from now so just a heads up on, that I don't know what else the a the yeah the AI endpoints some of them are basically the same thing as the other end points. But they return the values in a much more Compact and truncated way so, that. If

[10:00] you were using this in a language model, which which got a little bit of experience with, that and you don't want to have a bunch of excess data. If you look at a normal API response object in Json it's got for example the the label of the field on for every every value and you really don't need, that. So like the AI end points are like they're self-describing they they they return a schema schema Jon schema along with the data so, that a large language model is able to know what the data is and pars it it does things like truncates the addresses. Because it doesn't need the whole address most of the time you can change those truncation things with with URL prams different

[11:00] options those options are not yet documented. Because that's, that that system still in development as far as data validation we plan to use simultaneously building like sore band contracts, that allow you to use this data more or less as a controller to make automated trades I don't know. If you showed them the the portfolio page. But basically you would set up an account contract, that you. Then install strategies strategies too those strategies will tell the account. When to make trades. When to move liquidity things like, that. So you set like a base value or a base currency on the account right now, that is only like XLM USDC or euroc. But in theory it could be anything. And then the account's goal is to generate

[12:00] yields. So it earns rewards from AMMs claims and reinvests those and automatically moves your liquidity between different markets based on your the rules, that you set in the the strategy you can also make manual strategies where it's like you tell it exactly what pairs you want to use at what ratios and how off into to claim rewards things like, that. But with, that system it uses some of the data from the API, which is all calculated using transactional data, that's on chain. And so what we plan to do is more or less have a an asset in the protocol, that's in order to stake, that asset you need to run a validation set on the data. And then publish proof of, that to the chain

[13:00] that is still very much in development. So we're trying to find ways to keep as much data off-chain as possible. While still being able to accomplish what we want to with, that system basically it uses on the back end a swap aggregation system, that it Aggregates liquidity pools. But can work with many different types of liquidity pools different different protocols and gives them all the same interface using adapters to allow our account contracts to be able to not necessarily have to specify what pool it wants to interact with it's able to derive it based on yeah information, that's on chain chain. So yeah we're also working on a yeah chat a chatbot

[14:00] system, that will you know help you develop different strategies answer questions about like what what what's a good Market to invest this in you know what Market has the highest highest volatility things like, that yes I'm not sure what else to show you mentioned the O system the O systems pretty cool it's on our back end we have our own we're like a a a SLE provider to this website this is like sort of like client side off using access tokens and refresh tokens. But we've made it in such a way, that it can be integrated using a system very similar to SCP 10 to link your Stellar account to your Hoops account and use. So you you you'll be able to use your your your

[15:00] signer for your savings account contract can be a pass key or a traditional wallet basically. And then you can also like log into you have to create your account using either your email or a social provider. But once you've created your identity on our back end you can. Then link wallets to it and be able to log into our website through through either a pass key, which that pass key ends up being the signer on your account or through your wallet, which could also be Aigner on the same account yeah we we really believe, that pass keys are really essential for moving on in the future just to ensure a better user experience I think as we go along for for the everyday folk just keeping a bunch of wallets it's difficult to to remember those private Keys as well right. So

[16:00] past Keys We believe is the future and and we're looking forward to implement this into Mobile as well with a simplify version of the app. So oh yeah to be clear all the Stellar off right. Now and pass keyo we have disabled. Because we are developing this system against main main net and you know all the data is from mainnet and we don't want people to be risking real money until we've had you know the contracts ready for release to the point where everything's been audited and, that we know it's safe to use with real money right. So we may offer a test net version at some point it's a little bit hard. Because we have to build all the protocols on testnet as well in order to do, that. And then generate traffic through all those those protocols, which for most things it's it's not needed

[17:00] since we have live data to use you can basically simulate most things. So yes. So we leave you with a call to action really to just go to the Discord under projects we have hoops Finance just try it out try to break things ask questions and and let us know what you would like to see in the app what would you think would be best and yeah we're working on making authentication as strong as possible. So we can start keeping track of these users and start helping new projects and new developers to actually use this infrastructure, that we believe could really change and shake things up with within within Stellar, that's really great it's impressive to see how far you've come already I was just wondering U do you have any ideas what the timeline looks like. When when are you planning to go live on mainnet with the protocol

[18:00] I would say this summer. But it depends on. When we are ready for the Audits. And then how long the audits take you know we're we're actively working on it there's parts of it, that may go live on mainnet sooner. So we're trying to build it in stages so, that it can be basically used as we devel each feature however, that's probably going to be limited to like swaps and liquidity deposit with draw initially with some of the automated strategies. Then being added in so. If that makes sense more or less trying to be able to launch what we can, that we know is safe. While still making it so, that the the system will fit together. When it's live. So the the actual token is

[19:00] issuance is probably going to come through you know once the account factories are audited and live. Because it sort of integrates with with the treasury accounts, that control the token token. So so yeah it'll take a little bit. But we are focused right. Now on on getting all this on testet we we kind of have to build a around a little bit to create these new pools and and everything. But but testnet will be out soon I don't want to give dates maybe earlier maybe later. But test net will be out. So so we can start seeing how this works behind the scenes and and to answer twitch I did not connect to Twitch. So I cannot chat. But but yes Hoops wallet or something of the sorts is coming on the road as well I had worked on a wallet myself in the past many many moths ago. So I'm we're

[20:00] probably gonna be retr I imagine, that is going to be something of an app, that allows you to interact with your your account contract easily to be able to do sends withdraw you know standard wallet stuff even, though it is a a Sor band contract. So some way, that it could allow you to use funds from, that to interact with other parts of Stellar besides Soo yeah. When we think about Mobile Wallet I ask you to think about pass keys I'll just leave it at, that it sounds great really at the end of the day your wallet like. If if you you have your pass key issued from your phone. Then then your phone really is the wallet. So it's quite easy to make the system work multiple devices just. Because of all the work, that's gone into passwordless off already by. So many other

[21:00] people exactly and it just goes hands in hands with the idea of smart savings accounts accounts right okay just just quick TR oh. So the the the UI looks amazing and and just what we've seen here this last few minutes is looking very promising. And then you have the API, that's, that's really well documented. So who do you think is going to how is this going to be used who's is it more going to be used with the web interface or is it more like a developer solution or both what are your ideas on what thoughts about, that I think there'll be a lot of end users. But I think, that there will also be Enterprise customers I know we've talked to certain other projects in the ecosystem, that have been interested in using some of like the the

[22:00] scoring data like inside their apps for for like profiles about projects and I figured we need all this data in order to do the stuff we want to do with the automated accounts we might as well make it easy for other people to build on top of it as well. And so I imagine also there's a a good business opportunity in the the normalized data, that we provide provide. So yeah it's kind of both developer side it. But I'd say most people would would know it through the web interface developers right we want to encourage them to use the API and and just people, that have been in the default default Stellar for for a. While to use this tool

[23:00] and as we move along and and just more people is being on board in Stellar every day right to just have the basis for this becoming a retail application right. So to to have the everyone to really be able to use it and and get to this ideal of a universal savings account, which can be put as a retirement plan right, which I believe is is something, that that needs to change in the coming yeah our goals make an account, that it's a yield bearing account, that's permissionless basically, that anybody can have and it it you know they they manage it. But at the same time our our our application helps right. So you could build on top of it and make make your own interfaces I think, that's good for everyone, that's what absolutely

[24:00] thank you we are unfortunately running out of time it was really great to see all the progress here some of your thoughts about about the the future of the project and what you're working on and I'll encourage everyone to go go take a look at it as as you both said go find some errors go test it out yeah poke it yeah I would probably not use the data in anything live right. Now just the heads up there there's no way we can promise every. Because there's the audits there haven't been any audits done yet. So yeah and it's under active development very active like we we merge to main quite often often and every time, that's, that potential breaks. So many sleepless nights for sure thank you so much Carson I think it could be fun to do an update in a few months and

[25:00] see where you are. Because this this looks very promising yeah we're moving pretty fast I think we'll have plenty of updates okay great and thank you for everyone who joined right. Now we're going to switch over to Discord where the court team is going to to give a presentation about some of the the proposals they are launching related to Protocol 23. So see you over there thank you and join again next Thursday bye bye-bye

</details>

## Part 2

<YouTube ID="JDlIL5y5bn8" />

Recording from a protocol meeting where two Core Advancement Proposals were discussed.

### Resources

- [CAP-0062](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0062.md) — [Discussion](https://github.com/stellar/stellar-protocol/discussions/1575)
- [CAP-0066](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0066.md) — [Discussion](https://github.com/stellar/stellar-protocol/discussions/1585)

<details>
  <summary>Video Transcript</summary>

[00:00] The first is partial stay archival CAP 62. And then in memory Soroban state for CAP 66 and no Tom I will never stop using in Centric Linux distro cool. So I guess before I get started guess a little bit of background. So Protocol 23 is kind of where the rubber is going to start hitting the road as far as state archival is concerned. So kind of some you know background to you. When soron launched we of course had the interface for state archival with rent and all those sort of stuff with the intention, that eventually entries, that have run out of rent will be archived. And then removed from validators in order to free up space. So you don't have you know the issues, that come with large amounts of state, that have to be maintained and so, that's kind of where we're going. Now today the interface is such, that you still pay rent you still have to issue restores and all the sort of things things. But

[01:00] the data is not actually yet removed from from validators and so, that's where we're going. So initially the plan was for Protocol 23 was to have what we're calling full State archival and in full State archival what happens is entries once they have run out of rent be removed from the live State. And then they are added to this temporary data store called the hot archive is still maintained by all validators. But it's just a separate database, that just maintains entries, that have been recently archived the thinking being was, that eventually this hot archive would become full and. When the hot archive is full what you would do is you would create a Merkle tree of, that data validators would maintain the Merkel rout. And then delete all the information in the hot archive. And then you just repeat this process iteratively. So essentially you evict entries from the live state to the hot archive state

[02:00] eventually, that hot archive cach will become full. And then you'll actually delete and remove those entries from the validator. And then the restoration process once an entry has been archived in this way. If it no longer lives in the validator there's like a Merkel style proving scheme to, which you are able to restore an entry back to the live ler state so, that's kind of what we consider full state archival, which is where entries actually get deleted from validators. But thinking about this problem a little bit more and looking at the current metrics of soron it seems, that we're still a little too early for this full C archival I think long term. If you look at smart contract platforms, that have large amounts of State there are significant issues with maintaining all, that state you have lots of you know Hardware requirements on a network like salana to maintain large amounts of cashes. And then you have a networks like Ethereum, that don't have large hard requirements. But they're very slow just due to maintaining these very large

[03:00] databases. So long term at scale I think it's still very important to have the full State archival solution where entries are deleted from validators. And then restored VI proving schemes. But the reality of the situation is we're not quite there yet and I think, that currently there is less than a gigabyte of soron state currently live Stellar. And so going through all these hoops and adding all this complexity for this proving scheme just to delete a small amount of data isn't really worthwhile at this point and so, that's why for Protocol 23 instead of going the full cival route where we actually delete entries we are doing something called or I'm proposing something called partial SE archival and this is what CAP 62 kind of explains. And so in partial State archival what we do is we do kind of the first half of the full state archival. So we still maintain two different databases on the validator you have the live bucket list, which contains

[04:00] all of your live state, which is the The Ledger, that exist today. And then you still have a what's called the hot archive, which is a cache of recently archived entries. And so what you would do is, that whenever an entry runs out of rent it would be evicted and removed from the live State and added to this hot archive database. Now the key distinction here with partial stay archival is even, though you still remove things from the live bucket list and add them to to the hot archive you never actually delete the hot archive never becomes full such, that the entries are never actually removed from validators. And so it's partial St archival. Because you are still kind of storing live state in one database and storing archive state in a different database. But you're not actually removing any state from validators. And so I think my current proposal would be in Protocol 23 to implement the partial state archival with the intention of later on in the

[05:00] future extending this to the full State archival solution the only reason is is, that I think, that for the you can the the size at, which the the hot archive becomes full and becomes deleted is configurable. And so I think we could do something reasonable such as we could Implement on this St archival solution. But we could set the the capacity of the hot archive to something very high like 50 GB such, that it would take a long time and a lot of network activity to actually start deleting State. And then you know. If the network was to grow, that much such, that we had 50 gigabytes of archived sorb on state. Then it would actually make more sense to start deleting State and requiring proofs for all the operational benefit, that we get there and so, that's kind of the current proposal of 62 is we're going to still maintain all information on the validators. But we're just going to move archived information from one database

[06:00] to a different database. Now the reason, that we want to do this this separation of archive State and live state is, that it actually opens up a large number of optimizations and so, that's what we get into in CAP 66, which is inmemory Soroban State. And so we can. Because we have the system where the live State or the live bucket list holds all classic information and all live soron information and we have a completely separate database, that stores all the archive State now. Because of the rent system and. Because of the the way, that we do write fees you know where a write fee is a function of the total size of the bucket list we actually have a way to put a soft limit on the amount of live soron State at all times the reason being is, that you know with the the current bucketless size. If you were to add enough life State such, that

[07:00] you go beyond the target bucka size rights become very very expensive such, that the network users are incentivized to allow entries to run out of rent and become archived and so. Because of the way our fee system works we have a way to essentially have a soft limit on the amount of State in the light bucket list at all times. And so what I'm proposing is to change, that fee slightly such, that instead of the Soroban right fee corresponding to the size of the entire live leer of both classic and Soroban entries it only applies to the life soron entries. And so essentially the the buckus target size instead of being a buckus size would change to just be the the life soron Target size and I think this is much more fair given, that classic entries don't actually have to pay rent yet. And so it's a little unfair, that adding classic entries actually changes the WR fees and the rent fees for Soroban and especially as the network exists

[08:00] today classic State dominates sorant State size. And so changes in sorant usage don't actually really affect soron right fees rather changes in classic usage affect soron right fees. And so by changing the The Bucket List Target size to a Soroban state instead of just all total State we have a much more fair fee system. But what, that also allows us to do is to prioritize live sorup on state above arive state. So what do I mean by, that so. If we change the the way, that we calculate fees to only look at sbon size we can use the fee system to enforce a maximum amount of Life State at any time. So for instance we could set the target soron State size to 1 Gigabyte. And then the fee system would ensure, that there's not much more than 1 gigabyte of Life Sor on state at a given point. Now you could maybe you know go a little bit above, that. If people are willing to pay expensive fees

[09:00] but the way, that the fee growth works is, that you know you are reasonably capped to a small amount of state and so. Because we have the system where the amount of live sorb on state at any given time is fixed what we can actually do is just store all Soroban state in memory and not have disk access at all and so, that's the current proposal in CAP 66 is to prioritize all live soron state in memory and this is made possible. Because we store live soron state in one database and archived state in a different database. And so by splitting the state into two separate databases we can very easily just iterate over the the live database and store all, that sorab on state in memory and so, that's kind of what's Happening behind the scenes as to what the the validator is doing. Now we're able to do this. Because of the maximum soron State size. If we didn't have this and. If

[10:00] Soroban LIF state was able to grow unboundedly this would be a very dangerous optimization. Because validators might run out of ram but. Because of the St kival system we can actually fix the amount of Life State. And so there's no runaway R risk. And so we can very reasonably store all soron state in memory. And so there are some changes we need to make to the developer experience and the user experience to make this possible. So first we are going to to change some of the resource types A little bit. So currently today we only have one read resource, which is read bites and read entries and this assumes, that all the information you're reading is on disk. And so what we're going to do or what CAP 66 proposes is to split this into two different resource types. So they're going to be an explicit inmemory read resource. And then an explicit on disk read resource. Now the reason we're doing this is, that even, though all Soroban state is or all life soron state is held

[11:00] in memory Soroban contracts can still access classic State and classic State needs to be on disk now. Because classic entries aren't subject to State archival they have the runaway Ram risk. And so we can't store classic entries in memory. And so Soroban contracts will still have to pay disk fees for for classic State, that's exist additionally we're only storing live state in memory and so. If you access archive state for example a restore operation. Then you would still have to do dis reads. And so there's a dis vew for, that. But essentially what would be changed is, that you would there would be a network limit for the maximum number of on disk read entries as well as the maximum number of inmemory read entries now, that being said. Because the inmemory reads are a lot cheaper than the on disk reads we can actually pass aot, that savings down to the user. So in this proposal there would actually be no in memory

[12:00] read bites limit. So essentially the read limit for Life s on state would just completely go away. Because in memory reads are cheap. And so there's no reason to limit, that. Now we would still limit the total number of entries being read. But the bytes being read would not be limited additionally. Because we're not doing dis access there would no longer be a read fee associated with accessing Sor on State and so. Because you you still have to pay like a instruction like CPU count and things like, that to actually process large amounts of data but. Because we're not going to disk there doesn't need to be an explicit fee or resource for, that. And so essentially for live SW on state you don't have to pay for reads and you can read as many bytes as you want you still have to pay for the CPU, though. So it's still an implicit fee. But there's no explicit read fee and so, that's kind of the First Advantage to the inmemory versus on disk resource. Now the second thing this allows us to do is is also

[13:00] Implement autor restore functionality. And so previously. When we first launched Soroban we weren't sure what the final State archival proof system was going to look like and so. While from a technological or from a technical standpoint there was no reason to require a separate restore operation and a separate invoke host function operation we did, that just to give us flexibility later on in case the proof system turned out to be very involved. But in CAP 57 we've actually outlined a pretty lightweight proof system, that works with invoke host function. And so what we're going to do in CAP 66 is we're going to allow automatic restore. And so what this means is, that you no longer will have to issue a restore operation prior to your invo host function. But actually your invo host function operation will just automatically restore any archive keys, that are in the footprint. And so this you know reduces the transaction

[14:00] count required reduces fees and should just offer a much better user experience. Now the the way this works in resources, though is, that like I mentioned before the live Soroban state is all cached in memory in one database and archive state is uncached and on dis in a separate database and so. If you call info Coast function and every entry you're using is currently live. Then you would have the free inmemory resource bites and you wouldn't have to pay for dis, that being said. If you're using automatic restore the entries being restored would come out of the disk read bytes and would be charged disk fees. Because again for the the entries, that are archived and live in the hot archive database those do have to be read off a disk. And so I think, that's kind of a kind of at the high level of what we're proposing kind of you know the tldr except been talking for for a

[15:00] little bit is, that you know C the archived entries live in their own database and live sbond State lives in the separate dat or a live database we are. Then going to Cache all the sbond state in the live database in order to pass, that savings on to you there will be an inmemory rebite limit and an on disk rebite limit in fee. And then finally there will be automatic restore to you know essentially remove the need for the restore operation in most cases. So I guess are there there any questions or any conversation points we'd like to touch on more looks like there's a question in in the this in the chat box. But I think a lot of dApps and extend TTL by default will, that still be necessary necessary ah yeah. So I think. So just. Because we

[16:00] have automatic restore doesn't mean, that you don't want to still manage your TTL. And so like I managed before. If all the entries, that you're using are currently live. Then what're or. Then you don't have to pay read fees and you have much larger read limits. And so you are still incentivized to pay rent also. So but the issue is. When you restore. something you have to pay right fees for the restoration and you also have to pay discre fees for the restoration. And so I think from a fees perspective. If you're using an entry a lot it's still in your best interest to extend the TTL to save money as you know even BEC just. Because the restore is automatic do not make mean it's free. And so you still have to pay for, that restore and even. If it's the same invo Coast function invoking a

[17:00] function, that only accesses live state is significantly less expensive than invoking a host function, that has a automatic restore on the front end of, that. So we still definitely want to extent extent TL yeah let's see oh. So for OrbitLens will it be possible to tell in advance whether the entry will be automatically restored during the simulation yes. And so this is kind of more of the implement details, which are included in the CAP. But what we're doing is captive core has recently added a couple of HTTP n points for querying Ledger State, that will be used by RPC in order to simulate transactions correctly. And so essentially this endpoint is a high performance you know multi-thread HTTP endpoint, that has a similar performance to a SQL table queries. And so it should be appropriate for for production use cases and what this

[18:00] endpoint will do is it's a key value search where for every key you provide it it will tell you. If that key exists and then. If it exists it'll give you the value. And then it will also give you meta information about, that key. And so it will give you the Ledger entry it will tell you. If it's live or archived. And then it will also tell you you know what its current TTL value is and. If it's in memory or on disk. And so the captive core endpoint is kind of Ed to be the new kind of entry point for this information. And so you should be able to query the current archival state and the current in memory versus on dis state of any entry directly via captive core again there's also meta, that we're emitting for all these events so. If you wanted to it's theoretically possible to injust meta and maintain the state of Soroban entries, that way but. If you don't want to do, that and create your own SQL table and Pipeline and pipeline you can just use the the captive core htpn

[19:00] points. So will automatic restore become automatically available for existing contracts yes. So the this is all handled at the RPC level. And so the essentially what's changing is, that with Protocol 23 and this is detailed in CAP 66 specifically is, that we are changing the footprint to have this field where you distinguish in the footprint. If a sorond key is either in memory or on disk. And so essentially what the validators will do is, that for whenever they receive and apply an info Coast function they will look at the footprint and for every Soroban entry, that is marked as being on disk AKA marked as being Arch before running, that transaction

[20:00] the validator will essentially restore those entries automatically. And so the actual contract and the contract logic will not change, which means, that all deployed contracts are automatically compatible with this. Now the invocations to those contracts will change slightly. Because of the footprint changes. But again this will all be handled by RPC. And so pre-flight will will do all this automatically let's see other questions oh. So ler streaming mode I'm I'm not sure about the context. But but behind enabling or disallowing metast streams on validators versus captive core instances I imagine it has to do with performance reasons where you don't want ingestion to make a

[21:00] validator fall out of sync and essentially, that config setting is an opinionated way of saying, that validator should be high performance and never get blocked whereas like a watcher node, that's not participating in validation would be more appropriate for observing and adjusting the meta. Because there's not it doesn't depend on a downstream system where. If the meta stream gets clogged. Because the downstream system isn't adjusting fast enough you wouldn't want to lose sync and have a validating node fall off the network. Because of a a downst stream issue cool. So I guess

[22:00] I'll from George about the CAP mentioned somewhere, that autor restore won't always be possible can you elaborate on these scenarios ah yes okay thank you for pointing this out. So there are a couple of edge cases where an invocation will still require an explicit R operation sorry. And so essentially. Because the inmemory reads are. So much cheaper they don't have limits like the on desk read the on just do. While there there is no read byit limit at all and. While there is an entry read limit the expectation is, that this limit will be significantly higher than the dis limits. And so just for you know example suppose, that a in Protocol 23 the transaction in memory read limit is 40

[23:00] entries and the on disk read limit is 20 entries. And so say you have like this DEX you know trade, that will access 40 soron entries now. If all of those entries are Al. Then you know the it's within the limits the invocation Works no problem. But say, that all 40 of those entries are archived. Now even, though the inmemory limits are large enough for, that transaction to succeed you can only the the automatic Restorations will come from the on disk limits and so. Because you have to pay disk fees and are subject to the dis limits for the restore operation you can only restore in this example 20 entries automatically even, though you need to have authority to be live to complete this DEX trate operation. And so in this scenario you would need to to still manually submit a restore

[24:00] operation just. Because the way, that the limits are set you can't fit, that many restores in a single transaction now, that being said especially given some other exciting work, that's happening in 23 we expect to raise limits pretty significantly across the board. And so I suspect, that this Edge case will not affect most transactions it will only affect very expensive transactions, that are doing stuff. And so for instance. If you have a DEX trade and it's trading assets, that are mostly live you won't be affected really you're only going to be affected. If you have like a DEX trade, that's crossing a ton of orders and for some reason all those orders were archived. So you mentioned, that the restore op could be deprecated

[25:00] because of the automatic restore. But this Ed Case requires you to keep something something like, that around right yeah. So I think I mentioned the CAP, that we met deprecate the restore op and, that's just. Because that. If the footprint is automatically restored. Then having both the restore op and the extend TTL op is kind of redundant. Because for instance say, that you just want to restore something you don't actually need two operation types you could just essentially use the extend TTL put all the keys you want to restore in the footprint. And then just set the TTL extension to zero and this is functionally equivalent to the restore up and so. When I mentioned deprecating the restore op I don't mean deprecating the ability to restore transaction or to restore entries via an explicit transaction. But just mean like you know mechanically do we need both the restore op and the extend TTL up could. Now you know in theory at least both do a

[26:00] restoration as well as extend okay yeah, that makes sense Nico had a question about how the Soroban state size is initialized at upgrade time it's it's not specified in the CAP yeah I think I need to expand on this a little bit more. So I think part of this CAP is, that we are changing the semantic meaning of a network config setting. So so in particular The Bucket List Target size will become the Soroban state size. Now the issue is currently the bucket list is like 11 or 12 gigabytes. And so we all of our network settings are assuming, that the your target size is like 13 gigs. But now the issue is. If we you know do a protocol upgrade protocol upgrades previously have never actually changed config settings and so. If you just do the protocol upgrade all of a sudden instead of your Baseline for fees being 12 gigs with a target for

[27:00] 13 gigs. Because we're only tracking Soroban State your target is still 13 gigs. But now your Baseline is like 400 megabytes. Because there's like a lot less soron State compared to life State. And then you have this dos attack where until you upgrade the network confix settings you essentially have no read or write fees for both in memory and on dis State. And so you could have like a Doss attack where someone writes like tons and tons of temp entries and like you know spams The Ledger for essentially zero fees. And so I think what I'm proposing is, that you know currently there's like an operational lag between upgrades. Because core validators can only cue one upgrade at a time. And so we'd have to get all of tier one to arm for the Protocol 23 upgrade. And then after, that goes through have them all arm for the network config setting upgrade and in between, that time you have free free reads and free wrs, which is a huge security risk. And so what I'm proposing is, that. Because Protocol 23 is semantically changing

[28:00] what this config setting means the protocol upgrade itself should also change the value. And so you know this is slightly different implementation wise than what we've done previously. But I think it should be relatively straightforward implementation whereas like the Protocol 23 upgrade both semantically changes what the Buist Target size means as well as it resets it to a initial starting value, that's more reasonable given this new interpretation of the data Okay. So we've actually updated settings on protocol upgrades before, that I think we we know, that works cool okay great let's see a couple other questions okay. So for OrbitLens the storage for the hot archive yes. So the hot archive and the live Buck list are both part of ensus. So we need the

[29:00] hash of, that state. And so for, that reason both of the the live database and the hot archive database are both bucket list DB implementations and, that's just. Because they we have to meriz those structures. Then Buck list DB is pretty fast these days. Now with respect to offering tables to buckless DB we we don't really have any plans to do, that and the reason is it's a very difficult structure to add tables to. So it's a log structured merge tree, which is kind of a a variant of like database used by like rock CB or level DB and it's also completely made inhouse like we didn't Fork rocks or anything like, that. And so kind of we we have it it works very well for query types, that the valers require and it's very efficient at those. But we have to essentially like hand write C++ optimized code for those specific queries and. And so it would be both a

[30:00] very significant undertaking to allow like you know arbitrary index types for Downstream and it would also probably not be a very efficient database just. Because it's a lock structured merge treat. And so a SQL style index query would not work very well on it. And so I think what I'd like to do with this is you know we've we've for arbitrary key value lookups we have exposed end points, that are on the same scale as SQL queres. But again they're just raw key value stores they're not like you know indexes or you know really tables and I think there's been a lot of work done by the platform folks on like the the CDP and things like, that. And so I think given, that the complexity of the database of Stellar core is increasing a lot and for a variety of reasons we only support Buist DB. Now and no longer support SQL I think, that any sort of raw database

[31:00] access needs to move more in the direction of utilizing Downstream utilizing met ingestion using CDP and not rely on direct access to course databases just. Because you know nowadays with buck DB the core database is very specialized and is not suitable for generic queries cool I think there's a couple people typing. So I'll let them finish or. If anyone else has any other questions. If not I have a third CAP, that I'd like to introduce I'll give it a second. And then we can

[32:00] move on all right I feel like, that's we've had enough time oh answer about slp1 dial would you mind linking, that question again I'm not quite sure what the slp1 question is oh yeah the new limits sorry yeah cool. So I guess. Now I'd like to move on to CAP 65 the reusable module cache. And so like I mentioned before we were doing all this optimization stuff for a memory State and essentially in addition to saving all the contract data in memory

[33:00] we can also save all the contract code and by extension all the contract modules in memory. Because you know we have a way of EX of archiving contract instances and contract code, that hasn't paid rep recently. And so with, that I think gr's on the call. If you want come up and talk about CAP 65 I don't think gr's on the call I believe we were going to speak about CFE 65 next week right oh sorry I guess I got a I gave youall a little teaser for next week. Then my apologies got a littlee of the gun. But so yeah. So

[34:00] I I don't want to steal grain thunder. So I'll just you know leave you with a teaser, that we can you know have lots of this not only helps optimize the the read limits. But also optimize CPU utilization as well. But we'll talk about, that more later all right unless there are any other questions questions we can conclude this meeting thanks great Garen it was a great talk all right thank you and you know the dis. If youall have any more questions or concerns you know there's a couple of discussion tabs on the CAPs or just ping me on Discord

</details>
