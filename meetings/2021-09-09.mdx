---
title: "CAP-21: Generalized Transaction Preconditions for Payment Channels"
description: "This discussion explores CAP-21, a core protocol proposal that introduces generalized transaction preconditions to enable secure, high-throughput payment channels on Stellar. The session focuses on design tradeoffs, backward compatibility, and transaction ordering behavior required to safely support off-chain payment workflows."
authors:
  - david-mazieres
  - jonathan-jove
  - justin-rice
  - leigh-mcculloch
  - nicolas-barry
  - siddharth-suresh
  - tomer-weller
tags: [legacy, CAP-21, CAP-40]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="yz63i52W2Ek" />

This protocol discussion centers on CAP-21 and how generalized transaction preconditions make payment channels practical on Stellar. Participants examine how features like relative sequence constraints and time-based conditions allow parties to transact off-chain while preserving on-chain safety and eventual settlement guarantees.

Much of the conversation digs into implementation realities: how new account-entry extensions interact with historical ledger states, how validators should forward and prioritize conditional transactions, and how to avoid subtle “foot guns” for developers building advanced transaction flows. The group also reviews how CAP-21 underpins related work and what changes are still needed before final acceptance.

### Key Topics

- How generalized transaction preconditions (such as `minSeqAge` and `minSeqLedgerGap`) enable scalable payment channels.
- Tradeoffs between protocol-version changes versus opt-in account flags, and why flags were rejected.
- Implications of new account-entry extensions on legacy tests and ledger invariants.
- Keeping time bounds unsigned for consistency with existing transactions and SDK behavior.
- Validator behavior for forwarding, queueing, and prioritizing transactions with sequence constraints.
- Interactions between fee bidding, transaction ordering, and denial-of-service considerations.
- Design questions around combining delay-based and sequence-jump preconditions in a single transaction.

### Outcomes

- Agreement to keep new account-entry behavior protocol-wide rather than opt-in, despite test refactoring costs.
- Decision to retain unsigned timepoints for transaction preconditions.
- Consensus to adjust transaction-forwarding rules toward simpler, first-seen handling to reduce abuse.
- Action items identified to revise CAP-21 text and continue async review before acceptance.

### Resources

- [CAP-21 – Generalized Transaction Preconditions](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md)
- [CAP-40 – Protocol-level support built on CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md)

<details>
  <summary>Video Transcript</summary>

[00:00] Right now. Hello everyone and welcome to another Stellar Open Protocol Discussion. As per usual in these meetings, we discuss potential changes to the Stellar protocol that take the form of Core Advancement Proposal or CAPs. These are technical specs. They suggest changes to the Stellar protocol that allow the Stellar protocol to add new features and evolve to meet the needs of the ecosystem. We're live streaming them so that anyone who's out there can follow along. But again, I do want to point out it's technical. So if you are watching this, you should probably look at [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md) in order to understand what we're talking about. That is what we're going to be talking about today, [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md). And you should also join the Stellar dev mailing list where offline discussions about these changes take place.

[01:00] Also there it we do keep an eye on the. discussion box. So if you put comments or questions in there, they do help inform our decisions going forward. We may not actually address them in this meeting, although if they're super germaine, I may bring them up. Today we are focusing specifically, as I said, on [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md). [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md), basically, is a CAP that lays the groundwork for building payment channels on Stellar. So what we're talking about today sounds a little obscure. Keep that in mind. Payment channels are things that allow multiple parties to securely transact off-chain and periodically settle on chain and, among other things, that make it easier to build high volume use cases on Stellar. So changes to [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md) are proposed by [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md). They allow for payment channels. Last time we also discussed [CAP-40](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md) and basically got it to a point where it was near ready to be accepted. But [CAP-40](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0040.md) is contingent on [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md). And so here's where we are with [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md). There are a few outstanding issues and questions that we're going to try to walk through today. I think that we are actually quite close to getting this accepted, although

[02:00] We'll see what the outcome of today's conversation is. So, with that in mind, let's just kick it off. [CAP-21](https://github.com/stellar/stellar-protocol/blob/master/core/cap-0021.md) is what we were talking about today and I believe that. you know. The question is, I think that there are a couple of questions that came up on the mailing list and there was a question about transactions failing during execution that came up during the last meeting. But to start with, I think we should sort of try to deal with the outstanding questions that are on the mailing list and Nico, if you can just share the first of those questions with us so that we can start to discuss it. I was looking, yeah, I didn't have those questions open, hold on, sorry. So, yeah, I guess like the first question was more of a minor

[03:00] Thing, that I minor I didn't, we didn't discuss, but that maybe is something we should be at least putting as like a potential thing. That is, should that behavior that we have for tracking the modified time, basically for the sequence number, this be an opt in right now, it's kind of making this blanket change right, like as soon as protocol whatever, as soon as CAP basically becomes active, we are going to kind of, yeah, make like track this new, less modified, basically in each account. I mean it seems like from an implementation. I mean the reason. I mentioned that as a flag right, instead of being a like a

[04:00] Protocol base right, like in terms of the code. It's actually the same thing. Like you have to in the code to deal with all protocol version, new protocol version. So instead of checking against protocol version, we could check against the flag. The difference is that it would make it that the tests for sure are going to be much simpler, because now we can actually just test that part, like in a solution, fairly easily, whereas I know that lee in the past raised the issue, and I think that's true- that we would have to rework a bunch of tests, given the way they are written today in core at least, I don't know in outside. I mean it seems like a pretty big foot gun to have this flag right, because now this is something that's going to like catastrophically break all kinds of, you know, smart contracts and stuff. If, like, this flag ever gets cleared or if, like, people don't realize that it's not set. So it would be, there would have to be like a huge advantage

[05:00] To having these like two kinds of accounts: one that just blanket failed preconditions and ones that actually implement the preconditions. No, it would not. So the preconditions would not fail. It's the last modified. Yeah, would not be enabled. Yeah, so I don't know if it's, yeah, like preconditions. I think that the CAP says that if the you know, if you take it there, it's treated as zero. Yeah, it's exactly, it would be zero. So I don't know. I think if you added the flag, we could. Actually, we don't need to have a default at all. We just say that a transaction is invalid if it tries to use that feature and the account doesn't have it enabled. And for any account to enable the feature, the new stack, that's a right now you have to like constantly check that when you're engaging in a protocol, that looks like it's okay, that account actually has this flag set.

[06:00] So I'm really I don't understand what benefit could possibly outweigh, like I was saying, it's a faster implementation. So you know if we, if you know if no, I just wanted to mention because I know that was raised before. And by looking at the first kind of round of questions on that lee had, on that prototype, I mean it looks like it's a legitimate concern. I mean, so maybe can you explain a little bit better, like what the complication is of having it. It's around the setup. I think of test code. Like a lot of times we're creating accounts, we're doing all those things and I think those accounts are created without accounting entry extensions. Well, with the- yeah, with the default change, like that they would be created with the this extension and then when we do the, so

[07:00] Basically we create like a snapshot of the ledger right and then we run a bunch of tests for different protocol versions and here the protocol version is obviously in. Like for older protocol versions, this would be invalid, an invalid ledger state. So I think there's a bunch of refactoring that needs to be done. I mean they've been involved ledger state. Well, an account with such an extension is not valid as of today, right? So I mean what happened when you upgraded to account entry extension v2. Why is this any different from the v2 v1 to v2? Beyond, v2 was an opt in right, like it was only when you were using. I mean from everyone to each other liabilities, right? So that's what one number sponsored and number sponsoring like sponsors. Yeah, otherwise we didn't change it, we would not and touch it. Clarify something: here is the

[08:00] Way that the proposal is currently written. I think what I'm understanding is that when an account is created, it'll automatically have an accounting entry extension v3. At that point there's no way otherwise, right, and yeah. So basically, like, the reason that this will be confusing, David, is that, like there's like old historical tests, which is a lot of the tests, like many thousands, possibly tens of thousands of tests were written such a way that, like, the setup was done in the current protocol version and then you, like time, travel backwards in time and do the tests in the appropriate protocol version. So we'd have to go and fix that. Yeah, so you're saying that right now, if I create, you're saying that right now, if I create a new account, I don't get an account entry extension v2, correct, you just have null extension. Now maybe there's an argument that we should go and fix our tests anyway. Yeah, I mean, that's why I was saying like,

[09:00] Yeah, I mean, that's why I was saying like it's a trade off right. Like: can I write you like an XDR write you? will automatically, I mean converter. That will automatically. I mean, it seems like we could. I mean, do the tests have like specific, like shot 256 hash values of ledger state? Is that the problem? Well, they can't, though, because there's other values like the ledger number and stuff, that aren't there. So what specifically is it that fails in the test? Like, maybe there's just like an XDR, there's like some template magic, invalid XDR at as of currently. So, if I just write a, an unmarching function that will unmarshal a new account entry to an old one by stripping off the entry extensions v2 and three, then we're good, right, all we need is that one function? No, not really. Like I mean yes, you could in.

[10:00] Like I mean yes, you could in principle do that, but like in practice what's happening is like we have these like invariants that are written right, like they check that, like nothing is broken things, that things only exist if they should exist in effect, like no negative balances, blah. And in practice, like you could imagine, the sequence of operations is like you're in protocol version 19, let's say, or 20 or whatever. Whenever we get this done, you create an account. It has account entry extension v3. You travel back in time, you go and do some other operation. You know you're not doing like raw XDR operations, it's like literally a seller operation. So let's call it like a payment. It goes and does this payment. It goes, it loads up this thing. It loads it happily because it knows how to load it. It's valid XDR in the terms of like what the xtr is, sorry, but it is marshalled or it's unmarshalled. You're loading up the bytes, so you're loading. You're just have a simple structure floating around. You're loading up the literal bytes but like this is happening like deep inside of cell record, like not in the test, it's like you know, in the actual production code.

[11:00] Then you like, load this thing up. It's happy because it's good. You go through, you do the operation, everything is fine. And then you hit the invariant and the invariant's like oh no, you're dead because you have this thing that shouldn't have been there. The right solution is just to fix the test so that they do the setup in the right version. Do you realize you guys are like massively making the case that I was making several weeks ago, that like we shouldn't be doing our extensions this way, right, because like, if there's any benefit to doing this horrible cascading nested thing that's wasteful of bites, wasteful of like program or keystrokes and wasteful of your right hand margin, it should be that, like this backwards compatibility stuff isn't an issue, but like, no, but David, this is not, this doesn't matter. Like this is invalid. This is an invalid byte configuration in existing critical version. That's what John just said. It's a. We have invariants that check that you don't have garbage in the ledger, right, that's garbage. Like, from today's point of view, it's garbage. Right, it's not bad. But I'm saying the extension: it has

[12:00] Nothing to do with how extensions are set up. Right, but which should I mean? Shouldn't that be one of the benefits of doing these cascaded xdrs? Is this backwards compatibility with sources not by quad compatible? That's the point. Like it, this is garbage from today's protocol. I'm just saying like this is we have to rewrite the test for this. But we would also have had to rewrite the tests if we'd done it the original way that I did the extension. So I don't. I don't see what we've won by this latest change that I've made, because a lot of things have been written wrong in the first place. But the production code doesn't have to change at all. The production code all just works exactly the same. That's the thing. Let's see. Well, whatever, okay, so do we have to write about yourself? The issue here is just tests, right, like the in the production code. The way that it works as proposed, is good. It prevents there from being like a foot gun, but it requires us to rewrite a bunch of tests. Is that correct?

[13:00] Yeah, I mean like the. So the reason going: back to the question I was asking right, the reason I was asking the question. was to see how bad would it be if we were making this- a flag, right? That's the question. Yeah, and David, it seems like it's about: yeah, it would be really bad, so that means we need to go fix the test. I mean that's kind of what you know. Okay, cool, fixing test is easy, it just takes time. Like, we just need exactly. It's going to be a bit of a time sync, but that's okay. I mean, if we're saying it's basically a non starter to have a flag, it takes either it takes time or it takes XDR trickery, right. So it may be like, oh no, the XDR trickery is going to be like, it's going to be validation protocol. That's why we can't do that. So let's, but let's. Yeah, that's fine, let's take it offline regardless. It's test hackery, right, yeah? So, yeah, I wanna just exactly one thing about like, if we do, if we were to introduce a flag, I think it would have an impact on the usability

[14:00] Of this stuff as well, because, like you're saying, like we would never be able to just create a transaction for an existing account that hadn't enabled the flag, which means that if you have an account that's just sitting there on the network and you want to interact with it in this way and with some new contract that maybe doesn't require interactive setup initially, you can't just go and use these features of the network, which is sort of inconvenient, like. So that doesn't won't affect the payment channel protocol that we're hoping to use this for right now, but it might affect some other future thing. Yeah, and it's worse than that because we don't have spv, so like there's no concise proof that an account actually has this flag. So it means that the receiver side would like synchronously need to like query some trusted Horizon instance in order to like participate in what should be an offline protocol. So okay, so flags fix the tests. Yeah, it's bummer for the test, but

[15:00] Okay, what was the second question time point? This one should be a lot easier. Yeah, the unsigned thing. So my question was: like I don't remember what happened there, but like making that change is there is actually not a lot of like? The mention in the backwards compat section is actually a bit misleading. It says that you know nobody cares. I'm actually not true. I'm not so sure. I'm pretty sure we do have in historical data transactions with large max time. And the other thing too is that to this code, regardless, you know core Horizon SDKs, they have to handle those large numbers. So in a way, like making that change just makes everything more complicated. So I'm not sure it's actually delighted to leave it unsigned. I originally had it unsigned. And then I feel like people objected to that. I mean all I

[16:00] People objected to that. I mean all I want is like I think all our time points should be like a type def, right. So it's like the only thing worse. I mean I think either signed or unsigned is fine. It's just it's bad to have like a mix of the two. Right, like we should just be consistent about sign and unsign, because when you start comparing signed and unsigned numbers, as we all know, like you know, bad things start happening. So we already have, a mix, claymore balances, you signed time down to use unsigned. I mean we're there. So, oh yeah, okay, so I guess, out of luck on this one. So I mean I so I really don't care, I just want to be the same as other things. So I guess I'll keep it unsigned And you know we can add, we can regular transactions. That's kind of the concern I have here is that it changes regular transactions.

[17:00] So I'll make it unsigned again, what make time point unsigned and we'll deprecate signed time points in future operations. Another option would be to make the time point inside the new precondition version 2 signed, since that's a new field. Nobody's going to be using that yet. We keep the old time point as unselling for backwards compatibility. I know it seems not worth it to me, but yeah, let's keep it unsigned. Okay, sounds good, all right. So time points remain unsigned. Where we got next one is around the section on how transactions get

[18:00] Forwarded. So basically says that they are like those two criterias a and b, that allows a node to decide if it's going to accumulate basically this transaction in its queues, and yeah. So, like the first question is that so, yeah, so I think that the criteria b talks about lower signals. It's actually not clear. It's? It seems to imply that you are actually allowing transactions to be accumulated if we receive them out of order, and I think this is probably not the intent. That's the first question. Oh so, in other words, I say don't forward.

[19:00] Oh so, in other words, I say don't forward it, but you should. It should be stronger than that. It should be just like throw it away, like should, yeah, like it should not be better. That's a good point, right. E is actually it should be. It shouldn't have this lower sequence number. I think on the does that make sense. Wait, what do you mean? There shouldn't be? No, you like. Right now you're saying it's invalid if either you know the condition is not met. So this is, those are easy, right? The b is what I'm interested in. Like, right now it says it's invalid if there is already a transaction with a lower signal. Sorry, it's invalid. No, it is valid. That's what your text says. Is the polarity: a transaction with a non zero min ck german sequential atmosphere is invalid enough forwarded if either a the appropriate condition doesn't hold

[20:00] Or b there are pending valid transactions with lower sequence numbers on the same source account. Okay, so wait, so what's? And you're saying b, I should say: and in case b should also throw it away, instead of storing it in memory, well, it's in valid, right, so we are throwing it away. My the question: it's not, no, it's not invalid in a, it's in validity, it's valid. But they're things with a lower sequence number. So let me give you an example, just to make sure we're on the same page: right, so you have a transaction with sequence number two, but that could execute immediately because it has like a min seq noun, that's less than the default, and you have a transaction with sequence number one, right, so you could just execute two in isolation, or you could execute one, but you can't execute them both in

[21:00] The same ledger. No, that understands, yes, like, but here, so if you receive so in your example, I have two right and I have two and that, and then I receive one. What happens then? One takes priority, like you, okay, so this, then it makes. So. Then this is kind of broken. Right, it's actually under specified, first of all it's. Why is it underspecified? Well, you're actually not saying that you're talking about, I guess it. So, yeah, you're saying you're kicking out highest transactions with higher signal. That's right, like, you'll still vote for a block, you know you'll still nominate a block that has it. But is that a good

[22:00] Property? I mean, it sounds kind of weird We already have this. It's exactly. We already have this exact situation. No, we don't kick out transactions like this. Okay, let me tell you a situation I believe is exactly analogous, which is that you receive a transaction with, you know, a fee of 100, and then you receive another transaction, the same sequence number, that has, like, a fee of 200, right. And so now, if you've already, of course, forwarded the 100, well, you forwarded it, fine, but now you'll forward the 200, and if you see both, you'll only forward the one with the higher fee, and so that's the same way. We're basically like, among multiple, mutually exclusive but valid transactions, you need a way to prioritize them, and I'm saying here, you prioritize the one with the lower sequence number. Now, why do you need this question?

[23:00] Oh, because the second one might, could later be valid, right? You're not. Yeah, unlike the case of the sequence numbers, you're not necessarily completely invalidating the second transaction. You're just it will have to execute like a- you know an hour later, or whatever the, you know whatever the min seq age is so, or men, whatever the, yeah, so you know, you might as well like keep them the most number of transactions valid. I mean if you're, if you are strongly opposed to this, we could, I guess doesn't really matter. We could favor the other one, but I don't. I don't see. I mean I like it sounds like kind of arbitrary. So that's why I'm asking, like, why the first one shouldn't win in this case. In some ways it might be more useful if the second one is yeah, because we often say that sequence, numbers is how you invalidate prior transactions.

[24:00] So it sounds like if we say that the first one wins, then you could never use a higher sequence number transaction. With a min sequence number it was lower to invalidate a prior transaction, but someone could always- well, they might not be able to submit it because they might not. It might have a timeout, it might have a min, I'm in time right, which is generally how this works. So, like you know the reason I did this is because, like you know, it seems more useful to have like two. If you have two transactions, then they can both execute. It seems helpful to have it be such that both can execute. And you know, if you accidentally send out the second transaction too soon, like you know, I don't strongly care. If you want it the other way around, I can do it around. It has some implication, right from a dos money because you can basically like,

[25:00] If you have this rule, right, well, you don't say first win, first one wins, you basically can. So, with the example, this will collect more fees. Right, the current draft will collect fees on both transactions. No, you're dropping transfer, you know. Let me finish. So with the example you give, you said it's the same thing when the fee with the fee. You have to actually outbid, right, so you have to constantly like if you want to cancel a pending transaction, you have to update it. Right, with this one you don't have to update. It can be the same fee. No, you are. You said the. That transaction that was in the transaction queue gets discarded because I have a lower sequencing number, I'm not currently discarding it. I thought that's what you were suggesting. Currently you're just not forwarding it. So just to be clear, David, what you're suggesting is like: imagine that I have.

[26:00] Suggesting is like: imagine that I have seen like currency numbers 10, I submit a transaction with minstic 10, actual seek 12, and then I later receive seek 11. Europe, your approach- that what you, what we think is in the cap- and I've been rereading it like five times here while everybody's been talking to make sure I think that's what actually it says- is that you would actually keep both in that case. Because, yes, because you don't know like you could see a block that has either one. It's the same way like if you see, I mean I really, I think it's very similar to the, to the multiple, prices, to the multiple fees, right, whereas, like you know, you see you saw the fee of 100, then you see this fee of 200, so you forward that as well, and you know you still need to kind of- keep the fee of 100 around, because it could be that the block that gets nominated has that. So you don't want to like, if you see a hash of that transaction, you still will need to, like, you know, get the pre image of it right. We actually don't have that transaction,

[27:00] So you might actually have to then go fetch it from another Stellar Core potentially, but the other thing we can do. The other thing, though, is that, with the fees back off exponentially, that's a big difference against this, and I think that's what Nico was saying. Like, I think I forget what multiplier we use. I forgot if we ended up using two or ten. We debated about it for a long time. But either way it's like you can only do this 64 times with fees, or maybe it's 10 times or 20 times with the like. The scenario that I've been trying to think about while I was rereading your thing repeatedly, is like: what if I have two transactions and they can't both execute because of fees? I don't have enough balance to pay both fees. But so, like, going back to my scenario, you're at 10, you have minty 10, seek 12, then you receive 11. The fees be the fees. Make the two transactions mutually exclusive. Which one do I take? Because then I do end up throwing out,

[28:00] Because then I do end up throwing out the future one, if I take the other one, and how do you implement this and what's the right thing to do, like it's not obvious in that case, if the fees make it mutually exclusive, so like the implementation is actually harder in that case. Well, so hold on. This seems no different from today. So suppose, forget you know the min seq num, right? Suppose that I put out transactions you know with sequence numbers 11 and 12. Like, right now they'll both be forwarded, but what if there's not enough fee for 12? Well then you would throw that out, I guess right, but I wouldn't accepted 12 because you kind of, so you're in the so for pending transactions, you're sort of keeping track of the cumulative fee that's been charged. I'd have to go and check how we do this for fee bumps. I don't remember if we reject the fee bump if you wouldn't have the

[29:00] Fee to pay for all the subsequent transactions, or if we- yeah, it's the same thing, we have to. Basically, you have to. Whatever is in the queue you have to be able to pay for the feeds. That. Are you sure that's what we implemented? I'm not positive. Okay, oh no, it's yeah, I'm sure, yeah, so then that would make this a little bit trickier. David, where it's like: then you have to handle the case like, oh, like I have this earlier transaction, but I don't have a fee for it. We should make it symmetric against the female case, I guess. So wait, so you're saying that the feedback, the outer transaction, doesn't have enough to pay, or sorry, what's your yeah, the outer, like, oh, it doesn't really matter. Like it like I both. I have both a normal transaction and a fee bump transaction that I'm paying for

[30:00] On a different account. You're saying: and now you have to like prioritize one or the other. Does it even have to be on a different account? I think it could even work on the same account on different accounts. It definitely works like this: if you don't have the balance on this, the other account, then we'll just reject, you know. But there's different sequence numbers so I can issue like fee bumps on like three different transactions on three different source accounts, right, and if I can only pay for two out of those three, then like different validators are gonna forward, potentially like different subsets of these three transactions, right? So this seems like a very similar situation, so I don't know why. No, it's not, it is similar. If you, that's what I was teasing out. Right, like what you described in the. So imagine your mean is like one, you know, it's like whatever, it doesn't matter, basically it's set, but it's one. And now you're at: yeah, like the six, you get first transaction sequence number ten,

[31:00] Then you get the second transaction, sequence, number eleven. That's what you actually described here. You're dealing with that case that is 11 in this case, you will consider it invalid because for now, right, because you have 10 and you can't process it in the same block. So you just, in this case, you just discard 11, everything is good. 11, basically, can be resubmitted later. All, right, now consider the this in the other direction: I'm receiving 11 and right now you're saying that 10 is valid. So if 10 is valid, I need to discard 11. That's the difference here is that you're discarding things from the queue. So you basically can have a situation where now, take this to you, know a thousand. I'm going to flood t 1000 then and I'm going to basically submit to the network nine, you know, like one less every time, and those are all going to get forwarded, they're all getting flooded and at the end only one gets actually in the queue, which is the smaller one.

[32:00] So I use all this capacity on the. overlay, right, with no repercussion keywords. Okay, so you so. And this would be fixed if we picked the highest sequence number. If we pick no, you don't need you. So highest has the same problem, right? It's what you want to do is the first one wins, I think, independent of the sequence number. The first string don't care, basically you don't care, like, if there is the first one that's in the queue, then if the second one arrives, it says: oh, there is already something with a condition like that. Yeah, so the like. We can totally do that. The downside now is that you're more likely to get conflicting nominations if there's more than one leader. Yeah, but that's fine. It's

[33:00] Actually better from a flooding point of view. The network is going to be much more resilient at this point and it's just: yeah, sorry, and is this what happens with female transactions, like in my example of? Because we should just be consistent. So in my example where I create three bumps, any two of the three are okay and I basically flood different pairs of them to different validators. They'll each forward like the first two that they got. I mean it's the same with p bunk. It's the same if you have, like you know, if I submit two transactions with the same fee and same signal, right, they're basically competing on the overlay, and the second one is always like a bad check, right, like you always say, and then just you just have to fetch it from the denominator. It's not that you fetch it, it's like whoever submitted that thing will have to resubmit it to the network. But what's going to happen is someone's going to nominate a block and, like a bunch of validators are going to be missing like one of the transactions in that blocks, and then they'll just all,

[34:00] Like, have to make sure the validators that had the other, the, this other transaction in their queue, right when the block is going to close, they are going to discard the transaction because it's actually, at this point, in time it's going to be likely invalid and, like, depending on the condition, right, on your like, if you say, for example, all that, the, I need the transaction to be submitted, you know like five ledgers from now, well, obviously it's invalid right now. Right, and one thing you know, I guess my question is kind of independent of this particular example- right, we have this ability to actually create fee bump transactions and kind of spray out a bunch of mutually conflicting fee bump transactions. The result is that whatever block gets nominated is likely going to be missing. It's good looking to have transactions that most of the validators don't have,

[35:00] And so of course they'll just fetch the pre image of that transaction hash. But that of course has overhead on its own. But you're saying like that's not how it was, kind of okay, oh yeah, like transaction sets are flooded independently of transactions today, right. So, basically, like you end up nominating a transaction set, I see the pre, I see the hash of that transaction set in the nomination message and I turn around and I'm like I don't know what that transaction set is, and then I have downloaded all the transactions anyway, sorry, so you download all the transactions anyway, even if you already have most of them. That's right, Nico, I mean you're more active. Yeah, I'm pretty sure that's right. Okay, yeah, that's the way to do it right now. Okay, well, then, that's easy. So I think the simple change is: you don't prioritize one way, it's just: first come first serve for forwarding? Yeah, right, I think that's good, cool, yeah, actually that's that was what I was asking with my question number three in the in that section,

[36:00] Yeah, which is so there's like a. In the second question inside this, you know, section on transaction forwarding and ordering, there was this I was asking basically if we should do something about like bad properties, basically so we have like, with the current proposal, right, that we have for the payment channel, it's actually using separate transactions, for di and ci is basically like on, like one is the one that is allowing you to move the, sequence number, and the other one allows you to or uses the delay, right, if you were going to create a smart contract where you're putting

[37:00] The condition and the jump in one transaction, you're going to have like those kind of delay attacks, right, you can delay those contracts pretty badly. So the question I had was: should we just not allow people- we can always, by the way, like leave that constraint later, but like for now, like if you make it that you can't have a mean condition combined with, yeah, like minsektom, I mean, yeah, you can't combine those. The two mean, second mean age. Basically, sorry, like you can't jump, you can't combine in the same transaction the jump and the age restriction.

[38:00] What would that? I think I did. We're in the two way payment channel, or so imagine, yeah, right now, so the cur, the prop, the protocol for payment channel, doesn't have this problem because the two transactions, ci and the I, they actually have very different purpose, right, and one allows you to jump in the sequence numbers and the other allows you to delay, you know, to grab like those grace periods. Yeah, imagine if, yeah, but imagine if you try to do like a, and I know we're actually looking at that at some point- where it was actually the same transaction has allows you to jump and has the condition right, the on the delay, right, if you have. I'm not sure I understand. So you're saying that you would make it so that people couldn't use nin sikh num and men see gauge in the same transaction, in order to. Because if you imagine, like, imagine that pidi somehow could be

[39:00] Combined- yeah, which is what this is talking about. Like, imagine you have a contract for your lrc in the ai to be combined, right, at that point I don't know what that means, because di is like: declares that you're going to do something, and then ci actually does it, yeah, but the way you could do it right, you could imagine having a cidi combined, yeah, on the same sequence number. Yep, you have one that is just the jump, and then you have another one that, oh, I don't know, like it's a different protocol. Right, like I'm not talking about the existing payment channel. I'm is where you have like: okay, you still have d, and so you declare something, and then you have this thing that could happen in the future. But then in the middle, maybe you have optional transactions that can be submitted like, maybe

[40:00] That would be a use case for that. Like, that last transaction. You want it to be submittable without a specific sequence number of the account, so maybe you want like 10 sequence numbers in between and there are some other transactions that can be submitted before that. I don't know how you, why, that's not. I don't think that's what I'm describing. A transactions where you have both the open range right on sequence number and the delay Yeah, so that's what I'm. describing. So you have that open range so you can submit optional transactions between d and c. The c still has both. It needs to have the min sequence num so that it can be submitted, whether those optional transactions be submitted or not, but they will have the delay. Said, yeah, I see, yeah, you could do that. Basically like you jump and then you

[41:00] Have like and you need to wait for a little bit to do those other transactions, but they are not going to be and if they don't happen, closing your final transaction can still be submitted after the delay. Yeah, so my point here was that if you actually allow people to craft such transactions, those transactions can be attacked because you can like. The assumption with those things is that you can always like- you know, I, because I have the more recent version of that transaction, I can always submit it in a way, but here it's not the case anymore. and I have because I can submit those state ones in the contract, right and now you can basically like, if s like the grace guys or whatever is like fairly big, like as in, you know, like

[42:00] Media, more than a few minutes, and you have a lot of transactions that you're exchanging offline, you can basically submit all of them, all the ones that were until you expire, until you actually expire the window. Certainly I'm confused. Can you like imagine that I clarify something is this, is this: is your objection that this is like too restrictive, that there's transactions that you want, that you can't, that are illegal, or it's the other way around, that there's transactions, okay, all right. So what specifically, is the transaction that is bad? It's so. The transaction itself is not so it's not that you have bad transactions, that they are. You have like in a payment channel kind of situation, right, you exchange a lot of those things offline. That's why you need

[43:00] To be able to jump forward right so that you have the latest transaction being processed. And what I'm saying here is that I think that is if you were designing a smart contract that was combining the two constructs. It's actually a foot gun. Like it's slightly wrong. Which two constructs are we talking about? The delay and jump is the reason why you're saying this is a foot gun. Like I'm trying to understand, just like David is as well. Like imagine that you have like a. The gap is like from sequence number 10 to 20, just to be super concrete. So you've got 11 slots, let's make it 10 to 19, so you've got 10 slots. And then, like the delay is one day, like the min seq age is one day. What you're, I think what Nico's saying, but I could be wrong- is that you actually might incur a

[44:00] 10 day delay because somebody could play in each of the slots and then you go into the last one. Is that what you're saying, Nico, or is that not? I mean yeah, like for the delay ones. That's exactly what will happen. Yet also, the other way around, where you expect, like the, you say, oh, this is only valid for the next five minutes. Right, and now you can basically cause the thing to expire by replaying older ones, like it's. Actually, those would not be sorry, those would be like ones with conditions that are going to be absolute. I mean, yes, of course, if you, I can give you a protocol where, when I throw in time bounds or ledger bounds, like it doesn't work because those bounds aren't there, but like to say that's a foot gun. It's like, well, just don't use that feature if you don't need it and it won't get in your way, right? I just don't see it. What I'm saying is:

[45:00] I just don't see it. What I'm saying is that I don't see right now a use case where you would put both. I can only think of bad things, so you can put both. So the argument is understandable. You're saying like we, should we have a tx conditions v2 and the tx conditions v3 or something? No, it doesn't need to be v1, d2, it's more like a tx conditions like relative delay and a tx conditions like jump sequence gap or something like. Should there be like two flavors of precondition that are mutually exclusive and that have subsets of these things, rather than throwing them all into one tx giant, tx conditions v2? I mean like, yeah, I don't think it's. Yeah, maybe annoying. I don't hate that. If that's necessary to go in, I don't see it. I guess I'm not. Yeah, I'm not. Maybe explaining very well, like the

[46:00] Like I know that we were when we were looking earlier at this protocol. Some transactions had actually those two things you know: set, and if you're not careful, yeah, like you did you end up with a bad like, basically a broken contract, and you know it's part of like you know, kind of giving the ecosystem, like you know, the least number of things that they can trip on those present transactions. I don't object to that. If we want two flavors of tx conditions, I can do that. I can see you know there's some benefit in terms of like transactions take fewer bytes. When serialized there's some additional complexity. Then now you have to, like you know, check the union. Can we keep it as one structure? and then just have core, not allow those two things, fields- to be set at the same time. I was going to ask the same question. Like

[47:00] I mean that seems like the worst of all worlds, because now we're adding, like, additional constraints that aren't expressed in XDR. So, like, reading the XDR is not enough to know like you can make transactions that look valid because XDR is valid, but they violate some rules, so that I don't like plus, you don't get the benefit of the more compact transactions. So I would suggest either keeping it this way or having two different kinds of tx conditions. I think can we maybe take this on offline, like I'm. Yeah, I feel like crosstalk syntax at this point. Right, but it's clear there are two choices and there is like a design decision involved. Right, one is like split it in two in order to protect users more, and two is keep it simpler, even though we are potentially leaving something in there that, although we could use an error, actually I do have a potential a use for this-

[48:00] Well, maybe this is not a very good one, but like, but like you could worry about someone submitting a bunch of these transactions and like draining the fees from the account or something, and so maybe you want to make sure that you know the other person is only doing so many. But that's kind of bogus also because you could always just use a fee bump transaction to get the one that you want to execute through. So, all right, well, again, I'm pretty agnostic on this, so it's a binary choice and it's pretty clear what it is. So I think, moving it offline and just trying to find a way to just ultimately choose one or the other, that makes sense, okay, the only other thing I'd add to this, it's like we should probably convince ourselves that like it actually is always a bad idea to do this. It probably is Nico. I think Niko's point sounds right, but like I don't think any of the rest of us had actually considered that question, so we should probably take at least a few minutes offline to think about that. Yeah, okay,

[49:00] What else we got here? We got six minutes, now more questions. I'm sorry, I took over the meeting. I think this is great because we're getting through these questions- concrete action items, all right. So the next things are around transaction validation. So yeah, so those are like things that I think this is. There's a section that talks about, like, I think, dual validation, right in the dark, that basically what happens basically during nomination, basically, and then what happens later, right, when you actually apply. I think this is what this is really about. So there's a thing that says all but the transaction with the lowest signum on a given source account might have

[50:00] Zero for the mean sec age and mean sec, ledger fields. All right, we're nominated. Block of transactions, right. Asking is like, should we simplify this to just be something more like the rule that we just discussed for flooding transactions? And basically the rule is, if you have a mean sec you know thing condition on the transaction- you can never mix it with any other transaction in the same for the same source icon, like, instead of having this weird, like you know the lowest, I don't understand what we gain by having this extra kind of you know

[51:00] Like basically special cases where you can put two transactions, yeah, I mean. you often might want to do that, like you want to bump a sequence number and then execute a bunch of things at that sequence number that are valid. But are you talking about, yeah, but like it sounds like it's complicated things for no real good reason? I agree with the fact that it's an implementation detail, like we could not. This one is actually part of the. This one is part of the protocol. That's why this one is actually very important because, yeah, so the question said valid, so yeah, so the question is like: do you have, like you know, could you have like sort of various sequences of transactions that you can execute with? You know? So, after some

[52:00] Delay, right, and one reason you might want to do this is suppose, for some reason, you don't want to use claimable balances or you're worried about something failing right. You'd want to break a set of operations up into several different transactions in case, like, one of them fails. So the sequence number's consumed and you can execute the other ones right, and so then, if these, if this kind of series of things you want to do, has some relative delay, then what you do is like the first in the sequence would have like a min seq age, and then the others you would just kind of like not, because you can only execute them with the right sequence numbers. So to me this seems like something you'd want to do. It doesn't seem like a particularly hard thing to implement. I mean, you're already checking that all the sequence numbers are consecutive or that they have mint numbers that make sense. So just to sort of say that, like other than the first one, the min cage also has to be zero. It doesn't seem like a big deal. But

[53:00] I mean small conditions, right? Sorry, I don't. I was definitely useful, right. So the only reason to remove it would be if it's like really hard to implement, right, it's going to. You have to also do that in overlay, right in the queues, like for all the queues that are being managed, well, they all are going to have. You have to solve them. Yeah, they have to be sorted and yeah, like, yeah, but we already have to do that because we have to make sure, number one, it's more like now you have like certain conditions between, depending on the order, right, press, you know, if you think of, I think what we are talking about earlier, is that okay, if there's any transaction in the queue, you know, I don't, I can't add the condition, for example, if I can, I can't add that transaction. Sorry,

[54:00] Right, but that's why we already talked about that. We're gonna. Oh, sorry, what I'm not particularly offended by what you have. David, just to provide a counterpoint to Nico, like I think there could be merit to pipelining and I don't think like I don't think compared to the already very large amount of condition checking that we do in, especially in the transaction queue, I don't think this would be a very material change. That's my opinion. If we think that this kind of pipelining situation is like not only possible but plausible- and I think it probably is- I don't think it's that bad, personally, at this point we're also out of time. So, okay, are you okay with that Nico, or do you still? You're still not convinced? No, I say it's probably fine. Yeah, it's just more conditions, like I said, okay, this one, I do think it's useful. So,

[55:00] Okay, cool, so leave this one as is because it's useful and possible. Unless you know, obviously, I think that perhaps what happens now is that we move this back to the mailing list. There are a few action items for you, David, to make changes to, and a few things we still need to discuss async, but they're pretty simple, straightforward decisions to make, and then that also gives anyone time. If you know, Nico, if you think about this last point and decide that actually the utility may not warrant the actual work or something, you know, there's time to sort of bring it up. But it feels like these are fairly discreet changes and action items that can actually get this thing in a position where we can evaluate a draft, possibly with an eye towards acceptance, very soon. Does that seem true? Yeah, all right, well, awesome everybody. Thanks again for being here, thanks everyone at home for watching and I'll see you all soon.

</details>
