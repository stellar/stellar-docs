---
title: "Data Analytics on Stellar"
description: "Debnil Sur walks through an SDF analytics stack for exploring the network at scale, covering Horizon/Core data limitations, a BigQuery-based pipeline, and practical SQL patterns for market and account analysis."
authors: [debnil-sur]
tags: [tutorial]
---

import YouTube from "@site/src/components/YouTube";

<YouTube ID="W-JlvquCWcY" />

Do you want to analyze historical trends on the network in seconds? In this talk, software engineer Debnil Sur explains how SDF designed a large-scale analytics pipeline using off-the-shelf tools, and shares examples of the kinds of questions you can answer with basic SQL.

The session starts by outlining why Horizon and Core databases (optimized for transactional workloads and API access) are a poor fit for heavy historical analytics at scale, then walks through the design goals for a warehouse-backed approach: organization-wide access, daily refreshes, easy integration with existing infrastructure, and approachable metrics/visualization for non-technical stakeholders.

Debnil then covers an initial implementation that exports data from Postgres, stages files in cloud storage, and loads tables into a warehouse to enable much faster queries. The talk also surveys visualization options and why Metabase was selected for an early, internal drag-and-drop + SQL experience. It closes with v1 limitations and a roadmap toward higher update frequency, better observability, and more public-facing analytics.

### Key Topics

- Why Horizon/Core schemas are great for transactions but slow for large historical analytics.
- Design goals for an internal + community-friendly analytics system.
- A first-pass warehouse pipeline: exports, staging, loading, and fast SQL queries.
- Metrics workflows and why spreadsheets and Python fit the reporting layer well.
- Visualization tradeoffs and why Metabase works for an initial deployment.
- Roadmap topics: real-time-ish updates, task orchestration, observability, and public dashboards.

### Resources

- [Voyager](https://stellar.expert/explorer)
- [Horizon API](/docs/data/apis/horizon/)
- [Stellar Core](https://github.com/stellar/stellar-core)

<details>
  <summary>Video Transcript</summary>

[00:00] Hi everyone. Hope everybody is having a fantastic friday morning afternoon, wherever you are. My name is devneal. I'm a software engineer at SDF on the voyager team.

[01:00] We focus on exploring the Stellar ecosystem through data analytics and liquidity. Today I'll be talking about data analytics on Stellar. So the goal of our team, as well as a lot so the goal of our team, as well as a lot of the work that I do at SDF- is to do the fundamental meta work that powers the rest of the network. How can we ensure that assets have robust markets, how can we improve the visibility of the network, both inside and outside SDF and how do we use data to further drive network growth? In this talk I'll be talking about the first version of our data analytics system targeted and answering those three questions as well as many others. So who is this helpful for? Anyone interested in? One, building analytics pipelines from zero to one. I hope our experience is generally helpful to people who have a bunch of data that are just trying to build out some analytic system. Two, exploring the Stellar network, I'll present some useful access patterns, tips for navigating the Stellar data set

[02:00] And a few illustrative examples. With basic sql knowledge, you can ask some powerful network wide questions. Three, scaling analytics from one to one hundred. If you have a working data pipeline, what's next? I'll share some of my thoughts on the subject and I'd love to hear your wants. As a brief roadmap of the talk. It'll be in four parts. First, design: what we wanted to build. Second, implementation: how we built it. Design and implementation are the meat of this talk, as it is an engineering talk after all. Third analysis: I'll display some interesting queries along with some strategic tips on using our public data set in the interest of time and network connections. Over the talk, I'm not going to live code, but I will share links for folks do their own exploring. And finally, next steps: what's in the pipeline? No pun intended for data analytics. I note that if you have questions, there's a qr code on the youtube and you can also ask questions of the youtube chat

[03:00] And I'll be answering those questions at the end to make sure I get through all this material. So first design: to understand the design goals, let's first talk about our motivations. Why did we even want a cloud scale data pipeline? Ultimately, we want to quickly and efficiently generate the histories of assets, accounts, markets, whatever, over the history of the network. The primary use case is internal business analytics. We wanted to make it as easy as possible for our business development and ecosystem teams to understand the effects of their efforts. So Stellar uses the widely used database, as some postgres, which has been an industry standard for 20, 25 years. So Stellar Core writes data to postgres tables and then Horizon, our api, to Stellar Core uses the core data, but it also has its own database because it's a web application. So the pros of this are that it's well known and well documented. It's great for tran processing a bunch of online transactions when there's a bunch coming into you in a highly concurrent heavy write

[04:00] Environment. The cons are also pretty well documented. It's slow for historical queries for specific column values, which makes it much worse for analytics. Horizons, database structure and indices are optimized for the exposed api of Horizon. That also means that if you're trying to do deeper queries across a bunch of different accounts or tables, it quickly becomes inefficient. So, for example, complex joins or filters become really extremely time inefficient apps and indexes. A good example of this would be trying to do analysis of the network but excluding arbitrage operations, because that would require a bunch of joints of different tables, and then now you have to make filters over a really big table. So, to understand behavior at scale, it became pretty clear to us that we would have to build some new infrastructure. It's also important to talk about our goals. So, above all, we wanted value for internal stakeholders. This is a really important node. When you're building data products, it's really easy to nerd out and build something that's really cool for

[05:00] Engineers, but the end user is always your non technical users. If product guarantees don't align with business needs, then you won't ship the right solution for your end users and, even worse, you might just make more work for yourself because you have to onboard a bunch of people into a non intuitive system. Second, open source or open access. Open source is really core to fdf dna and we want to support open source tools wherever possible. Of course, some things require spend, like cloud infrastructure, but we want to optimize for open access in the organization, followed by easy public access. Third, a daily updated data set. Well, some businesses need real time updates. At the point that we were when we started building this, daily updates were sufficient for the course grain metrics that we wanted to do. Fourth, seamless integration with our current staff. As mentioned before, we rely heavily on postgres as our databases and when you have existing tools, you need to make sure that support for those tools is well tested in anything you bolt onto your system.

[06:00] Additionally, it had to also be deployed easily. It had to be in house controlled on our infrastructure and easy to deploy by our site for liability and ops team. So our stack last year migrated from puppet to docker and kubernetes, so we needed technology that easily integrated with this. And for some quick buzzword explanation: puppet is a legacy system administration software. Docker lets you contain custom programs in their environment and kubernetes is orchestration software that lets you decide how those containers are then deployed and run. And fifth and finally, easy metrics and visualization. This is pretty self explanatory it lowers the bar for non technical users and it brings much more value to the organization as a whole. So many of the requirements about birth fuzzy. Some are more technical and tools oriented. So how do those break down into technical decisions? So this is ordered from most to least. Defined as a deployment infrastructure, we would use docker and kubernetes. This would be important for orchestrating software, having regularly

[07:00] Scheduled cron jobs and the like. It was already our technical stack so we needed to play with it. Second, the cloud scale warehouse. This needed a bunch of capabilities determined by what I just said. It had to have scripting abilities, really good postgres integration and really easy organization wide credentialing so all engineers and non engineers could easily access the warehouse and make their own queries. Third, metrics and visualization. This was the least defined for sure. We decided that it would be secondary to a data warehouse because ultimately, the engine matters more than whatever you're doing on top of it. But we did know that we want this to be open source, free and easy for non technical users through something like the drag and drop interface. Let's now talk about implementation. So the first was infrastructure: docker and kubernetes. While this decision was made for us. Some details of it made everything a lot easier, which is what commonly happens when you have to integrate with an existing stack.

[08:00] Docker based containerization gave us a really easy standard for evaluation of other tools, particularly visualization. Could it be easily deployed on our infrastructure? Second, kubernetes cron jobs gave us an easy scheduling method for running scripts deployed via docker. We could just set a schedule and run the procedure as is. It was also really easy to integrate the postgres cluster. It was easy to provision external storage in case we needed to have external memory for the data pipeline and it also allowed for retries on failure. In all, this is a really good deployment stack and it's pretty clear why it's become the modern stack of choice. It's really good for integrating a lot of different technology. It's really resilient. In general, it's pretty easy to work with no real complaints about docker and kubernetes. Second, for the cloud data warehouse, I was kind of worried about this at first because there are a ton of options, but google bigquery actually ended up being a really easy choice for us. For one, the queries are super fast. They were 10 to even a thousand times faster than some queries from the Horizon

[09:00] Database. We were honestly just blown away by how much better it was. Second, intermediate cloud storage. Where do files live during the data pipeline? Google cloud has a cloud storage service which is called google cloud storage. This means that we can separate the pipeline into a few different parts: one: export tables from postgres to disk to upload from the disk to cloud storage. And three: download from cloud storage to bigquery. So separating the pipeline like that reduced the risk of failure and it also made retries less expensive because we would just retry one of the scripts. Third, there was really painless command line scripting. It's super straightforward to script: exporting files from postgres, then uploading to bigquery. It made creating a basic pipeline really easy to reason about from the command line and also, in turn, pretty easy to containerize because we were just uploading a bash script into the docker container. Fourth, google suite authentication. So within SDF we used g suite, so it meant that everyone had g suite permissions. It also meant it was really easy to set

[10:00] Up and share permissions across the organization. I would say that this is like a low priority in terms of deciding a warehouse. But it is actually why we tried bigquery up. First, because it was really easy to integrate within our organization and I suspect that organizations that run on microsoft, like microsoft teams or whatever, might have similar experience with azure. So I do think it probably determines live organizational needs. Third, for metrics we decided to use google sheets. This was actually more straightforward than the warehouse because we decided that we wanted some basic data science capabilities that were slightly outside the scope of queries and after we talked to a bunch of folks around the organization, it made the most sense to surface this through spreadsheets. It's a really common user interface for non technical folks and it keeps data tabular so you can pretty easily connect. Here's whatever the table I got out and here's how it looks on the actual spreadsheet. So python scripts are also the right tool for post processing by a lot. It really is a swiss army knife for data. You can really easily read a table

[11:00] From bigquery, run some custom post processing using common data science tools like pandas and then write the output to a google sheet. Finally, note that we deployed these scripts in a server less fashion on google cloud scheduler. Once the bigquery dataset was updated by the kubernetes orchestrated cron job, you could then trigger these scripts and then update the spreadsheet. It was a really cool event driven architecture: really simple, really powerful and honestly, I think that this sort of way to automate metrics is a really good one. Fourth and finally, visualization. This was by and far away the hardest part of the implementation, since there's actually a million tools and documentation that compares them directly doesn't really exist. So at this point, like we've settled on bigquery, so we wanted really easy bigquery integration, really easy deployment on docker and kubernetes, some nice visualizations that use drag and drop and some sql, and we also wanted to be open source. So I tried a lot of tools that fit some or all of those capabilities.

[12:00] Some of the most prominent ones that we looked at were google data studio, apache, superset and looker. What we generally found was the tools that played nice with bigquery, usually either didn't have drag and drop or weren't free, and so we ended up settling on metabase because it honestly fit all of our original pillars quite well. It has both sql as well as a drag and drop interface. It's free and really easy to set up and I think if you need to mvp a pipeline, it's a really good option. Finally, let's talk about some of the downsides of our v1 implementation. For one, daily frequency is slow, while good enough for initial use cases. A daily update limits building out data intensive apps on seller. We want to get closer to real time updates for next version of the system. Two, observing failures is hard. We were still learning our way around kubernetes logging and ideally we want to be able to reach by really specific, granular portions when they fail. So there are good task management systems out there and that seemed like a

[13:00] Natural evolution of the system. Three: visualizations were private. We'd love to expose our information visualizations publicly, but we can't mix the platform that we use for internal business analytics with everyone else, so we decided that we'd have to think about an intermediate solution. And four, exploring data is painful, so you'll notice I didn't actually talk about a data science platform. Above and while serverless functions provide some capabilities for robust and regular jobs, ideally the platform enables exploratory data analysis through scripts. Every data scientist now uses jupiter notebooks and we want to make it really easy to do the same on Stellar data. So now let's look at some basic analysis of some queries on our system. So for one, we'll start off with an easy query: what account has the highest lumen balance? I call this easy because it's pretty short, but it's really cool because it illustrates how powerful even basic sql tools are. So this shows some really basic sql syntax:

[14:00] Select from, order by and limit. Those give you the tools to ask basic ordinal questions about Stellar history. Select from chooses specific fields from a table. Order by orders the results of the field and limit says how many you want to see. As you can all see the account with the most lumens is the galaxy void account, which received the lumen burn last merity. Now a medium difficulty ferry: how many payments of an asset are made daily? So we show you some new syntax here. Date converts a time to justice. Day sum is an example. Aggregation function which takes a bunch of results and then combines them into one specific number. And where can be used for various condition where clauses shown here. Finally, group by will, group by a specific field. So this lets us pretty organize pretty quickly: organize and group results by day, compute daily amounts and do some pretty good aggregations.

[15:00] All in all, it's actually really simple to be able to make these sort of day by day analysis and it's really core part of some of the organizational metrics we track. So finally, a high difficulty query: how many trades of a trading pair are made per day? You'll notice that this has a lot of lines, but the actual primitives are pretty similar to things you just saw. It has some new syntax, so we use with as to create some temporary in memory tables to query and join. Lets us join together different tables on common fields so you can have some really big tables that you can now condition, filter, group, so on, so forth. Once again, I want to say that, like the queries are not the focus of this particular talk. The engineering is much more of one, but feel free to reach out to me either on the youtube chat or after. I'm happy to provide more instructions on how anyone can make these queries. We also put out a blog post earlier this summer that covers a lot of this and I highly recommend checking it out.

[16:00] So forth. And finally, let's talk about some next steps to motivate these, think about the downsides from implementation and we'll talk about some of the things we could do. So, for one, going from slow to real time frequency. So new Horizon capabilities, like the new ingestion engine, help us get closer to real time because that enables much faster extraction of data from Stellar Core and history archives. Our awesome intern, isaiah turner, has been building a command line tool that uses these capabilities to read in data in close to real time and then output it in the expected schema, and the link to it is right here in the slice. Second, going from low to high observability. Fixing failures is pretty hard, but task management makes observing and debugging a lot easier. So while our current system wasn't unmanageable, the number of moving parts made reading kubernetes logs the primary solution. It's important to note that we added some sentry logging around business metric, but the thing about all of that is that it shows you that a problem happened.

[17:00] It doesn't really tell you where it did or what to do to fix it. So, combined with the above tooling, isaiah has been working on an airflow task management system for the pipeline. Airflow is a task management system that airbnb open sourced a few years ago and honestly, it's been great in our experience at being able to orchestrate a bunch of different smaller scripts, being able to say hey, this failed and giving a really nice UI for engineers to go and just retry different parts. So this has the same overall flow as before. It reads ledgers from Stellar Core, it writes structured documents to google cloud and then it uses those as changes to the bigquery tables. So this one is almost done, it should be done next week, and the link to that, which is it's already open source, is on the slides as well. Third, going from private to soon public visualization. We're thinking about displaying markets and corridors in a public facing site like `stellar.org` it'll make it easier for prospective anchors to see what the volume looks like in and out of specific businesses and countries. We're just reasoning about the right

[18:00] Strategy in a way that meets both internal and external needs. So, once the new data system is stable, we're going to see how you can leverage bigquery data warehousing to build a scalable web application on top of it. It's its own engineering challenge, because it's pretty hard to do that in a time efficient way, but it's a really exciting one and I'm excited to get cracking on it. Fourth and finally, data exploration is painful, so a data science platform is very much tbd. If anybody in the community wants to take this challenge on, I'm more than happy to see some community implementation and feel free to reach out to me with either ideas or a desire to try to figure out how to do it. I'm very happy to talk about it. So that's all for me. I hope you really enjoyed hearing about this pipeline and all the stuff we've been doing, and I'm happy to answer any questions. So one question that's been asked: are there any cool projects using this data set that you wish existed? So I'll start by plugging again the two

[19:00] Things that I just said, because I think they enable a lot of the, cool projects. So, for one, I think it should be really easy to see- here's what all the major anchors on Stellar are doing- and it should be really easy to see what the volume in and out of specific quarters looks like, and it should be even easier to see what the rates look like. I think the rates are really hard to be able to do on this data set, but really powerful. So, for example, one of the reasons that the euro t to naira corridor has been killing it lately has been because the rates for that are so much better than what you would get when you have really good rates on Stellar. It makes the whole network work and I think that applications that surface information like that, like the key killer applications of Stellar, are the most useful to leverage this data set in the short term. Longer term things that I think would be cool, that leveraged this data set. I think it could be really cool to see how assets on the DEX itself can function as a hedge

[20:00] Against inflation through sort of price histories over time. This is one of the things that people talk about as a goal for crypto and one of the things that, with vibrant, SDF, has started working on and I think that if there were projects that demonstrated that longitudinal history, it would be another really cool value for Stellar in the network. So another question: what can a community do to help expand Stellar and improve it? For those who do not understand the engineering aspect of that- so that's a good and interesting question- I will say that you don't need to understand the engineering aspect of what I just talked about to use the data set. That's one thing that we really tried to make sure we could do, which is just being able to use some basic sql queries and show some really powerful data. What can the community do to expand on it? Lots of things, but within the scope of this talk, I would say that it looks like telling people that, hey, Stellar is really cool, but then also showing them what the volume on Stellar looks like. So this is one thing that I've thought a

[21:00] Lot over the course of the summer, when we've had a lot of volume on Ethereum with DeFi. Right, how do you show similar volume with Stellar? Because Stellar is one of the few layer ones that can actually do layer two things like that. So I think demonstrating that value like that exists, and showing people that seller isn't just a payments platform with XLM, there's all this other stuff you can do on top of it. I think that's one of the things that becomes a lot easier with really publicly queriable network history. It would be really cool for community members to promote it in a data driven fashion. Awesome. So it looks like no more questions. So last call if anyone has anything else. But feel free to reach out to me either over key base- my key base username is devnet nil- or over twitter. My twitter username is debna. Sir

[22:00] D e b n I l s u r I'm happy to talk about things Stellar related, as well as waste lovers, data set in the community.

</details>
