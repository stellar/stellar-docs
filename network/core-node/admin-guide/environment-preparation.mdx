---
title: Environment Preparation
sidebar_position: 50
---

import { CodeExample } from "@site/src/components/CodeExample";

## `stellar-core` Configuration

Cross reference your validator settings. In particular, you should be familiar with:

- environment specific settings
  - network passphrase
  - known peers
- home domains and validators array
- seed defined if validating
- [Automatic maintenance](#cursors-and-automatic-maintenance) configured properly, especially when `stellar-core` is used in conjunction with a downstream system like Horizon.

## Database and Local State

After configuring your [database](#database) and [buckets](#buckets) settings, when running stellar-core for the first time, you must initialize the database:

```bash
stellar-core new-db
```

This command will initialize the database, as well as the bucket directory, and then exit. You can also use this command if your database gets corrupted and you want to restart it from scratch.

### Database

Stellar-core stores the state of the ledger in a SQL database.

This DB should either be a SQLite database or, for larger, production instances, a separate PostgreSQL server is recommended.

:::note

Horizon currently depends on using PostgreSQL.

:::

To learn how to specify the database in your node's configuration, please see the [example config].

#### Cursors and Automatic Maintenance

Some tables in the database act as a publishing queue for external systems such as Horizon and generate **metadata** for changes happening to the distributed ledger.

If not managed properly, those tables will grow without bounds. To avoid this, a built-in scheduler will delete data from old ledgers that are not used anymore by other parts of the system (external systems included).

The settings that control the automatic maintenance behavior are:

- `AUTOMATIC_MAINTENANCE_PERIOD`,
- `AUTOMATIC_MAINTENANCE_COUNT`, and
- `KNOWN_CURSORS`.

By default, stellar-core will perform this automatic maintenance, so be sure to disable it until you have done the appropriate data ingestion in downstream systems (Horizon, for example, sometimes needs to reingest data).

If you need to regenerate the metadata, the simplest way is to replay ledgers for the range you're interested in after (optionally) clearing the database with the `newdb` command [referenced earlier](#database-and-local-state).

Note that in some cases automatic maintenance has just too much work to do in order to get back to the nominal state. This can occur following large catchup operations such as when performing a full catchup that may create a backlog of 10s of millions of ledgers.

If this happens, database performance can be restored. The node will take some downtime while performing the following recovery commands:

1. run the `maintenance` http command manually with a large number of ledgers, and
2. perform a database maintenance operation such as `VACUUM FULL` to reclaim/rebuild the database as needed.

#### Metadata Snapshots and Restoration

Some deployments of stellar-core and Horizon will want to retain metadata for the _entire history_ of the network. This metadata can be quite large and computationally expensive to regenerate anew by replaying ledgers in stellar-core from an empty initial database state, as described in the previous section.

This can be especially costly if it must be run more than once. For instance, when bringing a new node online. Or even if running a single node with Horizon, having already ingested the metadata _once_: a subsequent version of Horizon may have a schema change that entails re-ingesting it _again_.

Some operators therefore prefer to shut down their stellar-core (and/or Horizon) processes and _take filesystem-level snapshots_ or _database-level dumps_ of the contents of stellar-core's database and bucket directory, and/or Horizon's database, after metadata generation has occurred the first time. Such snapshots can then be restored, putting stellar-core and/or Horizon in a state containing metadata without performing full replay.

Any reasonably-recent state will do. Even if such a snapshot is a little old, stellar-core will replay ledgers from whenever the snapshot was taken to the current network state anyways. This procedure can greatly accelerate restoring validator nodes, or cloning them to create new ones.

#### Buckets

Stellar-core stores a duplicate copy of the ledger in the form of flat XDR files called "buckets." These files are placed in a directory specified in the config file as `BUCKET_DIR_PATH`, which defaults to `buckets`. The bucket files are used for hashing and transmission of ledger differences to history archives.

Buckets should be stored on a fast, local disk with sufficient space to store several times the size of the current ledger.

For the most part, the contents of both directories can be ignored as they are managed by stellar-core.

## History Archives

Stellar-core normally interacts with one or more "history archives," which are configurable facilities for storing and retrieving flat files containing history checkpoints: `bucket` files, and history logs. History archives usually use off-site, commodity storage services such as Amazon S3, Google Cloud Storage, Azure Blob Storage, or custom SCP/SFTP/HTTP servers.

Use command templates in the config file to give the specifics of which services you will use and how to access them. The [example config] will demonstrate how to configure a history archive through command templates.

While it is possible to run a stellar-core node with no configured history archives, it will be _severely limited_, unable to participate fully in a network, and likely unable to acquire synchronization at all. At the very least, if you are joining an existing network in a read-only capacity, you will still need to configure a `get` command to access that network's history archives.

### Configuring to Get Data from an Archive

Your configuration can utilize any number of archives to download from. Stellar-core will automatically use them in a round-robin fashion.

At a minimum you should configure `get` archives for each full validator referenced from your quorum set (see the `HISTORY` field in the [example config] for more detail).

:::note

If you notice a lot of errors related to downloading archives, you should ensure all archives in your configuration are up-to-date.

:::

### Configuring to Publish to an Archive

Archive sections can also be configured with `put` and `mkdir` commands to cause the instance to publish to that archive (for nodes configured as [archiver nodes](../README.mdx#archiver-nodes) or [full validators](../README.mdx#full-validators)).

The very first time you want to use your archive, _before starting your node_, you need to initialize it with:

```bash
stellar-core new-hist <historyarchive>
```

:::info IMPORTANT:

- make sure that you configure both `put` and `mkdir` if `put` doesn't automatically create sub-folders
- writing to the same archive from different nodes is not supported and will result in undefined behavior, _potentially data loss_.
- do not run `newhist` on an existing archive unless you want to erase it.

:::

If you want to run a [Full Validator](../README.mdx#full-validator) or an [Archiver](../README.mdx#archiver), you need to set up your node to publish a history archive. You can host an archive using a blob store such as Amazon's S3 or Digital Ocean's spaces, or you can simply serve a local archive directly via an HTTP server such as Nginx or Apache. If you're setting up a [Basic Validator](../README.mdx#basic-validator), you can skip this section. No matter what kind of node you're planning to run, make sure to set it up to `get` history, which is covered in [Configuration](./configuring.mdx).

## Caching and History Archives

You can significantly reduce the data transfer costs associated with running a public History archive by using common caching techniques or a CDN.

Three simple rules apply to caching the History archives:

- Do not cache the archive state file `.well-known/stellar-history.json` (**"Cache-Control: no-cache"**)
- Do not cache HTTP 4xx responses (**"Cache-Control: no-cache"**)
- Cache everything else for as long as possible (**> 1 day**)

## Local History Archive Using nginx

To publish a local history archive using nginx:

- Add a history configuration stanza to your `/etc/stellar/stellar-core.cfg`:

<CodeExample>

```
[HISTORY.local]
get="cp /mnt/xvdf/stellar-core-archive/node_001/{0} {1}"
put="cp {0} /mnt/xvdf/stellar-core-archive/node_001/{1}"
mkdir="mkdir -p /mnt/xvdf/stellar-core-archive/node_001/{0}"
```

</CodeExample>

- Run new-hist to create the local archive

`# sudo -u stellar stellar-core --conf /etc/stellar/stellar-core.cfg new-hist local`

This command creates the history archive structure:

<CodeExample>

```
# tree -a /mnt/xvdf/stellar-core-archive/
/mnt/xvdf/stellar-core-archive
└── node_001
    ├── history
    │   └── 00
    │       └── 00
    │           └── 00
    │               └── history-00000000.json
    └── .well-known
        └── stellar-history.json

6 directories, 2 files
```

</CodeExample>

- Configure a virtual host to serve the local archive (Nginx)

<CodeExample>

```
server {
  listen 80;
  root /mnt/xvdf/stellar-core-archive/node_001/;

  server_name history.example.com;

  # default is to deny all
  location / { deny all; }

  # do not cache 404 errors
  error_page 404 /404.html;
  location = /404.html {
    add_header Cache-Control "no-cache" always;
  }

  # do not cache history state file
  location ~ ^/.well-known/stellar-history.json$ {
    add_header Cache-Control "no-cache" always;
    try_files $uri;
  }

  # cache entire history archive for 1 day
  location / {
    add_header Cache-Control "max-age=86400";
    try_files $uri;
  }
}
```

</CodeExample>

## Amazon S3 History Archive

To publish a history archive using Amazon S3:

- Add a history configuration stanza to your `/etc/stellar/stellar-core.cfg`:

<CodeExample>

```toml
[HISTORY.s3]
get='curl -sf http://history.example.com/{0} -o {1}' # Cached HTTP endpoint
put='aws s3 cp --region us-east-1 {0} s3://bucket.name/{1}' # Direct S3 access
```

</CodeExample>

- Run new-hist to create the s3 archive

`# sudo -u stellar stellar-core --conf /etc/stellar/stellar-core.cfg new-hist s3`

- Serve the archive using an Amazon S3 static site
- Optionally place a reverse proxy and CDN in front of the S3 static site

<CodeExample>

```
server {
  listen 80;
  root /srv/nginx/history.example.com;
  index index.html index.htm;

  server_name history.example.com;

  # use google nameservers for lookups
  resolver 8.8.8.8 8.8.4.4;

  # bucket.name s3 static site endpoint
  set $s3_bucket "bucket.name.s3-website-us-east-1.amazonaws.com";

  # default is to deny all
  location / { deny all; }

  # do not cache 404 errors
  error_page 404 /404.html;
  location = /404.html {
    add_header Cache-Control "no-cache" always;
  }

  # do not cache history state file
  location ~ ^/.well-known/stellar-history.json$ {
    add_header Cache-Control "no-cache" always;
    proxy_intercept_errors on;
    proxy_pass  http://$s3_bucket;
    proxy_read_timeout 120s;
    proxy_redirect off;
    proxy_buffering off;
    proxy_set_header        Host            $s3_bucket;
    proxy_set_header        X-Real-IP       $remote_addr;
    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header        X-Forwarded-Proto $scheme;
  }

  # cache history archive for 1 day
  location / {
    add_header Cache-Control "max-age=86400";
    proxy_intercept_errors on;
    proxy_pass  http://$s3_bucket;
    proxy_read_timeout 120s;
    proxy_redirect off;
    proxy_buffering off;
    proxy_set_header        Host            $s3_bucket;
    proxy_set_header        X-Real-IP       $remote_addr;
    proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header        X-Forwarded-Proto $scheme;
  }
}
```

</CodeExample>

## Backfilling a history archive

Given the choice, it's best to configure your history archive prior to your node's initial synch to the network. That way your validator's history publishes as you join/synch to the network.

However, if you have _not_ published an archive during the node's initial synch, it's still possible to use the [stellar-archivist](https://github.com/stellar/go/tree/master/tools/stellar-archivist) command line tool to mirror, scan, and repair existing archives.

Using the [SDF package repositories](https://github.com/stellar/packages) you can install `stellar-archivist` by running `apt-get install stellar-archivist`

The steps required to create a History archive for an existing validator — in other words, to upgrade a Basic Validator to a Full Validator — are straightforward:

- Stop your stellar-core instance (`systemctl stop stellar-core`)
- Configure a history archive for the new node

<CodeExample>

```toml
[HISTORY.local]
get="cp /mnt/xvdf/stellar-core-archive/node_001/{0} {1}"
put="cp {0} /mnt/xvdf/stellar-core-archive/node_001/{1}"
mkdir="mkdir -p /mnt/xvdf/stellar-core-archive/node_001/{0}"
```

</CodeExample>

- Run new-hist to create the local archive

`# sudo -u stellar stellar-core --conf /etc/stellar/stellar-core.cfg new-hist local`

This command creates the History archive structure:

<CodeExample>

```
# tree -a /mnt/xvdf/stellar-core-archive/
/mnt/xvdf/stellar-core-archive
└── node_001
    ├── history
    │   └── 00
    │       └── 00
    │           └── 00
    │               └── history-00000000.json
    └── .well-known
        └── stellar-history.json

6 directories, 2 file
```

</CodeExample>

- Start your Stellar Core instance (`systemctl start stellar-core`)
- Allow your node to join the network and watch it start publishing a few checkpoints to the newly created archive

<CodeExample>

```
2019-04-25T12:30:43.275 GDUQJ [History INFO] Publishing 1 queued checkpoints [16895-16895]: Awaiting 0/0 prerequisites of: publish-000041ff
```

</CodeExample>

At this stage your validator is successfully publishing its history, which enables other users to join the network using your archive (although it won't allow them to `CATCHUP_COMPLETE=true` as the archive only has partial network history).

## Complete History Archive

If you decide to publish a complete archive — which enables other users to join the network from the genesis ledger — it's also possible to use `stellar-archivist` to add all missing history data to your partial archive, and to verify the state and integrity of your archive. For example:

<CodeExample>

```
# stellar-archivist scan file:///mnt/xvdf/stellar-core-archive/node_001
2019/04/25 11:42:51 Scanning checkpoint files in range: [0x0000003f, 0x0000417f]
2019/04/25 11:42:51 Checkpoint files scanned with 324 errors
2019/04/25 11:42:51 Archive: 3 history, 2 ledger, 2 transactions, 2 results, 2 scp
2019/04/25 11:42:51 Scanning all buckets, and those referenced by range
2019/04/25 11:42:51 Archive: 30 buckets total, 30 referenced
2019/04/25 11:42:51 Examining checkpoint files for gaps
2019/04/25 11:42:51 Examining buckets referenced by checkpoints
2019/04/25 11:42:51 Missing history (260): [0x0000003f-0x000040ff]
2019/04/25 11:42:51 Missing ledger (260): [0x0000003f-0x000040ff]
2019/04/25 11:42:51 Missing transactions (260): [0x0000003f-0x000040ff]
2019/04/25 11:42:51 Missing results (260): [0x0000003f-0x000040ff]
2019/04/25 11:42:51 No missing buckets referenced in range [0x0000003f, 0x0000417f]
2019/04/25 11:42:51 324 errors scanning checkpoints
```

</CodeExample>

As you can tell from the output of the `scan` command, some history, ledger, transactions, and results are missing from the local history archive.

You can repair the missing data using stellar-archivist's `repair` command combined with a known full archive — such as the SDF public history archive:

`# stellar-archivist repair http://history.stellar.org/prd/core-testnet/core_testnet_001/ file:///mnt/xvdf/stellar-core-archive/node_001/`

<CodeExample>

```
2019/04/25 11:50:15 repairing http://history.stellar.org/prd/core-testnet/core_testnet_001/ -> file:///mnt/xvdf/stellar-core-archive/node_001/
2019/04/25 11:50:15 Starting scan for repair
2019/04/25 11:50:15 Scanning checkpoint files in range: [0x0000003f, 0x000041bf]
2019/04/25 11:50:15 Checkpoint files scanned with 244 errors
2019/04/25 11:50:15 Archive: 4 history, 3 ledger, 263 transactions, 61 results, 3 scp
2019/04/25 11:50:15 Error: 244 errors scanning checkpoints
2019/04/25 11:50:15 Examining checkpoint files for gaps
2019/04/25 11:50:15 Repairing history/00/00/00/history-0000003f.json
2019/04/25 11:50:15 Repairing history/00/00/00/history-0000007f.json
2019/04/25 11:50:15 Repairing history/00/00/00/history-000000bf.json
...
2019/04/25 11:50:22 Repairing ledger/00/00/00/ledger-0000003f.xdr.gz
2019/04/25 11:50:23 Repairing ledger/00/00/00/ledger-0000007f.xdr.gz
2019/04/25 11:50:23 Repairing ledger/00/00/00/ledger-000000bf.xdr.gz
...
2019/04/25 11:51:18 Repairing results/00/00/0e/results-00000ebf.xdr.gz
2019/04/25 11:51:18 Repairing results/00/00/0e/results-00000eff.xdr.gz
2019/04/25 11:51:19 Repairing results/00/00/0f/results-00000f3f.xdr.gz
...
2019/04/25 11:51:39 Repairing scp/00/00/00/scp-0000003f.xdr.gz
2019/04/25 11:51:39 Repairing scp/00/00/00/scp-0000007f.xdr.gz
2019/04/25 11:51:39 Repairing scp/00/00/00/scp-000000bf.xdr.gz
...
2019/04/25 11:51:50 Re-running checkpoing-file scan, for bucket repair
2019/04/25 11:51:50 Scanning checkpoint files in range: [0x0000003f, 0x000041bf]
2019/04/25 11:51:50 Checkpoint files scanned with 5 errors
2019/04/25 11:51:50 Archive: 264 history, 263 ledger, 263 transactions, 263 results, 241 scp
2019/04/25 11:51:50 Error: 5 errors scanning checkpoints
2019/04/25 11:51:50 Scanning all buckets, and those referenced by range
2019/04/25 11:51:50 Archive: 40 buckets total, 2478 referenced
2019/04/25 11:51:50 Examining buckets referenced by checkpoints
2019/04/25 11:51:50 Repairing bucket/57/18/d4/bucket-5718d412bdc19084dafeb7e1852cf06f454392df627e1ec056c8b756263a47f1.xdr.gz
2019/04/25 11:51:50 Repairing bucket/8a/a1/62/bucket-8aa1624cc44aa02609366fe6038ffc5309698d4ba8212ef9c0d89dc1f2c73033.xdr.gz
2019/04/25 11:51:50 Repairing bucket/30/82/6a/bucket-30826a8569cb6b178526ddba71b995c612128439f090f371b6bf70fe8cf7ec24.xdr.gz
...
```

</CodeExample>

A final scan of the local archive confirms that it has been successfully repaired

`# stellar-archivist scan file:///mnt/xvdf/stellar-core-archive/node_001`

<CodeExample>

```
2019/04/25 12:15:41 Scanning checkpoint files in range: [0x0000003f, 0x000041bf]
2019/04/25 12:15:41 Archive: 264 history, 263 ledger, 263 transactions, 263 results, 241 scp
2019/04/25 12:15:41 Scanning all buckets, and those referenced by range
2019/04/25 12:15:41 Archive: 2478 buckets total, 2478 referenced
2019/04/25 12:15:41 Examining checkpoint files for gaps
2019/04/25 12:15:41 Examining buckets referenced by checkpoints
2019/04/25 12:15:41 No checkpoint files missing in range [0x0000003f, 0x000041bf]
2019/04/25 12:15:41 No missing buckets referenced in range [0x0000003f, 0x000041bf]
```

</CodeExample>

Start your stellar-core instance (`systemctl start stellar-core`), and you should have a complete history archive being written to by your full validator.

[example config]: https://github.com/stellar/stellar-core/blob/master/docs/stellar-core_example.cfg
