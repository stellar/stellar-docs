---
title: Comenzar
sidebar_position: 20
---

import history_table_export from "/img/hubble/history_table_export.png";
import state_table_export from "/img/hubble/state_table_export.png";
import dbt_enriched_base_tables from "/img/hubble/dbt_enriched_base_tables.png";

[repositorio de GitHub stellar-etl-airflow](https://github.com/stellar/stellar-etl-airflow/tree/master)

## Configuración de cuenta de GCP

La Stellar Development Foundation ejecuta Hubble en GCP utilizando Composer y BigQuery. Para seguir el mismo despliegue, necesitarás tener acceso al proyecto de GCP. Las instrucciones se pueden encontrar en la documentación de [Comenzar](https://cloud.google.com/docs/get-started) de Google.

Nota: BigQuery y Composer deberían estar disponibles por defecto. Si no lo están, puedes encontrar instrucciones para habilitarlos en la documentación de Google sobre [BigQuery](https://cloud.google.com/bigquery?hl=en) o [Composer](https://cloud.google.com/composer?hl=en).

## Crear instancia de GCP Composer para ejecutar Airflow

Las instrucciones para crear una instancia de GCP Composer para ejecutar Hubble se pueden encontrar en la sección de [Instalación y Configuración](https://github.com/stellar/stellar-etl-airflow?tab=readme-ov-file#installation-and-setup) en el repositorio [stellar-etl-airflow](https://github.com/stellar/stellar-etl-airflow).

:::note

Los requisitos de hardware pueden variar considerablemente dependiendo de los datos de la red Stellar que requieras. La configuración por defecto de GCP puede ser más alta/baja de lo realmente necesario.

:::

## Configurando GCP Composer Airflow

Se requieren dos cosas para la configuración y el establecimiento de GCP Composer Airflow:

- Subir DAGs al Bucket de Composer Airflow
- Configurar las variables de Airflow para tu configuración de GCP

Para instrucciones más detalladas, consulta la documentación de [Instalación y Configuración de stellar-etl-airflow](https://github.com/stellar/stellar-etl-airflow?tab=readme-ov-file#installation-and-setup).

### Subiendo DAGs

Dentro del repositorio [stellar-etl-airflow](https://github.com/stellar/stellar-etl-airflow) hay un script de shell [upload_static_to_gcs.sh](https://github.com/stellar/stellar-etl-airflow/blob/master/upload_static_to_gcs.sh) que subirá todos los DAGs y esquemas a tu bucket de Composer Airflow.

Esto también se puede hacer utilizando el [CLI de gcloud o la consola](https://cloud.google.com/storage/docs/uploading-objects) y seleccionando manualmente los DAGs y esquemas que deseas subir.

### Configurando Variables de Airflow

Por favor, consulta la documentación de [Explicación de Variables de Airflow](https://github.com/stellar/stellar-etl-airflow?tab=readme-ov-file#airflow-variables-explanation) para más información sobre qué debería y necesita configurarse.

## Ejecutando los DAGs

Para ejecutar un DAG, solo tienes que alternar el DAG encendido/apagado como se ve a continuación

![Alternar DAGs](/img/hubble/airflow_dag_toggle.png)

Se puede encontrar más información sobre cada DAG en la documentación de [Diagramas de DAG](https://github.com/stellar/stellar-etl-airflow?tab=readme-ov-file#dag-diagrams).

## DAGs Disponibles

Más información se puede encontrar [aquí](https://github.com/stellar/stellar-etl-airflow/blob/master/README.md#public-dags)

### DAG de Exportación de Tabla de Historial

[Este DAG](https://github.com/stellar/stellar-etl-airflow/blob/master/dags/history_tables_dag.py):

- Exporta parte de las fuentes: ledgers, operaciones, transacciones, intercambios, efectos y activos desde Stellar utilizando el lago de datos de archivos LedgerCloseMeta
  - Opcionalmente, esto puede ingerir datos utilizando captive-core pero eso no es ideal ni recomendado para uso con Airflow
- Inserta en BigQuery

<img src={history_table_export} width="300" />

### DAG de Exportación de Tabla de Estado

[Este DAG](https://github.com/stellar/stellar-etl-airflow/blob/master/dags/state_table_dag.py)

- Exporta cuentas, firmantes de cuentas, ofertas, balances reclamables, fondos de liquidez, líneas de confianza, datos de contratos, código de contrato, configuraciones y ttl.
- Inserta en BigQuery

<img src={state_table_export} width="300" />

### DAG de Tablas Base Enriquecidas con DBT

[Este DAG](https://github.com/stellar/stellar-etl-airflow/blob/master/dags/dbt_enriched_base_tables_dag.py)

- Crea las vistas de staging de DBT para modelos
- Actualiza la tabla enriched_history_operations
- Actualiza las tablas de estado actuales
- (Opcional) advertencias y errores se envían a slack.

<img src={dbt_enriched_base_tables} width="300" />
