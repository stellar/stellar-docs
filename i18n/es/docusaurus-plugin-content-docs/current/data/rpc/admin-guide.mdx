---
sidebar_position: 10
title: Guía de Admin
---

El servicio RPC le permite comunicarse directamente con Soroban a través de una [interfaz RPC JSON](./README.mdx).

Por ejemplo, puedes construir una aplicación y tenerla [enviar una transacción](./api-reference/methods/getTransaction.mdx), [obtener ledger](./api-reference/methods/getLedgerEntries.mdx) y [data del evento](./api-reference/methods/getEvents.mdx), o [simular transacciones](./api-reference/methods/simulateTransaction.mdx).

Alternativamente, puedes usar uno de Soroban [SDKs del cliente](/docs/tools/sdks/library), tal como [@stellar/stellar-sdk](/docs/tools/sdks/library#javascript-sdk), que necesitará comunicarse con una instancia RPC para acceder a la red.

## Ejecuta tu propia instancia de desarrollo

Para el desarrollo local, recomendamos [downloading](https://hub.docker.com/r/stellar/quickstart) y ejecutar una instancia local a través de [Docker Quickstart](https://github. om/stellar/quickstart) y ejecutando una red independiente o comunicándose con un live development [Testnet].

:::caution

No recomendamos ejecutar la imagen de Quickstart en producción. Mira la sección [desplegar tu propia instancia RPC](#deploy-your-own-rpc-instance).

:::

La imagen de Quickstart con el servicio RPC se puede ejecutar en un portátil estándar con 8GB de RAM y tiene requerimientos mínimos de almacenamiento y CPU. Es un contenedor único que ejecuta todo lo que necesita para probar una red completamente destacada. Contiene:

- [Stellar Core] - Node software que ejecuta la red, coordina el consenso y finaliza los contadores.
- [Soroban RPC] Servidor - JSON RPC server for interacting with Soroban contracts.
- [Servidor Horizon] - API HTTP para acceder a transacciones históricas y estado de ledger.
- [Friendbot server] - API HTTP para crear y financiar nuevas cuentas en redes de pruebas.

[Núcleo estelar]: https://github.com/stellar/stellar-core
[RPC Soroban]: https://github.com/stellar/soroban-rpc/tree/main/cmd/soroban-rpc
[Servidor Horizon]: https://github.com/stellar/go/tree/master/services/horizonte
[Servidor Amigos]: https://github.com/stellar/go/tree/master/services/friendbot

### Independiente

Para ejecutar una red local independiente con la imagen Stellar Quickstart Docker, ejecute el siguiente comando:

```bash
docker run --rm -it \
    -p 8000:8000 \
    --name stellar \
    stellar/quickstart:testing \
    --standalone \
    --enable-soroban-rpc
```

Una vez iniciada la imagen, puede comprobar su estado consultando la API Horizonte:

```bash
curl "http://localhost:8000"
```

Puede interactuar con este nodo local usando el CLI Stellar. Primero, añádelo como una red configurada:

```bash
red estelar añadir un \ independiente
    --rpc-url "http://localhost:8000/soroban/rpc" \
    --network-passphrase "Red independiente ; febrero 2017"
```

A continuación, generar una identidad única (par de claves pública/privada) y para financiarla utilizando:

```bash
las llaves estelares generan alice
```

:::tip Identidades de solo pruebas

Es una buena práctica nunca usar las mismas claves para probar y desarrollar que usas para la red pública Stellar. Genera nuevas claves para probar y desarrollar y evitar usarlas para otros propósitos.

:::

Ahora que tiene una red configurada y una identidad financiada, puede usarlos dentro de otros comandos Stellar CLI. Por ejemplo, desplegando un contrato:

```bash
despliegue de contrato estelar \
    --wasm target/wasm32-unknown-unknown/release/[project_name].wasm \
    --source alice \
    --network independiente
```

O invocando un contrato:

```bash
contrato estelar invoca \
    --id C... \
    --source alice \
    --network independiente \
    -- \
    hola \
    --a amigo
```

Cuando termines con tu nodo independiente, puedes cerrarlo con <kbd>ctrl</kbd><kbd>c</kbd> (no <kbd>cmd</kbd>). Esto eliminará completamente el contenedor (eso es lo que hace la opción `--rm` del comando `docker`), lo que significa que tendrá que volver a desplegar su contrato y volver a depositar su identidad la próxima vez que lo inicie. Si trabaja con los nodos locales a menudo, puede que desee crear scripts para hacer estos pasos de inicialización más fáciles. Por ejemplo, vea el [ejemplo dapp `initialize.sh`](https://github.com/stellar/soroban-example-dapp/blob/abdac3afdb6c410cc426831ece93371c1a27347d/initialize.sh).

[Testnet]: ../../networks/README.mdx

### Testnet

Ejecutar su propio nodo Testnet funciona de la misma manera que ejecutando un nodo Quickstart, como se describe anteriormente. Necesitará modificaciones menores para algunos comandos. Primero, necesitas `--testnet` para la instancia docker, en lugar de `--standalone`:

```bash
docker run --rm -it \
    -p 8000:8000 \
    --name stellar \
    stellar/quickstart:testing \
    --testnet \
    --enable-soroban-rpc
```

Y querrás configurarlo para usarlo con la bandera `--network` en Stellar CLI:

```bash
red estelar añadir testnet \
    --rpc-url "http://localhost:8000/soroban/rpc" \
    --network-passphrase "Test SDF Network ; septiembre 2015"
```

Reemplaza `testnet` en ese comando con el nombre de tu elección si quieres dejar el nombre `testnet` disponible para su uso con un proveedor RPC público (ver abajo). O haga un uso inteligente de las configuraciones `--global` para proveedores públicos y configuraciones locales (hechas con el comando `network add` no ecorado anteriormente) para nodos locales.

Ahora puedes reemplazar `--network standalone` con `--network testnet` (o cualquiera que sea tu nombre) en todos tus comandos que necesitan una red, como los comandos `deploy` y `invoke` mostrados arriba.

### Futuro

También puede desarrollar sus contratos Soroban contra la red Futurenet. Puedes elegir esto si quieres probar más características de bordes que aún no han llegado a la red de pruebas. Ejecutar su propio nodo Futurenet funciona como ejecutar un nodo Testnet, como se describe anteriormente.

```bash
docker run --rm -it \
    -p 8000:8000 \
    --name stellar \
    stellar/quickstart:soroban-dev \
    --futurenet \
    --enable-soroban-rpc
```

Y querrás configurarlo para usarlo con la bandera `--network` en Stellar CLI:

```bash
red estelar añadir futurenet \
    --rpc-url "http://localhost:8000/soroban/rpc" \
    --network-passphrase "Test SDF Future Network ; octubre de 2022"
```

Reemplaza `futurenet` en ese comando con el nombre de tu elección, si quieres dejar el nombre 'futurenet' disponible para su uso con un proveedor RPC público (ver abajo). O haga un uso inteligente de las configuraciones `--global` para proveedores públicos y configuraciones locales (hechas con el comando `network add` no ecorado anteriormente) para nodos locales.

Ahora puedes reemplazar `--network standalone` con `--network futurenet` (o como lo hayas llamado) en todos tus comandos que necesitan una red, como los comandos `deploy` y `invoke` mostrados arriba.

## Despliega tu propia instancia RPC

Recomendamos las siguientes formas de desplegar su propia instancia RPC:

1. Despliega a Kubernetes usando [Helm](https://helm.sh/docs/intro/install/)
2. Ejecuta la [imagen docker soroban-rpc](https://hub.docker.com/r/stellar/soroban-rpc) directamente

### Requisitos de hardware

Para despliegues que esperan hasta 1000 peticiones por segundo para Soroban RPC, recomendamos al menos 4GB de RAM y al menos un doble núcleo de CPU con un 2. Velocidad reloj GHz.

Para implementaciones que esperan más de 1000-10000+ peticiones por segundo para Soroban RPC, recomendamos al menos 8GB de RAM y al menos una CPU cuádruple con un 2. Velocidad reloj GHz.

Para todas las implementaciones, recomendamos al menos 10GB de espacio de disco/almacenamiento.

| Tipo de nodo | CPU    | RAM    | Disco                                | AWS SKU    | SKU de Google Cloud |
| ------------ | ------ | ------ | ------------------------------------ | ---------- | ------------------- |
| RPC Soroban  | 2 vCPU | 4-8 GB | 30 GB volumen persistente >= 3K IOPS | [c5.large] | [n4-highcpu-2]      |

_\* Asumiendo una ventana de retención de 30 días para el almacenamiento de datos._

### Kubernetes con casco

Si su entorno de implementación incluye la infraestructura de Kubernetes, esta es la forma preferida de desplegar su instancia RPC. Esto es lo que necesitas hacer:

1. Instalar [Helm CLI tool](https://helm.sh/docs/intro/install/) (versión mínima 3)

2. Añadir el repositorio estelar a la lista de repositorios del cliente de timbre, actualizarlo a las últimas versiones publicadas

```bash
helm repo add stellar https://helm.stellar.org/charts
helm repo update stellar
```

3. Despliega la instancia RPC a Kubernetes usando [nuestra carta Helm](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc):

```bash
helm install my-rpc stellar/soroban-rpc \
--namespace my-rpc-namespace-on-cluster \
--set global.image.sorobanRpc.tag=20.0.0-rc4-40 \
--set sorobanRpc.ingress.host=myrpc.example. rg \
--set sorobanRpc.persistence.enabled=true \
--set sorobanRpc.persistence.storageClass=default \
--set sorobanRpc.resources.limits.cpu=1 \
--set sorobanRpc.resources.limits.memory=2560Mi
```

Este ejemplo del uso de cartas Helm destaca algunos aspectos clave:

- Establece el `global.image.sorobanRpc.tag` a una etiqueta desde el [soroban-rpc dockerhub repo](https://hub.docker.com/r/stellar/soroban-rpc) para la versión de la imagen que quieres ejecutar. Vaya a [la página de versiones de software](/docs/networks/software-versions) para encontrar la etiqueta correcta para la versión Soroban que está ejecutando.

- El servidor RPC almacena una ventana de datos recientes de los contadores de red al disco. El tamaño de esos datos varía en función de la red y de sus volúmenes de transacción, pero tiene un rango estimado de 10 a 100 MB. Para asegurar que el pod RPC tenga acceso consistente al espacio de almacenamiento en disco y a través de lectura/escritura, este ejemplo demuestra cómo habilitar opcionalmente la implementación de cartas Helm para usar un `PersistentVolumeClaim` (PVC) de 100MB de la clase de almacenamiento `default` en Kubernetes habilitando estos parámetros de persistencia:

```bash
--set sorobanRpc.persistence.enabled=true
--set sorobanRpc.persistence.storageClass=default
```

Por defecto, esto está deshabilitado (`sorobanRpc.persistence. nabled=false`) y el despliegue RPC usarán almacenamiento efemeral de pod mediante `emptyDir`, lo que probablemente será adecuado para los volúmenes de transacción `futurenet` y `testnet`. Sin embargo, vale la pena destacar la compensación de no tener limitaciones de almacenamiento del clúster aplicado (probablemente por debajo de 100MB).

- Los presets de red están definidos en [`values.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values.yaml), que actualmente establece la configuración de red específica para `futurenet`. Puedes reemplazar este valor por defecto y usar otros archivos `values.yaml` "canned" que han sido publicados para otras redes. Por ejemplo, hay un archivo [`values-testnet.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values-testnet.yaml) para configurar el despliegue del servidor RPC con la red `testnet`. Incluye este parámetro `--values` en tu `helm install` para especificar la red deseada:

```bash
--values https://raw.githubusercontent.com/stellar/helm-charts/main/charts/soroban-rpc/values-testnet.yaml
```

- Configurar RPC para usar otras redes personalizadas puede ser alcanzado descargando el [`values.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values.yaml) localmente y actualizando la configuración bajo `sorobanRpc.sorobanRpcConfig` y `sorobanRpc.coreConfig`. Esto es aplicable cuando se conecta a redes específicas distintas de los archivos existentes como `values.yaml` disponibles, como su propia red independiente. Incluye el `values.yaml` local en `helm install`:

```bash
--values my-custom-values.yaml
```

- Verifique el valor predeterminado de `LimitRange` en el espacio de nombres de destino de Kubernetes para su implementación. `LimitRange` es opcional en la configuración del cluster. Si está definido, asegúrese de que el valor por defecto proporciona al menos límites mínimos de recursos del servidor RPC para la CPU `2.5Gi` de memoria y `1`. De lo contrario, incluya los límites explícitamente en el comando `helm install` a través de los parámetros `sorobanRpc.resources.limits.*`:

```bash
--set sorobanRpc.resources.limits.cpu=1
--set sorobanRpc.resources.limits.memory=2560Mi
```

Incluso si no está desplegando Soroban RPC usando Kubernetes, los manifiestos generados por las cartas pueden ser una buena referencia para mostrar cómo configurar y ejecutar Soroban RPC como un contenedor docker. Simplemente ejecuta el comando `helm template` para imprimir la configuración del contenedor en la pantalla:

```bash
plantilla de yelmo mi-rpc estelar/soroban-rpc
```

### Imagen de Docker

Si usar Kubernetes no es una opción, esta es la forma preferida de desplegar su propia instancia RPC.

:::caution

Aunque tenemos una [Quickstart Image](https://github.com/stellar/quickstart), es sólo para desarrollo local y pruebas. No es apto para el despliegue de grado de producción.

:::

Así es como ejecutar la [imagen docker soroban-rpc](https://hub.docker.com/r/stellar/soroban-rpc):

1. Lanza la imagen en la versión que quieras ejecutar desde [las etiquetas](https://hub.docker.com/r/stellar/soroban-rpc/tags):

```bash
arrastre stellar/soroban-rpc
```

2. Crea un archivo de configuración para [Stellar Core](https://github.com/stellar/stellar-core). Aquí hay un archivo de configuración de ejemplo para Testnet:

```toml
HTTP_PORT=11626
PUBLIC_HTTP_PORT=false

NETWORK_PASSPHRASE="Prueba de red SDF ; septiembre 2015"

DATABASE="sqlite3://stellar. b"

# Validadores de redes estelares
[[HOME_DOMAINS]]
HOME_DOMAIN="testnet.stellar.org"
QUALITY="ALITY="ALTAR"

[[VALIDATORS]]
NAME="sdftest1"
HOME_DOMAIN="testnet. tellar.org"
PUBLIC_KEY="GDKXE2OZMJIPOSLNA6N6F2BVCI3O777I2OOC4BV7VOYUEHYX7RTRYA7Y"
ADDRESS="core-testnet1.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_001/{0} -o {1}"

[[VALIDATORS]]
NAME="sdftest2"
HOME_DOMAIN="testnet. tellar.org"
PUBLIC_KEY="GCUCJTIYXSOXKBSNFGNFW5MUQ54HKRPGJUTQFJ5RQXZXNOLNXYDHRAP"
ADDRESS="core-testnet2.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_002/{0} -o {1}"

[[VALIDATORS]]
NAME="sdftest3"
HOME_DOMAIN="testnet. tellar.org"
PUBLIC_KEY="GC2V2EFSXN6SQTWVYA5EPJPBWIMSD2XQNKUOHGEKB535AQE2I6IXV2Z"
ADDRESS="core-testnet3.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_003/{0} -o {1}"
```

3. Ejecuta la imagen, asegurándote de [montar un volumen](https://docs.docker.com/storage/volumes/) en tu contenedor donde se almacena la configuración anterior. Un ejemplo para Testnet:

```bash
docker run -p 8001:8001 -p 8000:8000 \
-v <STELLAR_CORE_CONFIG_FOLDER>:/config stellar/soroban-rpc \
--captive-core-config-path="/config/<STELLAR_CORE_CONFIG_PATH>" \
--captive-core-storage-path="/var/lib/stellar/cautive-core" \
--captive-core-use-db=true \
--stellar-core-binary-path="/usr/bin/stellar-core" \
--db-path="/var/lib/stellar/soroban-rpc-db. qlite" \
--stellar-captive-core-http-port=11626 \
--friendbot-url="https://friendbot-testnet.stellar.org/" \
--network-passphrase="Test SDF Network ; septiembre 2015" \
--history-archive-urls="https://history.stellar.org/prd/core-testnet/core_testnet_001" \
--admin-endpoint="0.0.0:8001" \
--endpoint="0.0.0.0:8000"
```

#### Configuración

Para producción, recomendamos ejecutar Soroban RPC con un archivo de configuración [TOML](https://toml.io/en/) en lugar de las banderas CLI. Esto es similar a crear un archivo de configuración para Stellar-Core como lo hicimos anteriormente. Por ejemplo, usando [nuestra imagen docker](https://hub.docker.com/r/stellar/soroban-rpc):

```bash
docker run -p 8001:8001 -p 8000:8000 \
-v <CONFIG_FOLDER>:/config stellar/soroban-rpc \
--config-path <RPC_CONFIG_FILENAME>
```

Puede utilizar Soroban RPC en sí mismo para generar un archivo de configuración de inicio:

```bash
docker ejecuta stellar/soroban-rpc gen-config-file > soroban-rpc-config.toml
```

La configuración resultante debería verse así:

```toml
# Admin endpoint to listen and serve on. WARNING: this should not be accessible
# from the Internet and does not use TLS. "" (default) disables the admin server
# ADMIN_ENDPOINT = "0.0.0.0:8001"

# path to additional configuration for the Stellar Core configuration file used
# by captive core. It must, at least, include enough details to define a quorum
# set
# CAPTIVE_CORE_CONFIG_PATH = ""

# Storage location for Captive Core bucket data
CAPTIVE_CORE_STORAGE_PATH = "/"

# informs captive core to use on disk mode. the db will by default be created in
# current runtime directory of soroban-rpc, unless DATABASE=<path> setting is
# present in captive core config file.
# CAPTIVE_CORE_USE_DB = false

# establishes how many ledgers exist between checkpoints, do NOT change this
# unless you really know what you are doing
CHECKPOINT_FREQUENCY = 64

# SQLite DB path
DB_PATH = "soroban_rpc.sqlite"

# Default cap on the amount of events included in a single getEvents response
DEFAULT_EVENTS_LIMIT = 100

# Endpoint to listen and serve on
ENDPOINT = "0.0.0.0:8000"

# configures the event retention window expressed in number of ledgers, the
# default value is 17280 which corresponds to about 24 hours of history
EVENT_RETENTION_WINDOW = 17280

# The friendbot URL to be returned by getNetwork endpoint
# FRIENDBOT_URL = ""

# comma-separated list of stellar history archives to connect with
HISTORY_ARCHIVE_URLS = []

# Ingestion Timeout when bootstrapping data (checkpoint and in-memory
# initialization) and preparing ledger reads
INGESTION_TIMEOUT = "30m0s"

# format used for output logs (json or text)
# LOG_FORMAT = "text"

# minimum log severity (debug, info, warn, error) to log
LOG_LEVEL = "info"

# Maximum amount of events allowed in a single getEvents response
MAX_EVENTS_LIMIT = 10000

# The maximum duration of time allowed for processing a getEvents request. When
# that time elapses, the rpc server would return -32001 and abort the request's
# execution
MAX_GET_EVENTS_EXECUTION_DURATION = "10s"

# The maximum duration of time allowed for processing a getHealth request. When
# that time elapses, the rpc server would return -32001 and abort the request's
# execution
MAX_GET_HEALTH_EXECUTION_DURATION = "5s"

# The maximum duration of time allowed for processing a getLatestLedger request.
# When that time elapses, the rpc server would return -32001 and abort the
# request's execution
MAX_GET_LATEST_LEDGER_EXECUTION_DURATION = "5s"

# The maximum duration of time allowed for processing a getLedgerEntries
# request. When that time elapses, the rpc server would return -32001 and abort
# the request's execution
MAX_GET_LEDGER_ENTRIES_EXECUTION_DURATION = "5s"

# The maximum duration of time allowed for processing a getNetwork request. When
# that time elapses, the rpc server would return -32001 and abort the request's
# execution
MAX_GET_NETWORK_EXECUTION_DURATION = "5s"

# The maximum duration of time allowed for processing a getTransaction request.
# When that time elapses, the rpc server would return -32001 and abort the
# request's execution
MAX_GET_TRANSACTION_EXECUTION_DURATION = "5s"

# maximum ledger latency (i.e. time elapsed since the last known ledger closing
# time) considered to be healthy (used for the /health endpoint)
MAX_HEALTHY_LEDGER_LATENCY = "30s"

# The max request execution duration is the predefined maximum duration of time
# allowed for processing a request. When that time elapses, the server would
# return 504 and abort the request's execution
MAX_REQUEST_EXECUTION_DURATION = "25s"

# The maximum duration of time allowed for processing a sendTransaction request.
# When that time elapses, the rpc server would return -32001 and abort the
# request's execution
MAX_SEND_TRANSACTION_EXECUTION_DURATION = "15s"

# The maximum duration of time allowed for processing a simulateTransaction
# request. When that time elapses, the rpc server would return -32001 and abort
# the request's execution
MAX_SIMULATE_TRANSACTION_EXECUTION_DURATION = "15s"

# Network passphrase of the Stellar network transactions should be signed for.
# Commonly used values are "Test SDF Future Network ; October 2022", "Test SDF
# Network ; September 2015" and "Public Global Stellar Network ; September 2015"
# NETWORK_PASSPHRASE = ""

# Number of workers (read goroutines) used to compute preflights for the
# simulateTransaction endpoint. Defaults to the number of CPUs.
PREFLIGHT_WORKER_COUNT = 16

# Maximum number of outstanding preflight requests for the simulateTransaction
# endpoint. Defaults to the number of CPUs.
PREFLIGHT_WORKER_QUEUE_SIZE = 16

# Maximum number of outstanding GetEvents requests
REQUEST_BACKLOG_GET_EVENTS_QUEUE_LIMIT = 1000

# Maximum number of outstanding GetHealth requests
REQUEST_BACKLOG_GET_HEALTH_QUEUE_LIMIT = 1000

# Maximum number of outstanding GetLatestsLedger requests
REQUEST_BACKLOG_GET_LATEST_LEDGER_QUEUE_LIMIT = 1000

# Maximum number of outstanding GetLedgerEntries requests
REQUEST_BACKLOG_GET_LEDGER_ENTRIES_QUEUE_LIMIT = 1000

# Maximum number of outstanding GetNetwork requests
REQUEST_BACKLOG_GET_NETWORK_QUEUE_LIMIT = 1000

# Maximum number of outstanding GetTransaction requests
REQUEST_BACKLOG_GET_TRANSACTION_QUEUE_LIMIT = 1000

# Maximum number of outstanding requests
REQUEST_BACKLOG_GLOBAL_QUEUE_LIMIT = 5000

# Maximum number of outstanding SendTransaction requests
REQUEST_BACKLOG_SEND_TRANSACTION_QUEUE_LIMIT = 500

# Maximum number of outstanding SimulateTransaction requests
REQUEST_BACKLOG_SIMULATE_TRANSACTION_QUEUE_LIMIT = 100

# The request execution warning threshold is the predetermined maximum duration
# of time that a request can take to be processed before a warning would be
# generated
REQUEST_EXECUTION_WARNING_THRESHOLD = "5s"

# HTTP port for Captive Core to listen on (0 disables the HTTP server)
STELLAR_CAPTIVE_CORE_HTTP_PORT = 11626

# path to stellar core binary
STELLAR_CORE_BINARY_PATH = "/usr/bin/stellar-core"

# Timeout used when submitting requests to stellar-core
STELLAR_CORE_TIMEOUT = "2s"

# URL used to query Stellar Core (local captive core by default)
# STELLAR_CORE_URL = ""

# Enable strict toml configuration file parsing. This will prevent unknown
# fields in the config toml from being parsed.
# STRICT = false

# configures the transaction retention window expressed in number of ledgers,
# the default value is 1440 which corresponds to about 2 hours of history
TRANSACTION_RETENTION_WINDOW = 1440
```

Tenga en cuenta que la configuración anterior generada contiene los valores por defecto y necesita sustituirlos por los valores adecuados para ejecutar la imagen.

### Construir desde la fuente

Las instrucciones para construir el RPC Soroban desde la fuente se pueden encontrar [here](https://github.com/stellar/soroban-rpc/blob/main/cmd/soroban-rpc/README.md).

## Usar la instancia RPC

Puede configurar Stellar CLI para que utilice un punto final RPC remoto:

```bash
red estelar añadir --global testnet \
    --rpc-url https://soroban-testnet.stellar.org:443 \
    --network-passphrase 'Test SDF Network ; septiembre 2015'
```

Y deposite fondos en sus cuentas/identidades con el Friendbot:

```bash
curl "https://friendbot.stellar.org/?addr=$(stellar keys address alice)"
```

Mira el consejo anterior sobre la expansión de comandos (esa es la nota sobre `$(...)`) si no estás usando un shell basado en bash.

:::caution

Cuando interactúe con la red en producción, debe ejecutar su propio soroban-rpc o utilizar un proveedor de infraestructura de terceros.

:::

## Verificar la Instancia RPC

Después de la instalación, valdrá la pena verificar que la instalación de Soroban RPC es saludable. Hay dos métodos:

1. Acceder al punto final del estado de salud del servicio JSON RPC usando un cliente HTTP
2. Usa nuestra [imagen de System Test Docker](https://hub.docker.com/r/stellar/system-test/tags) precompilada como una herramienta para ejecutar un conjunto de pruebas en vivo contra Soroban RPC

### Estado final de salud

Si envía una solicitud JSON RPC HTTP a su instancia en ejecución de Soroban RPC:

```bash
curl --location 'http://localhost:8000' \
--header 'Content-Type: application/json' \
--data '{
"jsonrpc":"2.0",
"id":2,
"method":"getHealth"
}'
```

Debería recuperar una respuesta de estado HTTP 200 si la instancia es saludable:

```json
{
  "jsonrpc": "2.0",
  "id": 2,
  "result": {
    "status": "healthy"
  }
}
```

### Imagen de prueba de sistema Docker

Esta prueba compilará, desplegará e invocará contratos de ejemplo a la red a la que está conectada su instancia RPC. Este es un ejemplo para verificar tu instancia si está conectado a Testnet:

```bash
docker run --rm -t --name e2e_test \
docker. o/stellar/system-test:soroban-preview11 \
--VerboseOutput true \
--TargetNetworkRPCURL <your rpc url here> \
--TargetNetworkPassphrase "Test SDF Network ; Septiembre de 2015" \
--TargetNetworkTestAccountSecret <your test account StrKey encoded key here > \
--TargetNetworkTestAccountPublic <your test account StrKey encoded pubkey here> \
--SorobanExamplesGitHash v20.0.0
```

Asegúrese de configurar la prueba del sistema correctamente:

- Usa una [etiqueta de prueba del sistema](https://hub.docker.com/r/stellar/system-test/tags) publicada que esté disponible con la versión de versión RPC de Soroban implementada. Para cada lanzamiento, habrá dos etiquetas publicadas para diferentes arquitecturas de CPU. Asegúrese de usar la etiqueta que coincida con la arquitectura de CPU de su equipo anfitrión:
  - `docker.io/stellar/system-test:<release name>` para máquinas con CPUs AMD/Intel
  - `docker.io/stellar/system-test:<release name>-arm64` para máquinas con CPUs ARM
- Establece `--TargetNetworkRPCURL` a tu URL HTTP RPC
- Establece `--TargetNetworkPassphrase` en la misma red a la que tu instancia RPC está conectado:
  - `"Probar red SDF ; septiembre de 2015"` para Testnet
  - `"Test SDF Future Network ; octubre 2022"` para Futurenet
- Establece `--SorobanExamplesGitHash` a la etiqueta de lanzamiento correspondiente en el [repositorio de ejemplos de Soroban](https://github.com/stellar/soroban-examples/tags)
- Crear y depositar una cuenta para ser utilizada con propósitos de prueba en la misma red a la que su instancia RPC está conectado
  - Esta cuenta necesita un pequeño saldo XLM para enviar transacciones Soroban
  - Las pruebas se pueden ejecutar repetidamente con la misma cuenta
  - Establece `--TargetNetworkTestAccountPublic` en la clave pública codificada `StrKey` de la cuenta
  - Establece `--TargetNetworkTestAccountMonet` en el secreto codificado `StrKey` para la cuenta

## Monitorear la Instancia RPC

Si ejecutas Soroban RPC con `--admin-endpoint` configurado y [expone el puerto](https://docs.docker.com/engine/reference/commandline/run/#publish), tendrás acceso a las métricas [Prometheus](https://prometheus.io/) mediante el punto final `/metrics`. Por ejemplo, si el endpoint del administrador es `0.0.0.0:8001` y estás ejecutando la imagen Soroban RPC Docker:

```bash
curl localhost:8001/métricas
```

Verás muchas de las métricas predeterminadas de Ir y Procesar (prefijo por `go_` y `process_` respectivamente) como el uso de memoria, Uso de la CPU, número de hilos, etc.

También exponemos métricas relacionadas con Soroban RPC (prefijo `soroban_rpc`). Hay muchos, pero algunos notables:

- `soroban_rpc_transactions_count` - cuenta de transacciones ingeridas con una ventana deslizante de 10m
- `soroban_rpc_events_count` - cuenta de eventos ingeridos con una ventana deslizante de 10m
- `soroban_rpc_ingest_local_latest_ledger` - último libro de datos ingerido
- `soroban_rpc_db_round_trip_time_seconds` - el tiempo requerido para ejecutar la consulta `SELECT 1` en el DATABASE

Soroban RPC también proporciona registro a la consola para:

- Actividad de inicio
- Ingestar, aplicar y cerrar contornos
- Manejando solicitudes JSON RPC
- Cualquier error

Los registros tienen el formato:

```
time=<timestamp in YYYY-MM-DDTHH:MM:SS.000Z format> level=<debug|info|error> msg=<Actual message> pid=<process ID> subservice=<optional if coming from subservice>
```

[c5.grande]: https://aws.amazon.com/ec2/instance-types/c5/
[n4-highcpu-2]: https://cloud.google.com/compute/docs/general-purpose-machines#n4-highcpu
