---
title: Optimizando consultas
sidebar_position: 30
---

Hubble tiene terabytes de datos para explorar—¡Hay muchos datos! Con el acceso a tantos datos a su alcance, es crucial ajustar el rendimiento de sus consultas.

Una de las fortalezas de BigQuery es también su pitfall: tienes acceso a tremendas capacidades de cómputo, pero pagas por lo que usas. Si afinas tus consultas, tendrás acceso a poderosos asientos a la fracción del costo de mantener tu almacén de datos. Sin embargo, es fácil incurrir en costes onerosos si no se tiene cuidado.

## Mejores prácticas

### Preste atención a la estructura de la mesa.

Las tablas grandes son particionadas y agrupadas según patrones de acceso comunes. Limpia sólo las particiones que necesita y filtra o agrega por campos agrupados cuando sea posible.

Unirse a tablas en cadenas es caro. Refrain de unirse en claves de cadena si puede utilizar claves enteras en su lugar.

Lee los documentos en [Ver metadatos](./viewing-metadata.mdx) para aprender más sobre los metadatos de la tabla.

#### Ejemplo - Tipos de Perfilación de Operación

Digamos que querías perfilar los [tipos de operaciones](../../../learn/fundamentals/transactions/list-of-operations.mdx) enviados mensualmente a la Red Stellar. ======== Digamos que querías perfilar los [tipos de operaciones](../../../learn/fundamentals/transactions/list-of-operations.mdx) enviados mensualmente a la Red Stellar.

<CodeExample>

```sql
# Consulta ineficiente
select datetime_trunc(batch_run_date, month) as `month`,
  type_string,
  count(id) as count_operations
from `crypto-stellar. rypto_stellar.history_operations`
agrupa por orden de `month`,
  type_string
por `month`
```

</CodeExample>

La tabla `history_operations` es particionada por `batch_run_date` y agrupada por `transaction_id`, `source_account` y `type`. Los costos de las consultas se reducen en gran medida al podar particiones no utilizadas. En este caso, puede filtrar las operaciones enviadas antes de 2023. Esto reduce el costo de la consulta en 4x.

<CodeExample>

```sql
# Elimina las particiones que no necesitas
select datetime_trunc(batch_run_date, month) como `month`,
  type_string,
  count(id) as count_operations
from `crypto-stellar. rypto_estelar. istory_operations`
-- batch_run_date es un objeto datetime, formateado como ISO 8601
donde batch_run_date > '2023-01-01T00:00:00'
grupo por `month`,
  type_string
orden por `month`
```

</CodeExample>

Cambiar el campo de agregación a `tipo`, que es un campo agrupado, reducirá los costos en un tercero:

<CodeExample>

```sql
# Elimina las particiones que no necesitas
# Agregar por campo agrupado, `type`
select datetime_trunc(batch_run_date, mes) como `month`,
  `type`,
  count(id) as count_operations
de `crypto-stellar. rypto_stellar.history_operations`
donde batch_run_date >= '2023-01-01T00:00:00'
grupo por `month`,
  `type`
orden por `month`
```

</CodeExample>

**Resumen de rendimiento**

Limpiando particiones y agregando en un campo agrupado, los costos de procesamiento de consultas se reducen en un factor de 8.

|                     | Bytes procesados | Costo   |
| ------------------- | ---------------- | ------- |
| Consulta original   | 408,1 GB         | $2.041  |
| Consulta mejorada 1 | 83.06 GB         | $0.415  |
| Consulta mejorada 2 | 54,8 GB          | $0.274  |

### Ser lo más específico posible.

No escriba las sentencias `SELECT *` a menos que necesite que se devuelvan cada columna en la respuesta a la consulta. Dado que BigQuery es una base de datos de columnas, puede omitir completamente la lectura de datos si las columnas no están incluidas en el comando select. Cuanto más ancho es la tabla, más crucial es seleccionar _sólo_ lo que necesitas.

#### Ejemplo - Tarifas de transacción

Digamos que necesitaba ver las tarifas para todas las transacciones enviadas en mayo de 2023. ¿Qué pasa si escribes un `SELECT *`?

<CodeExample>

```sql
# Consulta Inefficiente
select *
from `crypto-stellar.crypto_stellar.history_transactions`
where batch_run_date >= '2023-05-01'
  and batch_run_date < '2023-06-01'
```

</CodeExample>

Esta consulta se estima que costará casi $4!

Si solo necesita información de honorarios, puede filtrar los datos, reduciendo los costos de consulta en un factor de 50x:

<CodeExample>

```sql
# Seleccione sólo las columnas que necesita
select id,
  transaction_hash,
  ledger_sequence,
  account,
  fee_account,
  max_fee,
  fee_charged,
  new_max_fee
de `crypto-stellar. rypto_stellar.history_transactions`
donde batch_run_date >= '2023-05-01'
  y batch_date < '2023-06-01'
```

</CodeExample>

**Resumen de rendimiento**

Hubble almacena grandes tablas. El rendimiento de la consulta se mejora notablemente seleccionando sólo los datos que necesita. Este principio es crítico a la hora de explorar las operaciones y las tablas de transacciones, que son las mesas más grandes de Hubble.

|                   | Bytes procesados | Costo   |
| ----------------- | ---------------- | ------- |
| Consulta original | 769,45 GB        | $3.847  |
| Consulta mejorada | 16,6 GB          | $0.083  |

:::tip

La consola BigQuery le permite previsualizar gratuitamente los datos de las tablas, lo que equivale a escribir un `SELECT *`. Haga clic en el nombre de la tabla y haga clic en el panel de vista previa.

:::

### Filtrar temprano.

Al escribir consultas complejas, filtre los datos tan pronto como sea posible. Empuja las cláusulas `WHERE` y `GROUP BY` en la consulta para reducir la cantidad de datos escaneados.

:::caution

Las cláusulas `LIMIT` aceleran el rendimiento, pero **no** reducen la cantidad de datos escaneados. Sólo los resultados _final_ devueltos al usuario son limitados. Usar con precaución.

:::

Empujar transformaciones y funciones matemáticas hasta el final de la consulta cuando sea posible. Funciones como `TRIM()`, `CAST()`, `SUM()`, y `REGEXP_*` son intensivas en recursos y solo deberían aplicarse a los resultados de la tabla final.

## Costos de estimación

Si necesita estimar los costos antes de ejecutar una consulta, hay varias opciones disponibles para usted:

### Consola BigQuery

La consola BigQuery viene con un validador de consultas incorporado. Verifica la sintaxis de consultas y proporciona una estimación del número de bytes procesados. El validador puede encontrarse en la esquina superior derecha del Editor de consultas, junto a la marca verde.

Para calcular el costo de la consulta, convertir el número de bytes procesados en terabytes, y multiplicar el resultado por $5:

`(lectura estimada de bytes / 1TB) * $5`

Pegue la siguiente consulta en el Editor para ver los bytes estimados procesados.

<CodeExample>

```sql
seleccione timestamp_trunc(closed_at, month) como mes,
  sum(tx_set_operation_count) como total_operations
desde `crypto-stellar.crypto_stellar. istory_ledgers`
donde batch_run_date >= '2023-01-01T00:00:00'
  y batch_run_date < '2023-06-01T00:00:00'
  y closed_at >= '2023-01-01T00:00:00'
  y closed_at < '2023-06-01T00:00:00:00'
grupo por mes
```

</CodeExample>

El validador estima que se leerán 51.95MB de datos.

0.00005195 TB \* $5 = $0.000259. _¡Esta es una consulta barata!_

### Parámetro de configuración de dryRun

Si estás enviando una consulta a través de una [librería cliente BigQuery](https://cloud.google. om/bigquery/docs/reference/libraries), puede realizar una ejecución seca para estimar el total de bytes procesados antes de enviar el trabajo de consulta.

<CodeExample>

```python
de google.cloud importar bigquery

# Construir un objeto cliente BigQuery
cliente = bigquery. lient()

# Establecer ejecución seca a True para obtener bytes procesados
job_config = bigquery. ueryJobConfig(dry_run=True, use_query_cache=False)

# Pasar en la configuración de trabajo
sql = """
    select timestamp_trunc(closed_at, mes) como mes,
      suma (tx_set_operation_count) como total_operations
    de `crypto-stellar. rypto_estelar. istory_ledgers`
    donde batch_run_date >= '2023-01-01T00:00'
      y batch_date < '2023-06-01T00:00:00'
      y cerrado_at >= '2023-01-01T00:00:00:00+00'
      y cerrado_a < '2023-06-01T00:00:00'
    grupos por mes
"""

# Hacer la solicitud de API
query_job = cliente. uery((sql), job_config = job_config)

# Calcular el costo
cost = (query_job. otal_bytes_processed / 1000000000000) * 5

print(f'Esta consulta procesará {query_job.total_bytes_processed} bytes')
print(f'Esta consulta costará aproximadamente ${cost}')
```

</CodeExample>

También hay [plugins IDE](https://plugins.jetbrains.com/plugin/15884-bigquery-query-size-estimator) que pueden aproximar el costo.

Para más información sobre los costos de consultas, lee la [documentación de BigQuery](https://cloud.google.com/bigquery/docs/estimate-costs).
