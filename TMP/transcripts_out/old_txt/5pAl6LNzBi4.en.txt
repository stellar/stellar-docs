so welcome to the Stellar open protocol
discussion in these discussions we
discuss and plan for
changes to upcoming versions of the
Stellar protocol right now we're
focused on project jump cannon which
will bring smart contracts to Stellar in
addition to changes to the Stellar
protocol it will also lead to the
creation of a new smart contracts
platform
and all of the discussion that we're
having about this is being tracked and
and you can participate in it much of it
is happening here on discord in the jump
cannon channel and the drum cannon dev
channel there are also a series of core
advancement proposals or caps that
relate to changes that would enable jump
cannon or the new smart contracts
platform
caps 46 through 55 I believe at this
point they're pretty modular so each one
sort of takes on an aspect of the
changes that need to be made in order to
bring smart contracts to Stellar and we
are working through those modules bit by
bit and discussing sort of the segments
necessary and the changes necessary to
allow those segments to actually come to
life
at some point all of the work that we do
will go through the normal process in
other words caps are sort of
put up in this github repository that's
linked to in the show notes they're
discussed here they're discussed on a
mailing list and they're discussed in
discord and after after they sort of
reach a point where they are stable they
move from being a draft into a formal
acceptance period finally they're
accepted and implemented in a version of
the Stellar protocol and before that
version of the Stellar protocol goes
live validators actually vote to accept
it now we are still fairly well we've
actually made a lot of progress in the
in sort of the jump cannon trajectory
but as of yet the caps that we have in
front of us have not been accepted
there's still a lot of questions and
today we will dig into some of those
questions
if you
who are listening actually have
questions you can leave them as text in
the live chat channel i'll try to keep
an eye on that
we're certainly trying to move this
discussion forward and have
the substantive issues come to light
so
you know we may not be able to answer
all the live chat questions but we
definitely will later
if we can't answer them in the course of
this meeting
so I think everyone is here and I think
we are ready to kick off

today I know that
recently there was a new cap cap 55
fee model in smart contracts I don't
know if it just hit the mailing list
yesterday there were a few comments that
came in I guess I'm going to start by
asking you
nicola if we're ready to discuss that
if that's where we should start
maybe yeah like we can
maybe

like just go over like a quick overview
of like what's going on in that cap and
then

you know like we we don't have to go in
details basically so I'm not sure like
we need to need to have like a lot of
of a pre-reading basically as part of
this
is that
good use of time of people yeah I think
that sounds good also nico if you can
you know obviously a lot of these things
are kind of like normal quote unquote in
the world of crypto but some of these
are
a bit more contentious so I would like
emphasize specifically like the
contentious bits so that we can have a
good old-fashioned argument
all right
let's see and yeah I mean
so do we want to start with this cap
or I think there were there was also
like the events one that cida
opened that is maybe a little more scope
I don't know
yeah I argue let's start with the
siddharth one because it's less
contentious and we'll probably be
arguing this okay great so we'll start
there we'll end up at cat 55 sit down
do you want to yeah
I don't know if
everyone had time to read the cap
document I actually made a fix of this
morning
but I can give a quick overview of it
and
you can
we can discuss after that
so I made I added this this change on
to cap 51 which is the host functions
cap
I just added
the ability for contracts to log
data so I added a contract logs back to
the transaction meta
and as a part of this change I we
also moved the transaction result into
the meta as well
both contracts so both contract logs and
transact transaction results are hashed
and a hash of these hashes are stored in
the transaction result pair which is
this is how you would cryptographically
verify them
and one change I haven't made to the
document yet but we talked about
yesterday was that we're gonna add
another
contract log type where the logs are
only emitted if there's an error
so that's a very high high level
overview are there any questions
I guess like those logs they are like
logs that are like the
equivalent to
events in ethereum

yeah yeah so they well
so the way it would work is

you know that the contracts allow
whatever they want since it gets sent to
core
or write some transaction meta and
horizon can serve them up
in any way you want so I was I would
imagine that you know if you want to
listen to a specific event
horizon would provide that ability
allowing you to you know write
applications that
would hook on to specific events
so one thing that I want to
point out here is that
it basically means that in order to sift
through

data coming in from horizon as as an
ingestor you need to basically
read everything
which may become a lot
as as the network grows in capacity
ethereum has this concept of this
bloom filter that's included in
every ledger header so that you can
actually

get a like a strong indicator whether or
not the smart contract that you're
interested in or the the account that
you're interested in
is actually included or has emitted
events in that specific block
should we consider doing something
similar
yeah I don't think I can't think
off top of my head why we wouldn't do
optimize like that that's that area
I'm sorry I can look into that I got i
don't think I don't see I don't think
I see anything wrong with that
well actually I think it's kind of
maybe like
premature optimization type of situation
like there are probably better ways to
do it than doing this kind of arbitrary
broom filter thing
like
you know I can imagine for example like
we have just made a stream you could
have horizon
telco which which
filters it wants to apply instead of
kind of doing it after the fact
and then the mirror would be a subset of
the meta like maybe like you're not
interested in
ledger changes let's say
maybe you're not interested in classic
transactions you know like all those
things
well you're you're assuming a horizon
here and I think that's like part well
there's always a consumer right of
but if you want something that's like an
application specific consumer
which you do see in other ecosystems
quite a bit
you know if I'm developing a dap
and I want to have a
you know like a stream coming in of my
of my specific information why should i
run a full-blown horizon
rather than some sort of a light client
that could just
you know share
logs that are specifically relevant to
me
and that's again like a subscriber
I don't see why this is like part of
this gap
well I'm not saying I'm not saying it's
part of this cap but I am saying like is
is there a place in the in the protocol
to to optimize for these use cases
because we actually want more people
to run
nodes
and
what we're doing here right now is
that we're like really tying them down
to like the the horizon model
which is consume everything
in just everything
no
like horizon doesn't force you into

consuming everything
you're you're getting the entire meta
and then you filter it yes like is
the is the amount of meta being produced
going to be a bottleneck in the you know
even in the middle
you know
medium
term i
don't think so
like you know xdr is fairly efficient
like I would like to see the actual
performance problems before
picking arbitrary type of
filtering technology
because I don't know what the use cases
are like you're saying they are events
yeah sure then
you just keep advance it's a very small
subset actually of the of the media
stream
is anything that we're adding here in
this cap prevent us from adding a bloom
filter or other sort of strategies in
the future I don't I don't think so but
yeah you can do any kind of filtering i
mean
I think the the
the place where I can see having an
actual bottleneck in the future is the
actual size of the
meta may get too large for like if
you want to run a light lighter node
but then we are getting into
custom logic
in core to kind of
you know filter that somehow
and I think the the best way to do it is
actually like when you're producing it
instead of trying to do it after the
fact like with like
like with the bloom filters
something that I don't see here when it
comes to filtering is
any way to filter beyond the contract so
like
presumably the contract being out of
filter up by the contract

is there because that would probably be
in the transaction matter
but

if a contract wants to
emit
a whole lot of different logs
how would an application filter on those
specifically or or is that just too
granular to micro
well at the moment
you know the
body is an sc val so if you wanted to do
that
you would add the filtering in there
but
you know I think we would discuss this
okay like maybe that's not reasonable
we should add a higher level filters
above the sc bell
something we can consider
yeah like that right now this structure
would make sense here yeah like
originally I thought we would do
something like right now you have this

block type right system or contract
info and basically if it's a
I mean actually for both of them you
probably want to have like an actual
event name right like which is like a a
short symbol of source
right
yeah I think that makes sense
I would I would I would honestly look at
sort of what the subscription patterns
that you see in other smart contracting
platforms are because they they have
explored this this space fairly
extensively and I think it's what a lot
of the sdks really lean on like if
you're writing a dapp it's fairly common
for for it to to latch onto a bunch of
subscriptions so like however however
they're normally doing it we kind of
want to support those patterns
is there more anyone else have thoughts
questions suggestions is it sort of
clear what the next move is for you here
siddharth
you move on to catholic
actually like there is something yeah
that I just thought about that I think
you know from what
I think graydon was
asking in terms of use cases like
like
are there expectations for example that

and that's related to this filtering
question
that
you want to have
proofs of events that do not happen
so like positive proofs are easy like
like like with with the proposal you
have like you know basically like
you can prove that a given ledger had a
specific event inside a space you know
from a generated by a specific contract
what if you want to prove the negative
that is
that a specific event yeah was not
emitted in a ledger
is that like the type of things that
people
try to do in other systems
so yeah that's a question for yeah to
look into the
what happened
the use cases
yeah 55.
let's move on to it
all right
yeah
yeah so 55 is basically like
like trying to layer
fees on top of the various
like
resource
metering that
started to get introduced in the system
so it's kind of a problem is it's a
little bit ahead of that because we
didn't actually
finish all this like I think we have
a the beginning of like gas metering for
example
in cap 46
but there are like other things that are
not covered yet

so yeah this is so the test
disclaimer this gap has a bunch of
open-ended
things
let's see and yeah so so like they
are
where the cap
is
covered there are like several aspects
kind of
maybe like more important to discuss
I think
there's the the first one around
the
classification of resources and having
market dynamics based on those
resource types so this is an area where

if you look at other blockchains it's
actually a mix of things
like some systems
like historically started with just
like ethereum just like gas as being
the
yes I I'm also a gas
in the context in this context meaning

the
kind of this
metric right that allows to to
to kind of
count the cost of a to to execute a
transaction and cost here is kind of a
pretty loose in terms of
definition

like mostly computation but when you do
like when you load like a
like a ledger entry like when you
access ledger you also
pay for for gas
and then yeah like yeah and then so
you have like gas cost which is
this aggregate metric
basically of multiple resource types and
then more recently in ethereum there
have been discussions around
other types of resources that are kind
of interesting
such as bandwidth
and and any other like basically
you have a kind of a
funny
crossroad there that is
do I want to
have a market for

for each of those resources or do I want
to kind of generate like a composite
market
for those so like the
you have like I think in
polkadot what they do is they put a
the aggregation with utilities with a
polynomial function so basically take
all those resource types and then you
assign them

a weight actually it's not even I think
they are linear it's a linear
thing
and then yeah you combine all those
things and you get with your synthetic
I don't remember how they call it weight
I think in
in
over there but like yeah like that's
that's a way to kind of compute this
aggregate gas
so the challenge so to talking about
challenges that comes with those
aggregate models is that it's
actually very hard to discover price of
things
like an example is

if you take a
transaction that does a lot of I o and
very little compute
that is competing with a transaction
that does very little I o but a lot of
compute
like

with those aggregate functions
if try to pay
like 10 times more for example bid more
right for
one transaction

you don't know if you're signaling that
you're that your I o is is what you want
to prioritize or if it's your compute
that you want to prioritize

so
yeah so it basically causes the
overall
prices to have like this uncertainty in
terms of like what should I bid
so that's kind of one of the
the problems with those kind of
aggregate
metrics so with that said with
john cannon like one of the things that
we are doing is we have a very
clean separation between the
different resource types
so io for example
when we read or write the ledger those
are done
basically outside of the
main
execution like you can think of
before applying a transaction before
executing a contract
we load all the ledger entries that this
contract needs and then
it does its thing and then at the end
it produces potentially side effects
that will be applied
as like a post step
that's kind of a logically the way to we
can you can think about this
and the opportunity here for us is
that because we have those
kind of completely separate

we can
actually dis
and and we also do it because of
performance reasons
for parallelism but like because of that
we can

we can actually express those markets
like separately and we can therefore
I I think
have like cheaper
fees overall because you can price
things properly so you don't have to
do like
to kind of articulate inflate for
example inflate the price of
of reading data from the ledger because
compute happens to be expensive which
kind of
would happen with the aggregate model
so
in the proposal what I'm
doing is actually
I have like three categories three
really
brackets for fees
so one is for
gas which is the compute time exactly
you can think of it as execution time in
in our model really because like I said
earlier we we have a
full separation between I o and and
and execution
so this one you can
bid

second
market that i
have in the proposal is for reading and
writing to the ledger so here there is
actually a competition for
there's like there are so many
there's like a you have like
constraints right in terms of a number
like the bandwidth to the disk
subsystem both in reason rights
so because of that you have to
have like
a market for that
and
it is separate right now
and it is separate also because there is
a
interesting fee model for rights that
I'm going to talk about later
let's see and the third category
is actually something that is not a
market it's not really a market there
are dynamic fees
for what I would consider like commodity
on the network so things like

like
producing meta or
data that ends up being stored in
archives
those do not
there's no reason to have like really
competition between transactions
instead we have like limits per
transaction like basically we say
you can only produce
I don't know like a
500k or something of meta right for a
transaction and then
that's your limit and then two
transactions are actually not competing

you know against each other so there is
no
need to
you cannot have a market dynamics there
so
there are actually a few of those
of those resource types and because
there are there is no market you can
actually aggregate them so they end up
in one big bucket of like
like deterministic
fees basically based on the current
state of the ledger plus
plus
yeah like the actual transaction
so those are like the three
three categories that we have in this
proposal
any question at this point on this
i've been going back and forth actually
on this like a
should we or not separate
piece
we could go with one like I said one
market but I think it pushes price
quite a bit too much
for like cheap cheaper
whichever
resource will be cheaper
which is hard to predict
do we anticipate that this will be
difficult for users to reason about to
like understand these different types of
fees and to think about how to set them
so
yes and no I think that's actually one
of the things that's kind of interesting
is that when
we have already in the system a
strong dependency on a pre-flight
mechanism so like before submitting
most transactions to the network like
they will have to go through a
pre-flight endpoint
the pre-flight is the
the thing that basically will allow
people to
compute
gas for example to estimate gas for
for transaction
in addition to that

let's see in addition to that yeah we
have
like I was saying like certain
fees that are like that more like
dynamic because they are based on the
current ledger like for example like
a bunch of those things that I was
saying are
like the price of storage in archive
this is
voted
by the
or determined by the validators
so at before submitting a transaction

you have to know basically what those
parameters are

and yeah so as part of the pre-flight
endpoint you basically get a
an estimate for
your the minimum fee for
for those categories
well in the case of the
non-market-based
resources the the minimum fee is
basically equal to your
or very likely to be equal to to what
you need
in the case of well you do have
markets it's more like today where
you have to decide how much
do you want to over bid based on that
because the minimum fee doesn't
necessarily translate to

to what the market is is willing to pay
so you have to look more at historical
data
but
this is like something we can
yeah that that like
endpoint scan like a horizon can
can can expose right
and having yeah having them tracked as
separate resources in terms of
historical price
allows you have actually something a
little
more stable I would imagine than
if it was like an aggregate
I think the yeah the complexity from
multiple markets comes from
I mean
one of the implications is actually when
we
construct one and say we validators
construct
a transaction set
it's going to be a kind of a
multi-dimensional
nexa problem
which is not great
but that's the but that algorithm would
not be part of the protocol it's more
like
it's
if you have like five seconds to produce
a block here
that's there's so much compute you
can span in assembling the perfect
transaction set
yeah to answer original question justin
at the end of the day the you know
the wallets should have an easy way to
present an like an estimated cost
in
an xlm currency that they can
understand
and they can tell the user hey like
you know this is how much more
you can you can
propose for that like they don't need to
actually understand the mechanics
of how this works yeah it's true that
when you over beat right like you're
doing for example you say I want to to
spend like 10 more
than whatever happened in last few
lectures
that 10 percent you can put it
I mean I imagine pretty safely across
like those different those different
resource types like the ones with
markets
because the assumption there is like
the they are priced
accurately
but I imagine that more yeah if you want
to really save
like a it's hard to predict but like
like if if there are like some of those
like a gas for example becomes very
expensive

yeah you don't like maybe you don't want
to be
as aggressive on
on the other resource types
can I jump in with a couple comments
yeah
so I guess so
one it's

I guess high level it's not clearly what
this adds over
for having like a multi-dimensional
optimization problem over the the
one-dimensional one and the reason is
that sorry I haven't thought about
this all that much but the reason is
that like
when when
at least in the current execution
model sort of
everything like everything executes
everything in one lane is going to
execute sequentially right and so the
main like
resource that's that's truly limited in
like a block is is time right and
it's
it like unless there's some sort of
weird
interleaving going on between like
transaction executions
that's sort of the one-dimensional
resource that we have to optimize anyway
and so it's it's not clear to me why
like
your example of like a transaction that
does lots of I o versus one that does
lots of compute well both of them are
going to take a lot of time if they're
using a lot of one resource and so it's
not clear to me that

at least in the current execution setup
we have

that it's we gain by sort of
allocating some
I don't know
resource to like I o versus some to to
compute as opposed to just like looking
at the whole picture of like the total
end-to-end time of the transaction
that said it it does seem like
we it'd be good to like have some
kind of like price discovery
mechanism for different resources and
certainly like you want like an
overall limit perhaps on like the total
number of ledger entries
and so
I know I don't think
sort of thinking off the top my head
I don't think it's incompatible to have
like a one-dimensional
like
gas market and then like sort of price
markets on each resource in the sense of

transactions could bid like
you know for the the amount of resources
they want to use and like the fee per
resource
and then you do some like filtering step
but that's sort of thinking
very much off the top my head
I don't think we necessarily I guess
high level I don't think we necessarily
have to go to the full multi-dimensional
operation problem
that's but yeah that's possible like
it it just looked like
from
historical
kind of experience right like a like
I o is a huge problem
and
trying to model that as time
is actually making it's actually a
disservice in a way to the to the
network because
you have like
very expensive

from
like this point of view right like
something that's going to suck your your
disk resources that now stole
all your calls right on in a multi you
know parallel
execution model
like store as in you know because
like I said we do all I o
early on and if you're actually maxing
out
your
drive then
you're just stuck
right I mean it makes sense to have

perhaps a limit on

overall io I guess
and then there's yeah the other aspect
actually maybe if
we can you know like
I don't think we're going to
necessarily like close on this
multi-dimensional thing you know now but
like there's the other aspect of yeah
ledger size and rights that is
actually another kind of key thing in
there that
I guess makes

io a little more special also
nico just to go back to
to jeff's point like I understand that
why I o needs to be you know priced
significantly higher
than you know the compute operations
but I don't necessarily understand why
it needs to have like its own market
so the the pricing right that you have
is the minimum price it's not
the market price
like when you
market prices is
like
in the in the ideal
like what is describing the cap is is
trying to be closer to like the ideal
situation where
you can actually construct
a transaction set that's going to
basically be like right at the edge in
terms of the capacity that you have on
your actual you know underlying hardware
so like cpu
and io
for disk
if we lose that visibility
then
you may actually allocate
too many transactions to compute when
then like
you know you
you don't have like basically like the
the
kind of natural way of of of having a

a transaction compete against other
transactions that are paying for
expensive stuff
that's kind of what I'm getting to
like like if you have like
what was it like a good example would be
like

yeah I don't know which one of those
resources would be more expensive but
they are not going to be in the same
order of magnitude let's say like a
compute is the one more expensive at a
given time
so
you have to pay like 10 times more right
or 100 times more than the minimum
fee for for compute
to get to get into the ledger
but your your
your storage price
is also kind of expensive and by bidding
a hundred times you also bid a hundred
times on
storage
and you're basically overshooting quite
a bit
compared to the ideal
model
yeah I think I think the general point
here is just that you cannot that in
reality
it's not the case that there's
there's just
time when a transaction is executing
there there are two different
resources and there are different
contention patterns on them and you
can't trade one for the other the the
the system does not actually trade one
for the other like
if I if I for example submit you know
a hundred transactions every one of
which is doing incredibly cpu and
expensive stuff
that that doesn't saturate the I o
system and there's there's still no
contention on the I o system whereas
if I submit a 100 transactions that are
just doing I o and they're doing no cpu
that doesn't saturate the cpu so they
are really two separate resources and
the point where one of them gets a limit
and can no longer do
transaction processing it
doesn't
represent a limit on the other and vice
versa and so you you you can't trade
between the two of them from from a
market perspective
sorry I'm not quite following something
didn't didn't we say earlier that we
were going to do like all of the sort of
disk reads first and then do the
executions right so that if we have a
lot of disk reads then we have less time
for execution
and so the vice versa
I I sort of understand that there's not
they're not like directly tradable but
they seem
correlated or anti-correlated
well they're they're they're different
devices so like right I'm using the disk
and then I'm doing the cpu right I'm not
using it at the same time
I mean like sure there's different
offers and things but
yeah sure but the the execution
characteristics of each of them are are
different so you you use everyone uses
the same desk and then everyone sort of
farms out to multiple threats
right
I feel like we're talking past each
other I mean yes one goes in in order of
the other the two of them do get added
together
in order to represent the the total time
but you can't trade time on one of them
for time on the other that's what I'm
saying
I think I'm not quite following but
that's okay
nico are there any other
networks and fee systems that
introduce a split
or that work similarly those are like
the two
like the yeah execution time and and
like compute right and and disc are
like the two big things that I isolated
like the right side
I figured it would probably
is probably not needed
so I kind of put a flat fee for the
other ones
like basically like
in a proposal I'm basically saying like
we're okay with you know if you want to
have like
something that
like if you have a transaction that is
very important that happens to emit a
lot of miller for example
but you have to get just over a bit like
crazy on your compute even though you're
not really that's not what it's about
you have to find a way to get it
prioritized

let's see
are we talking about this okay like i
think I kind of wanted to talk about it
here is in your proposal you're talking
a fair amount about state expiry
yeah
before we go into estate expiration
which I know is a big topic I'm still
trying to to reason about like the
kind of the wallet experience and user
experience of having having these like
multiple dimensions for
for for gas like
what is the expected
behavior here for

for wallets
to be clear right like tomorrow like
single dimension or multi-dimension
from a white point of view if you want
to estimate it's the same problem
right like it's like in the single
like a cost model like like you
aggregate everything into one you have
to actually
you have like a function right that
just aggregates
but you do have to estimate your your
bid for each things
so it's not it's actually like a
funny thing that
that you have except maybe the tools you
have for discovering price are not as
great
because it's all implicit
okay so so a wallet can can do a
pre-flight can tell me like the expected
cost
I can

you know
bid
bit over
but how do I know how to divide that
between like the the compute and the I o
well like it's
it's the same in you know like like i
said like it it doesn't that question is
not
a question around multi-dimension versus
single dimension
because that that like if you want to
multi like if you want to say like i
want to pay 10 percent more
for
on top of the market rate right for
storage let's say
because that's where there is contention
you have to know that
that's exactly what you you have to have
like you have to have the yeah market
price for for storage
if you just layer like 10 flat on
everything
you're just going to above overbid
which is maybe okay right like for some
people if the fees are relatively low
you know what's the difference between
you know
you know half a lumen and
two-thirds of aluminum or something i
don't know
like historically we've seen that
like in some situations people are
getting are bidding very high on
on certain
for certain patterns
it would be great to maybe and
forgive me if it's already in the cap
but just like understanding what is the
like the
what's the expected wallet strategy
or client strategy here
when like in terms of user
experience like what do they present to
the user
and what you know what kind of inputs
do they expect from the users
yeah sure
okay let's talk about state exploration
nico
where is it well so state exploration
yeah goes kind of hand to hand with the
model that I have there for storing
data on the ledger so like the in the
proposal it's basically like there are
two parts to it there's the how do you
model
a write
and as in
and so writers can be a
create a ledger entry or an update
and how does it work
like how do we have like the
the right price basically for the cost
of storage

so in the proposal what I what I did is

I basically used as an approximation for

for the cluster storage the
bucket list size so like the

some basically like the
the ledger is organized into those
like 19 buckets
I think it's 19 and then
it you if you
if you
do a
an update
or a create you basically append that to
the
to the very first bucket in the bucket
list
so that's how
basically like based on the
size of the the the total size the or
total size of the ledger

I allocate like a a price function
that kind of looks like an exponential
from fall like basically it starts
with a slow slope
up to
some number let's say you say oh like
validators here kind of determining
those parameters but like you can think
of it as the leaders say oh yeah right
now we are running on
drives with I don't know like
25
gigs or 50 gigs of space right and
they're going to basically set
parameters such that

they don't have to

kind of
buy new drives you know like if
there's too much traffic
so
so the price function is basically looks
in this case like you have like your
normal slope that goes to
in like I don't know let's say you have
100 gig and it would be like
I want to use maybe like the first 80
gig at a
rate
that's going to be

like a good rate but not
not like overly aggressive
and then the last 20 gigs I want to
really slow down like the the growth
so that's right like from 5 looks like
this hockey stick type of shape right
like an exponential
and that's kind of
the model for pricing
growing the bucket list
so
you have that for that's four rights
then the problem is that this is only
like this is like saying okay you
can add to the bucket list but then
like
and and by the way like if you delete
entries

eventually those get collapsed into the
buckets and so the bucket is shrinks
in that model a delete you still pay
for or delete actually because delete
is actually adding a little bit of
of data to the bucket list
so that's like first thing to note
here
and then yeah what I wanted to get
here is
as a kind of
more like a desired property is that i
want the price of storage for people to
to kind of be the same for everybody
regardless if they
signed up for you know created an
account like
two years ago or you know in five years

it should be
over time same cost
and there should be no way to do like
to have like a free ride on the on on
the on the ledger right like so you
shouldn't be able to have like
store
I don't know like
nfts like jpegs whatever
on chain you pay
for for this when storage is cheap
and then now you have like
something that is cheaper than
than even storing in aws right like that
that doesn't make any sense
so this
in with that said
there's then a need for having some way
of
kind of resetting in a way the
the
the price of of storage over time
and the mechanism that I use there is
state expiration
so state expiration here means that
you have to
basically

pay for
market market price of storage

to maintain a ledger entry on on the
like a live in the in the ledger
if you do not pay for this
for those
for this
for your brands basically
you get perched that's kind of the
the choice that I made in this cap
there are a bunch of other ways that
can be done that are actually
mentioned in the recap in the other
approaches
but like the
the
reason this kind of works with the other
mechanism is that basically like if you
if you set a policy for example by
default you have to
pay rent like a refresh every year let's
say

and then
you don't pay your your
your renewal after a year
yeah your the data gets deleted

and yeah so there's this kind of
constant churn I guess on the ledger
which is kind of a new pattern
and
that construction is basically a way to
to guarantee that everybody
in the last year has
has been paying basically something
that is
market rate
how long do you expect the
like the how far in the f like when i
trade a letter entry
how far is the maximum expiration date
that I can choose or that will get
chosen for me in the future this stuff
isn't as far as I can tell it's not
specified in the cap how that works so
right so so right now the cap what it
says is that it do not
so you can renew indefinitely right
like the the renewal
window is determined by validators
so that's why I said it's like a

like you say every year you have to pay
run
right and then every time you write you
do an append
that happen is valid for a
year right that's good that's demo then
isn't there a natural trade-off between
like renewal time and like
fluctuations in this price of storage or
are we expecting like this storage cost
to not
increase too quickly
well it depends like what we've seen
on the on the current network is
ledger size has been increasing actually
rapidly over the last few months
because of
like some
strange token activity
which is not entirely I mean it's
it's not
I mean there are a combination of
factors like
one is yeah like just price of in
crypto assets go you know
going down but also like when they
were
still
pretty high you had like an incentive to
to create more crypto assets so it
basically those things kind of cancel
each other and the growth has been
pretty significant
so I would say like
seeing a growth rate

that
takes you to
yeah something that will be
you want the market basically to kind of
get to an eco equilibrium right like
where

where like
you do not have like like those weird
use cases
appearing on the network if they if they
are like
cheaper than
right now I think on on the on the
network the the the problem we have is
yeah we are cheaper than
aws fault in some situations
isn't there a trade-off here between
not necessarily a trade-off but isn't
there a consequence here that people
will have to go and touch their data
from time to time and
people are procrastinators and like
let's say that like
the expiration date is you know when
you're in the future or something

everybody at the end of that year then
has to go and touch all their data and
there's gonna be a huge log jam to get
it done
well that's there would be a large jump
if everybody creates their stuff at the
same time but you would that's not the
case it's going to be
you know like basically like the thing
that expires in a year is the whatever
happened today right like
at the given date
right but I mean like imagine that today
you have a day with like a lot of
activity like you can look back
historically of Stellar's history and
there are periods when there were like
lots of token creations and stuff you
know there are days when there's
hundreds of thousands of blood draw
entries
created

and then abundant
well many of them are abandoned but like
you could imagine a world where they're
not all abandoned right and then what
happens a year in the future
well nothing well people are
incentivized to come back some time
between here and then I don't I don't
see this as
any worse than the fact that we have to
handle load spikes in general yeah i
mean we have to handle load spikes and
let's bikes may get
replicated but
yeah like what
so what I sketched or what we
sketched actually in the cup which is
you know just a more of a strowman type
of thing because I'm sure we can do
better than that is
is we actually are kind of
ensuring that you do not have like
giant spikes so like it like I think
the spike would be not be because of
situations don't like what you're
what you mentioned because actually
activity from today

if you have like a comp a a
a
a linear like a translation right like
an actual just
shapes right like all this activity gets
translated exactly a year from now I you
don't have a problem I think the
it's just like additional
cost of running a validator right
like it's let's you say okay I need
to
when I set my limits right the number of
rights
I actually have to think about well
actually my capacity in rights is half
of of what I can
add to the ledger because I also need to
delete right
but what can happen is more of a
like

if we have different expiration times
which I actually
kind of
briefly talked about there
is that if you have different expiration
times
you can have actually different dates
that
end up expiring
at the same
date and and that's for that for
those type of situations you have to
have an algorithm that kind of
smooths things out and that doesn't
actually cause the system to
kind of create a gigantic spike you know
at a specific dates
so just we don't have a lot of time but
I just want to ask what like the
biggest question I think that there is
like what is the what's the expected
behavior here like you know these ledger
entries are representing financial
instruments
let's say assets just for simplicity
even though we have like a standard
asset contract
and I'm
you know paddling on
shares of something

is what is the expectations
or what is what's expectations there's
like am I supposed to like once a year
like come and touch this
is like the operator
of this financial instrument supposed to
do that for me
if
you know if you look at
you know various common immutable
contracts like uni swap you know and so
and and I'm holding on like these uni
tokens
what's the expectation here like who's
going to touch these for me
right so in the in the
in the cap I actually let this kind of
flexible like there's a
when you there's actually a special host
function to that you can call that is
basically a
rewrite equivalent to like a rewrite
ledger entry so that you refresh the
that that expiration time
anybody can do that all right like
there's no
it's not I understand that anyone can do
that but who is who do you expect to do
that
well it depends on the type of users
right like like power users probably
don't want to do it themselves like
other situations you know if you're if
you're like you said like this very
passive type of person maybe you
should pay somebody to maintain your
stuff if that's what you really want
in other situations I suspect
if people are not active they probably
should just be using centralized
infrastructure
like you know contracts or whatever that
are a little more
centralized
I think trump's point is that there's a
free rider problem here like imagine
that all the people in this room are
using a single contract right
like
which one of us can touch me all of us
have an incentive to wait until the last
second and play chicken and hope that
somebody else oh like yeah for a short
contract I think that
yeah well it's if it's a shared goodness

like
just
i
that's not what actually what I asked
john I assumed that
like each of us will have like our
own ledger entry
within that contract
so maybe did I not understand correctly
I think I think I'm concerned about
people's money vanishing into thin air
which is completely reasonable these are
balances and we're just going to delete
them that's not that's not super great
I recall there being a proposal to like
have
but what happened to this like when
you have a ledger engine deleted it like
gets
dumped into like some kind of merkle try
and you store the hat root of that try
and then like when I want to bring it
back I can like bring in a proof that
this is what the state was right so this
is actually in the appendix yeah the
alternative section so this there is
actually a very detailed
proposal in ethereum foreign v2
about
this
so the complications from
this archive approach is
is when you want to
so like restoring is actually yeah
like a trivial like I said like you know
you have like a maybe a way to do like a
to just to basically store that that
entry in in inside a
merkle tri of source right and then
you just need to provide the proof for
that
the complexity comes from
when you want to create an entry
because you have to prove that that
entry doesn't exist in historical data
like that was actually archived
and that gets really nasty
very fast right right sure
but like that that like not wanting to
do that doesn't address creating this
question like
what do you do if your money gets
deleted
it's it's it's an event like like
you know if it's a
like with an issue it's like today if
you're on the seller network if you're
sending back to the issuer you know you
basically
burn it you can ask the issuer like hey
sorry I didn't mean to do that but
but I didn't burn it
but like what if you're like
what if it's not like you know an
off-chain issuer that you can appeal to
like what if your unit swap lp tokens
get get deleted what do you do
it's tough yeah like you know
these those are the rules of the network
wait but that's not
that's not a great solution long term
right like we're gonna have a lot of
people
consider the alternative right which is
infinite growth of a ledger
with infinite price which one do you
prefer I mean like
objectively like
if somebody had a million dollars of
like uniform lp shares get liquidated
it would have been better to pay for a
million dollars of storage
so that one person yes but like what
about everybody else
and that person with a million dollars
like if they have that it's kind of like
key management like you you have
procedures to make sure that
you don't lose your million dollars it
is it is currently the case that people
with million dollar balances can in fact
lose them because they can lose their
keys so there's there is there is
something to appeal to here like it is
it is actually possible for you to lose
money just by
misusing the system but
what about the other end of the spectrum
though somebody who doesn't have a
million dollars they have a small amount
of their balance and they're just
constantly eating that up by paying
these fees
to keep their balance alive I mean it
sort of reminds me of like bank accounts
where you're like bank accounts just
disappearing because you're paying all
these fees
yeah they do eventually disappear if you
if you put five dollars in a bank
account and then wait for 20 years it'll
go away
I mean this just points to yeah you're
not stirring your your balance in the
right place
like this is shared infrastructure like
if you don't if you don't use it you
lose it
but nikki you can't just like ignore the
entire
industry that we're in and you know I'm
not pretending that this is not a
problem like people are overlooking this
problem
but
you know it's
I think it will be really difficult to
bring people into Stellar
telling them oh this is the way it works
in selena right so
it's a fader no it's not selena doesn't
actually
do any of this right now they have rant
no they don't they literally don't
charge it right now it's a to be done in
the future feature that no one wants so
they're never actually going to get
around to doing it
they have infinite ledger in memory
they they allow they charge you money to
make to allocate space but they never
actually reclaim it there's no there's
no active garbage collection process
so we're over time at this point and i
think that means that we have to stop i
mean I know that this is an interesting
conversation and there's seems like
there's a lot to say about
the concept of expiration
but I think
we'll push it to next week's meeting and
hopefully have some of this discussion
on the Stellar dev mailing list and here
also in discord in the various john
cannon channels so if anyone is watching
and has thoughts about that feel free to
join the Stellar dev mailing list or to
chime in on the discord here we'll
continue to share work and ideas and
conversations
debates as they happen and we will see
you here again soon thanks everybody
