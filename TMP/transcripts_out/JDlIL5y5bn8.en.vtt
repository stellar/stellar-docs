WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
The first is partial stay archival CAP 62. And then in memory Soroban state for CAP 66 and no Tom I will never stop using in Centric Linux distro cool. So I guess before I get started guess a little bit of background. So Protocol 23 is kind of where the rubber is going to start hitting the road as far as state archival is concerned. So kind of some you know background to you. When soron launched we of course had the interface for state archival with rent and all those sort of stuff with the intention, that eventually entries, that have run out of rent will be archived. And then removed from validators in order to free up space. So you don't have you know the issues, that come with large amounts of state, that have to be maintained and so, that's kind of where we're going. Now today the interface is such, that you still pay rent you still have to issue restores and all the sort of things things. But
00:01:00.000 --> 00:01:59.999 align:start position:0%
the data is not actually yet removed from from validators and so, that's where we're going. So initially the plan was for Protocol 23 was to have what we're calling full State archival and in full State archival what happens is entries once they have run out of rent be removed from the live State. And then they are added to this temporary data store called the hot archive is still maintained by all validators. But it's just a separate database, that just maintains entries, that have been recently archived the thinking being was, that eventually this hot archive would become full and. When the hot archive is full what you would do is you would create a Merkle tree of, that data validators would maintain the Merkel rout. And then delete all the information in the hot archive. And then you just repeat this process iteratively. So essentially you evict entries from the live state to the hot archive state
00:02:00.000 --> 00:02:59.999 align:start position:0%
eventually, that hot archive cach will become full. And then you'll actually delete and remove those entries from the validator. And then the restoration process once an entry has been archived in this way. If it no longer lives in the validator there's like a Merkel style proving scheme to, which you are able to restore an entry back to the live ler state so, that's kind of what we consider full state archival, which is where entries actually get deleted from validators. But thinking about this problem a little bit more and looking at the current metrics of soron it seems, that we're still a little too early for this full C archival I think long term. If you look at smart contract platforms, that have large amounts of State there are significant issues with maintaining all, that state you have lots of you know Hardware requirements on a network like salana to maintain large amounts of cashes. And then you have a networks like Ethereum, that don't have large hard requirements. But they're very slow just due to maintaining these very large
00:03:00.000 --> 00:03:59.999 align:start position:0%
databases. So long term at scale I think it's still very important to have the full State archival solution where entries are deleted from validators. And then restored VI proving schemes. But the reality of the situation is we're not quite there yet and I think, that currently there is less than a gigabyte of soron state currently live Stellar. And so going through all these hoops and adding all this complexity for this proving scheme just to delete a small amount of data isn't really worthwhile at this point and so, that's why for Protocol 23 instead of going the full cival route where we actually delete entries we are doing something called or I'm proposing something called partial SE archival and this is what CAP 62 kind of explains. And so in partial State archival what we do is we do kind of the first half of the full state archival. So we still maintain two different databases on the validator you have the live bucket list, which contains
00:04:00.000 --> 00:04:59.999 align:start position:0%
all of your live state, which is the The Ledger, that exist today. And then you still have a what's called the hot archive, which is a cache of recently archived entries. And so what you would do is, that whenever an entry runs out of rent it would be evicted and removed from the live State and added to this hot archive database. Now the key distinction here with partial stay archival is even, though you still remove things from the live bucket list and add them to to the hot archive you never actually delete the hot archive never becomes full such, that the entries are never actually removed from validators. And so it's partial St archival. Because you are still kind of storing live state in one database and storing archive state in a different database. But you're not actually removing any state from validators. And so I think my current proposal would be in Protocol 23 to implement the partial state archival with the intention of later on in the
00:05:00.000 --> 00:05:59.999 align:start position:0%
future extending this to the full State archival solution the only reason is is, that I think, that for the you can the the size at, which the the hot archive becomes full and becomes deleted is configurable. And so I think we could do something reasonable such as we could Implement on this St archival solution. But we could set the the capacity of the hot archive to something very high like 50 GB such, that it would take a long time and a lot of network activity to actually start deleting State. And then you know. If the network was to grow, that much such, that we had 50 gigabytes of archived sorb on state. Then it would actually make more sense to start deleting State and requiring proofs for all the operational benefit, that we get there and so, that's kind of the current proposal of 62 is we're going to still maintain all information on the validators. But we're just going to move archived information from one database
00:06:00.000 --> 00:06:59.999 align:start position:0%
to a different database. Now the reason, that we want to do this this separation of archive State and live state is, that it actually opens up a large number of optimizations and so, that's what we get into in CAP 66, which is inmemory Soroban State. And so we can. Because we have the system where the live State or the live bucket list holds all classic information and all live soron information and we have a completely separate database, that stores all the archive State now. Because of the rent system and. Because of the the way, that we do write fees you know where a write fee is a function of the total size of the bucket list we actually have a way to put a soft limit on the amount of live soron State at all times the reason being is, that you know with the the current bucketless size. If you were to add enough life State such, that
00:07:00.000 --> 00:07:59.999 align:start position:0%
you go beyond the target bucka size rights become very very expensive such, that the network users are incentivized to allow entries to run out of rent and become archived and so. Because of the way our fee system works we have a way to essentially have a soft limit on the amount of State in the light bucket list at all times. And so what I'm proposing is to change, that fee slightly such, that instead of the Soroban right fee corresponding to the size of the entire live leer of both classic and Soroban entries it only applies to the life soron entries. And so essentially the the buckus target size instead of being a buckus size would change to just be the the life soron Target size and I think this is much more fair given, that classic entries don't actually have to pay rent yet. And so it's a little unfair, that adding classic entries actually changes the WR fees and the rent fees for Soroban and especially as the network exists
00:08:00.000 --> 00:08:59.999 align:start position:0%
today classic State dominates sorant State size. And so changes in sorant usage don't actually really affect soron right fees rather changes in classic usage affect soron right fees. And so by changing the The Bucket List Target size to a Soroban state instead of just all total State we have a much more fair fee system. But what, that also allows us to do is to prioritize live sorup on state above arive state. So what do I mean by, that so. If we change the the way, that we calculate fees to only look at sbon size we can use the fee system to enforce a maximum amount of Life State at any time. So for instance we could set the target soron State size to 1 Gigabyte. And then the fee system would ensure, that there's not much more than 1 gigabyte of Life Sor on state at a given point. Now you could maybe you know go a little bit above, that. If people are willing to pay expensive fees
00:09:00.000 --> 00:09:59.999 align:start position:0%
but the way, that the fee growth works is, that you know you are reasonably capped to a small amount of state and so. Because we have the system where the amount of live sorb on state at any given time is fixed what we can actually do is just store all Soroban state in memory and not have disk access at all and so, that's the current proposal in CAP 66 is to prioritize all live soron state in memory and this is made possible. Because we store live soron state in one database and archived state in a different database. And so by splitting the state into two separate databases we can very easily just iterate over the the live database and store all, that sorab on state in memory and so, that's kind of what's Happening behind the scenes as to what the the validator is doing. Now we're able to do this. Because of the maximum soron State size. If we didn't have this and. If
00:10:00.000 --> 00:10:59.999 align:start position:0%
Soroban LIF state was able to grow unboundedly this would be a very dangerous optimization. Because validators might run out of ram but. Because of the St kival system we can actually fix the amount of Life State. And so there's no runaway R risk. And so we can very reasonably store all soron state in memory. And so there are some changes we need to make to the developer experience and the user experience to make this possible. So first we are going to to change some of the resource types A little bit. So currently today we only have one read resource, which is read bites and read entries and this assumes, that all the information you're reading is on disk. And so what we're going to do or what CAP 66 proposes is to split this into two different resource types. So they're going to be an explicit inmemory read resource. And then an explicit on disk read resource. Now the reason we're doing this is, that even, though all Soroban state is or all life soron state is held
00:11:00.000 --> 00:11:59.999 align:start position:0%
in memory Soroban contracts can still access classic State and classic State needs to be on disk now. Because classic entries aren't subject to State archival they have the runaway Ram risk. And so we can't store classic entries in memory. And so Soroban contracts will still have to pay disk fees for for classic State, that's exist additionally we're only storing live state in memory and so. If you access archive state for example a restore operation. Then you would still have to do dis reads. And so there's a dis vew for, that. But essentially what would be changed is, that you would there would be a network limit for the maximum number of on disk read entries as well as the maximum number of inmemory read entries now, that being said. Because the inmemory reads are a lot cheaper than the on disk reads we can actually pass aot, that savings down to the user. So in this proposal there would actually be no in memory
00:12:00.000 --> 00:12:59.999 align:start position:0%
read bites limit. So essentially the read limit for Life s on state would just completely go away. Because in memory reads are cheap. And so there's no reason to limit, that. Now we would still limit the total number of entries being read. But the bytes being read would not be limited additionally. Because we're not doing dis access there would no longer be a read fee associated with accessing Sor on State and so. Because you you still have to pay like a instruction like CPU count and things like, that to actually process large amounts of data but. Because we're not going to disk there doesn't need to be an explicit fee or resource for, that. And so essentially for live SW on state you don't have to pay for reads and you can read as many bytes as you want you still have to pay for the CPU, though. So it's still an implicit fee. But there's no explicit read fee and so, that's kind of the First Advantage to the inmemory versus on disk resource. Now the second thing this allows us to do is is also
00:13:00.000 --> 00:13:59.999 align:start position:0%
Implement autor restore functionality. And so previously. When we first launched Soroban we weren't sure what the final State archival proof system was going to look like and so. While from a technological or from a technical standpoint there was no reason to require a separate restore operation and a separate invoke host function operation we did, that just to give us flexibility later on in case the proof system turned out to be very involved. But in CAP 57 we've actually outlined a pretty lightweight proof system, that works with invoke host function. And so what we're going to do in CAP 66 is we're going to allow automatic restore. And so what this means is, that you no longer will have to issue a restore operation prior to your invo host function. But actually your invo host function operation will just automatically restore any archive keys, that are in the footprint. And so this you know reduces the transaction
00:14:00.000 --> 00:14:59.999 align:start position:0%
count required reduces fees and should just offer a much better user experience. Now the the way this works in resources, though is, that like I mentioned before the live Soroban state is all cached in memory in one database and archive state is uncached and on dis in a separate database and so. If you call info Coast function and every entry you're using is currently live. Then you would have the free inmemory resource bites and you wouldn't have to pay for dis, that being said. If you're using automatic restore the entries being restored would come out of the disk read bytes and would be charged disk fees. Because again for the the entries, that are archived and live in the hot archive database those do have to be read off a disk. And so I think, that's kind of a kind of at the high level of what we're proposing kind of you know the tldr except been talking for for a
00:15:00.000 --> 00:15:59.999 align:start position:0%
little bit is, that you know C the archived entries live in their own database and live sbond State lives in the separate dat or a live database we are. Then going to Cache all the sbond state in the live database in order to pass, that savings on to you there will be an inmemory rebite limit and an on disk rebite limit in fee. And then finally there will be automatic restore to you know essentially remove the need for the restore operation in most cases. So I guess are there there any questions or any conversation points we'd like to touch on more looks like there's a question in in the this in the chat box. But I think a lot of dApps and extend TTL by default will, that still be necessary necessary ah yeah. So I think. So just. Because we
00:16:00.000 --> 00:16:59.999 align:start position:0%
have automatic restore doesn't mean, that you don't want to still manage your TTL. And so like I managed before. If all the entries, that you're using are currently live. Then what're or. Then you don't have to pay read fees and you have much larger read limits. And so you are still incentivized to pay rent also. So but the issue is. When you restore. something you have to pay right fees for the restoration and you also have to pay discre fees for the restoration. And so I think from a fees perspective. If you're using an entry a lot it's still in your best interest to extend the TTL to save money as you know even BEC just. Because the restore is automatic do not make mean it's free. And so you still have to pay for, that restore and even. If it's the same invo Coast function invoking a
00:17:00.000 --> 00:17:59.999 align:start position:0%
function, that only accesses live state is significantly less expensive than invoking a host function, that has a automatic restore on the front end of, that. So we still definitely want to extent extent TL yeah let's see oh. So for OrbitLens will it be possible to tell in advance whether the entry will be automatically restored during the simulation yes. And so this is kind of more of the implement details, which are included in the cap. But what we're doing is captive core has recently added a couple of HTTP n points for querying Ledger State, that will be used by RPC in order to simulate transactions correctly. And so essentially this endpoint is a high performance you know multi-thread HTTP endpoint, that has a similar performance to a SQL table queries. And so it should be appropriate for for production use cases and what this
00:18:00.000 --> 00:18:59.999 align:start position:0%
endpoint will do is it's a key value search where for every key you provide it it will tell you. If that key exists and then. If it exists it'll give you the value. And then it will also give you meta information about, that key. And so it will give you the Ledger entry it will tell you. If it's live or archived. And then it will also tell you you know what its current TTL value is and. If it's in memory or on disk. And so the captive core endpoint is kind of Ed to be the new kind of entry point for this information. And so you should be able to query the current archival state and the current in memory versus on dis state of any entry directly via captive core again there's also meta, that we're emitting for all these events so. If you wanted to it's theoretically possible to injust meta and maintain the state of Soroban entries, that way but. If you don't want to do, that and create your own SQL table and Pipeline and pipeline you can just use the the captive core htpn
00:19:00.000 --> 00:19:59.999 align:start position:0%
points. So will automatic restore become automatically available for existing contracts yes. So the this is all handled at the RPC level. And so the essentially what's changing is, that with Protocol 23 and this is detailed in CAP 66 specifically is, that we are changing the footprint to have this field where you distinguish in the footprint. If a sorond key is either in memory or on disk. And so essentially what the validators will do is, that for whenever they receive and apply an info Coast function they will look at the footprint and for every Soroban entry, that is marked as being on disk AKA marked as being Arch before running, that transaction
00:20:00.000 --> 00:20:59.999 align:start position:0%
the validator will essentially restore those entries automatically. And so the actual contract and the contract logic will not change, which means, that all deployed contracts are automatically compatible with this. Now the invocations to those contracts will change slightly. Because of the footprint changes. But again this will all be handled by RPC. And so pre-flight will will do all this automatically let's see other questions oh. So ler streaming mode I'm I'm not sure about the context. But but behind enabling or disallowing metast streams on validators versus captive core instances I imagine it has to do with performance reasons where you don't want ingestion to make a
00:21:00.000 --> 00:21:59.999 align:start position:0%
validator fall out of sync and essentially, that config setting is an opinionated way of saying, that validator should be high performance and never get blocked whereas like a watcher node, that's not participating in validation would be more appropriate for observing and adjusting the meta. Because there's not it doesn't depend on a downstream system where. If the meta stream gets clogged. Because the downstream system isn't adjusting fast enough you wouldn't want to lose sync and have a validating node fall off the network. Because of a a downst stream issue cool. So I guess
00:22:00.000 --> 00:22:59.999 align:start position:0%
I'll from George about the CAP mentioned somewhere, that autor restore won't always be possible can you elaborate on these scenarios ah yes okay thank you for pointing this out. So there are a couple of edge cases where an invocation will still require an explicit R operation sorry. And so essentially. Because the inmemory reads are. So much cheaper they don't have limits like the on desk read the on just do. While there there is no read byit limit at all and. While there is an entry read limit the expectation is, that this limit will be significantly higher than the dis limits. And so just for you know example suppose, that a in Protocol 23 the transaction in memory read limit is 40
00:23:00.000 --> 00:23:59.999 align:start position:0%
entries and the on disk read limit is 20 entries. And so say you have like this DEX you know trade, that will access 40 soron entries now. If all of those entries are Al. Then you know the it's within the limits the invocation Works no problem. But say, that all 40 of those entries are archived. Now even, though the inmemory limits are large enough for, that transaction to succeed you can only the the automatic Restorations will come from the on disk limits and so. Because you have to pay disk fees and are subject to the dis limits for the restore operation you can only restore in this example 20 entries automatically even, though you need to have authority to be live to complete this DEX trate operation. And so in this scenario you would need to to still manually submit a restore
00:24:00.000 --> 00:24:59.999 align:start position:0%
operation just. Because the way, that the limits are set you can't fit, that many restores in a single transaction now, that being said especially given some other exciting work, that's happening in 23 we expect to raise limits pretty significantly across the board. And so I suspect, that this Edge case will not affect most transactions it will only affect very expensive transactions, that are doing stuff. And so for instance. If you have a DEX trade and it's trading assets, that are mostly live you won't be affected really you're only going to be affected. If you have like a DEX trade, that's crossing a ton of orders and for some reason all those orders were archived. So you mentioned, that the restore op could be deprecated
00:25:00.000 --> 00:25:59.999 align:start position:0%
because of the automatic restore. But this Ed Case requires you to keep something something like, that around right yeah. So I think I mentioned the cap, that we met deprecate the restore op and, that's just. Because that. If the footprint is automatically restored. Then having both the restore op and the extend TTL op is kind of redundant. Because for instance say, that you just want to restore something you don't actually need two operation types you could just essentially use the extend TTL put all the keys you want to restore in the footprint. And then just set the TTL extension to zero and this is functionally equivalent to the restore up and so. When I mentioned deprecating the restore op I don't mean deprecating the ability to restore transaction or to restore entries via an explicit transaction. But just mean like you know mechanically do we need both the restore op and the extend TTL up could. Now you know in theory at least both do a
00:26:00.000 --> 00:26:59.999 align:start position:0%
restoration as well as extend okay yeah, that makes sense Nico had a question about how the Soroban state size is initialized at upgrade time it's it's not specified in the CAP yeah I think I need to expand on this a little bit more. So I think part of this CAP is, that we are changing the semantic meaning of a network config setting. So so in particular The Bucket List Target size will become the Soroban state size. Now the issue is currently the bucket list is like 11 or 12 gigabytes. And so we all of our network settings are assuming, that the your target size is like 13 gigs. But now the issue is. If we you know do a protocol upgrade protocol upgrades previously have never actually changed config settings and so. If you just do the protocol upgrade all of a sudden instead of your Baseline for fees being 12 gigs with a target for
00:27:00.000 --> 00:27:59.999 align:start position:0%
13 gigs. Because we're only tracking Soroban State your target is still 13 gigs. But now your Baseline is like 400 megabytes. Because there's like a lot less soron State compared to life State. And then you have this dos attack where until you upgrade the network confix settings you essentially have no read or write fees for both in memory and on dis State. And so you could have like a Doss attack where someone writes like tons and tons of temp entries and like you know spams The Ledger for essentially zero fees. And so I think what I'm proposing is, that you know currently there's like an operational lag between upgrades. Because core validators can only cue one upgrade at a time. And so we'd have to get all of tier one to arm for the Protocol 23 upgrade. And then after, that goes through have them all arm for the network config setting upgrade and in between, that time you have free free reads and free wrs, which is a huge security risk. And so what I'm proposing is, that. Because Protocol 23 is semantically changing
00:28:00.000 --> 00:28:59.999 align:start position:0%
what this config setting means the protocol upgrade itself should also change the value. And so you know this is slightly different implementation wise than what we've done previously. But I think it should be relatively straightforward implementation whereas like the Protocol 23 upgrade both semantically changes what the Buist Target size means as well as it resets it to a initial starting value, that's more reasonable given this new interpretation of the data Okay. So we've actually updated settings on protocol upgrades before, that I think we we know, that works cool okay great let's see a couple other questions okay. So for OrbitLens the storage for the hot archive yes. So the hot archive and the live Buck list are both part of ensus. So we need the
00:29:00.000 --> 00:29:59.999 align:start position:0%
hash of, that state. And so for, that reason both of the the live database and the hot archive database are both bucket list DB implementations and, that's just. Because they we have to meriz those structures. Then Buck list DB is pretty fast these days. Now with respect to offering tables to buckless DB we we don't really have any plans to do, that and the reason is it's a very difficult structure to add tables to. So it's a log structured merge tree, which is kind of a a variant of like database used by like rock CB or level DB and it's also completely made inhouse like we didn't Fork rocks or anything like, that. And so kind of we we have it it works very well for query types, that the valers require and it's very efficient at those. But we have to essentially like hand write C++ optimized code for those specific queries and. And so it would be both a
00:30:00.000 --> 00:30:59.999 align:start position:0%
very significant undertaking to allow like you know arbitrary index types for Downstream and it would also probably not be a very efficient database just. Because it's a lock structured merge treat. And so a SQL style index query would not work very well on it. And so I think what I'd like to do with this is you know we've we've for arbitrary key value lookups we have exposed end points, that are on the same scale as SQL queres. But again they're just raw key value stores they're not like you know indexes or you know really tables and I think there's been a lot of work done by the platform folks on like the the CDP and things like, that. And so I think given, that the complexity of the database of Stellar core is increasing a lot and for a variety of reasons we only support Buist DB. Now and no longer support SQL I think, that any sort of raw database
00:31:00.000 --> 00:31:59.999 align:start position:0%
access needs to move more in the direction of utilizing Downstream utilizing met ingestion using CDP and not rely on direct access to course databases just. Because you know nowadays with buck DB the core database is very specialized and is not suitable for generic queries cool I think there's a couple people typing. So I'll let them finish or. If anyone else has any other questions. If not I have a third cap, that I'd like to introduce I'll give it a second. And then we can
00:32:00.000 --> 00:32:59.999 align:start position:0%
move on all right I feel like, that's we've had enough time oh answer about slp1 dial would you mind linking, that question again I'm not quite sure what the slp1 question is oh yeah the new limits sorry yeah cool. So I guess. Now I'd like to move on to CAP 65 the reusable module cache. And so like I mentioned before we were doing all this optimization stuff for a memory State and essentially in addition to saving all the contract data in memory
00:33:00.000 --> 00:33:59.999 align:start position:0%
we can also save all the contract code and by extension all the contract modules in memory. Because you know we have a way of EX of archiving contract instances and contract code, that hasn't paid rep recently. And so with, that I think gr's on the call. If you want come up and talk about CAP 65 I don't think gr's on the call I believe we were going to speak about CFE 65 next week right oh sorry I guess I got a I gave youall a little teaser for next week. Then my apologies got a littlee of the gun. But so yeah. So
00:34:00.000 --> 00:34:59.999 align:start position:0%
I I don't want to steal grain thunder. So I'll just you know leave you with a teaser, that we can you know have lots of this not only helps optimize the the read limits. But also optimize CPU utilization as well. But we'll talk about, that more later all right unless there are any other questions questions we can conclude this meeting thanks great Garen it was a great talk all right thank you and you know the dis. If youall have any more questions or concerns you know there's a couple of discussion tabs on the CAPs or just ping me on Discord