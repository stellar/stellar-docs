WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
Okay I think we have like a couple of things on the agenda. So I think the first one was around please to kind of give a quick update of on where we are. And then like we have a follow-up discussions on the archiving work, that Garen has been working on maybe I can start with yeah face given, that this is like a quick thing. So yeah. So like we have we started to kind of go back to the, that fee cap, that I was looking at the draft it was first put together end of June last year. So it has been sitting accumulating dust for a long time and yeah. So we're actually like looking into this again we started a by doing a couple things. So first one
00:01:00.000 --> 00:01:59.999 align:start position:0%
was around refreshing the CAP to reflect as much as possible where we left it where it left things off in the on the mailing list there's plenty of still of open questions and yeah we're going to go over, that I think the other thing was, that we started a thread on this code on the channels. So yeah at this point it's more like yeah we need to go through and iterate on this before we can give more updates on on what's going to be next on the feed front and basically like the yeah what we're realizing is, that there are a lot of things, that we didn't incorporate yet like there are you know this was written before we had for example the pre-flight in the picture things like, that. So
00:02:00.000 --> 00:02:59.999 align:start position:0%
yeah there will be a for sure a little more a few more changes, that need to have to be done but, that's kind of what we are with this not everyone to kind of spend more time on this just like you know I'm inviting people to to kind of follow the conversations and yeah talk about and maybe help on you know helping us making decisions on this one do we maybe want to keep a free high level overview of what is there and you know what are the open questions we are trying to solve a recent page let's spend more time on the archival proposal yeah I think, that yeah like. So I guess yeah good good point like the type of things we are trying to
00:03:00.000 --> 00:03:59.999 align:start position:0%
big issues I would say, that we need to to kind of converge on on the feed front is on what type of experience we want to expose to contract developers. When it comes to the different markets, that that exist in the system. So we have the different resource types we have a Ledger space we have a compute. So like. When when transactions execute we have a network bandwidth and NBS I think in terms of C markets, that's Canada we have other things, that are related more to external systems. So like. When people for example produce metadata, that. Then gets consumed by systems like horizon or sort of an OPC or yeah like still are
00:04:00.000 --> 00:04:59.999 align:start position:0%
expert in red like uses this kind of of a data stream. So like making it, that people can just spam those other systems is is part of the you know in school for for the the fee schedule. And then what did I forget oh yeah we have also archives what has basically answered mandatory right like published to those history archives. So making sure so, that people don't use, that as they are alternative to S3 or you know like other places, that you know. When people can store data. If they want want to the difference here is, that those history archives are romantic analysis forever after. So there are some constraints there and yeah. So like the type of problems around those this the the model for fees is how do we make it, that we
00:05:00.000 --> 00:05:59.999 align:start position:0%
can balance usability. So right. Now you know people used to do the classic system they have a very simple way to think about things basically you have a base fee for one operation. If your transaction contains more than one you just multiply and, that's kind of your your base in a way to kind of think think about the you know in terms of fee and. If you want to get ahead of other transactions on the network for whatever reason it just increase your your feeling, that's kind of it right and in Soroban like the. Because of the competition between those different resource types, that are open-ended right in terms of consumption and competition. So yeah we're going to need something a little bit better than, that I mean like they're like in
00:06:00.000 --> 00:06:59.999 align:start position:0%
places like in you know Italian they have you know a version of what you can do do with these. So this is a single fee for for everything there are like proposals to make it maybe okay it comes to to different resource types but, that's nothing yet in Italian it is implemented in other on cases like polkadot I think. But yeah, that's why we are kind of trying to get something usable okay I think, that's kind of what I wanted to talk about on the feed from just you know heads up it's coming let's see and yeah we have the next guarant, that I think wanted to give us a little more updates on last time there was like we started to talk about the
00:07:00.000 --> 00:07:59.999 align:start position:0%
archive mechanism, that allows to to save space on the Ledger so, that we can keep the network as cheap as as possible and I think there were a few interesting follow-up conversations, that also happen after, that in this Garden of this goal. So Aaron and give us maybe a little a few updates on what's going on there yeah. So I guess first I want to talk a little or just have some time for questions about the interface, that we talked about last week. And so kind of just like a high level summary of what we went over last time is essentially all sort of on data has this rent fee and so every Ledger or periodically you have to pay rent for keeping an entry live on The Ledger. And then whenever an entry runs out of its rent
00:08:00.000 --> 00:08:59.999 align:start position:0%
balance it could still be in from The Ledger. And then sent to the archive. And so with, that interface we've kind of exposed three different classes of storage these kind of three different types of storage replace today what is currently the storage layer, which is like end dot storage in your smart contract code. And so with these three types of storage Ares we have a unique storage, which is there's only ever one version of the entry, that exists on the bucket list or there's a single version of, that entry on the archive. But never both and this is useful for types of data, that have security concerns such as nonsense or certain types of authorization where there could be security risks and issues. If you have multiple versions of, that entry, that could be restored kind of the the use case here is. If you could think about Implement a nonce where you didn't have this unique storage guarantee you could find yourself where you have a
00:09:00.000 --> 00:09:59.999 align:start position:0%
version one of the nonce in the archive of like say value five. And then version two of the nonce and the archive with a different value. And then you can imagine how a malicious user could restore those entries in such a way, that your knots values out of date and not the correct value, that should be so, that's unique data it's more expensive. Because you have to prove, that something doesn't exist in the archive whenever you create something new. And so there's a little bit of work, that needs to be done. So it's the most expensive data type. But it's reserved for like those security and high-risk sort of entries. And then after unique storage we have what's called recreatable storage, which is a similar in, that recreable storage entries whenever you run out of rent balance also get sent to the archive the only difference is, that recreatable storage might have different versions in the archiver multiple different versions exists at the same time the reason for this is, that whenever you create a recreatable storage entry you don't check the archive to see. If something already exists there. And so say you have
00:10:00.000 --> 00:10:59.999 align:start position:0%
something like a balance, that got archived. And then you go to create a new version of, that after your old key got archived in recreatable storage you don't check the archive. And so you just create a new entry with the exact same key. And so you have this key collision and so, that's it's a little cheaper than unique storage. Because you don't have to check the archive and actually show, that this entry is unique, that could be multiple versions of it. So it's cheaper. But it's not appropriate for security types such as like nonsense or auth where you don't want multiple versions and so, that's unique storage and recreatable storage both of, which can be archived. And then the final type of storage is called temporary storage and this are for short-lived entries. And so whenever a temporary storage entry runs out of rent it just gets deleted it doesn't get sent to the archive. And so temporary storage is an appropriate for sensitive data, that you want to keep around like user balances. But can be useful for data types, that either don't need to live very often like a short-term authorization to let an
00:11:00.000 --> 00:11:59.999 align:start position:0%
address spend your funds for instance or for data types, that can be easily recreated. If they get deleted such as like a payment path or something like, that. And so I think. Now even I think first I just want to open up the floor for questions and to talk about this kind of like a interface and in particular talk about this like three-tiered approach and having three different classes of storage. Because I know, that there's a little controversial and it's definitely a little bit more complex on the current interface just wondering. If there are any questions, that I do have a question. If I can go for it. So this is in my sense this is actually a very good design I like it excuse me I'm wondering in terms of staging this work I it's quite complicated and involved in some some quantity of it is going to depend on
00:12:00.000 --> 00:12:59.999 align:start position:0%
some pretty big components being built out in terms of the archivers, which is fine and and I think we can do some staging I'm I'm in the sense of you know deploying versions of zoroban, that have the interface. But but you know some of it is just defined to do nothing at this point or, that sort of thing I'm a little bit concerned about the unique storage one. Because unique right off the bat has to have these these exclusion proofs in order to do any rights is, that correct yes I think what we could do is we can still do a staging process right. And so I think what this would probably look like in practice is whenever we launch sort button we don't have the archive built out. And so kind of the current plan for you know v0 on launch is to have the interface set. So to expose the unique recreable and temporary storage entry types to the user. And then to charge rent. And so the thing, that won't be
00:13:00.000 --> 00:13:59.999 align:start position:0%
there, though is, that whenever your rent balance goes to zero you won't get delayed and you won't get sent to the archive. Because the archive won't be built yet right. And so I think for Unique data in particular what we probably just want to do is we can still I think we still should launch unique data and recreable storage just so, that contracts can have the correct Paradigm written at launch. But what we can do I think at the implementation level is just the four just special cases right. So like before we have these proofs we can just say essentially like creating unique entries does not require proof of exclusion until we actually provide an interface for those proof of exclusions, that's, that's exactly the part I was asking about is is do you think we would just sort of make it an optional field at first and then. When we rev the protocol the optional field has to have a value in it and we'll Define what, that value is in the future well we could do, that we're actually now, that I'm thinking about it what we could do is just zero initialize through cache and say the root archive hash is null. And then proof of exclusion become
00:14:00.000 --> 00:14:59.999 align:start position:0%
trivial right. Because if your hash is null. Then you're guaranteed, that it's empty. And so I actually think we can provide proofs of exclusion on day Zero actually. If we just Define the the null hash. And so the proof will always be null but, that's about proof. If your root hash is null okay the other thing I'm thinking is, that you want this you want to use uniques for nonsense and it seems like we we update nonsense quite a lot. And so I'm a little bit concerned about sort of an expensive operation, that involves constructing a proof of exclusion has to adhere to every every nonce update certainly I wonder about the nonsense, that are maintained by the auth system maybe Dima could speak to, that I think the third yeah I can I'm sorry. If you don't mind current. So I think guaranteed is using non's example is well just an example of why this problem
00:15:00.000 --> 00:15:59.999 align:start position:0%
matters for the built-in nonsense at least we decided to move forward with a temporary knowns approach or nonsense and signatures basically have some time boundaries and certain the temporary Ledger entries. So they don't even run again and I would say this should be a preferred approach right like I reiterated this several times in the discussions here on Discord, that with the existence of temporary storage it's a really good idea to try benefit from it and try to design for it right like in this case of nonsense right we can have this we need to bump it multiple times and it's not super convenient and stuff and I think the main use case for The Unique storage is really some admin data like you really don't want your admin entry to be taken over by someone just
00:16:00.000 --> 00:16:59.999 align:start position:0%
because it has expired right you have a token contract, that you have insured once a year ago and you have never touched it. But you don't want to wake after a year the rent has expired you don't really want someone else to just re-initialize it. Because the entry has expired. So I feel like this is the main use case and this is a really pretty cold entries. If you think about it right. So yeah I think thinking about this a bit I sort of retract my concern. Because I think you're right, that if. If the system has a well-defined notion of temporary storage with with time limits on it. Then you just time bound anything, that is you you propagate those time bounds to the things, that use, that storage such, that they they would become invalid at the same moment. So I can see, that being quite quite a viable approach I like, that that's good thank you I also once make one additional clarification I think in your original question you said oh do you have to like provide a proof of exclusion every time you update it and, that's not true you only have to provide the proof at creation time
00:17:00.000 --> 00:17:59.999 align:start position:0%
and. Then of course you have to provide proofs. If you run out of rent. And then get sent to the archive. But once it's actually live on the bucket list. Then it says just as. If you're modifying any other entry the proofs only apply. If it's not on the bucket list or. If you're creating something for the first time and so. If it's like again this is just an example. But like. If it's a nonsense, that's regularly used. Then it wouldn't matter it would be very cheap and efficient. Because it would never get to the archive okay and oh I have one other very minor question and this is more of a design like time to hit the thesaurus we already have something in a system called an archive I just feel like we gotta use a different word for this. Because it's just gonna follow up a lot of things, that are already referred our guys yeah maybe like deep State source and like the yeah I don't know what you're gonna call it. But maybe maybe not start with deep state, that might be a tough a graveyard yeah cool. So I guess any other questions about specifically the recreatable temporary. And then unique storage
00:18:00.000 --> 00:18:59.999 align:start position:0%
interface before you move on yes I'd like to ask why like is there any reason why you don't use something like a bloom filter to to check whether the entry already exists in the arch Avenue. Because given given the notion, that we have a bloom filter of all entries in the archive we can probably avoid having multiple versions of archived entries and I think having only one version in the archive and preventing users to recreate some entries, that already exist in the archive makes a lot of sense easier yeah. So we definitely investigated this
00:19:00.000 --> 00:19:59.999 align:start position:0%
a lot and we tried to find a way. If there is a way for validators to store keys or to at least have some knowledge of what's in the archive I think there's a couple issues there. So first the goal of the archival state is to bound the amount of storage, that values need to store and so. If they need to store a key even, though that's you know less than storing the entire data entry, that's still an unbounded storage so, that's issue number one. But I think in particular you'd have to have a set of keys, that grows unbounded and, that's not a great solution. Because especially for sorbonne data types there's a lot of instances where the key is actually as big or larger than the value. Because if you can think like the keys are 32 bytes and I'm not you know super in depth at the current sort of implementation. So correct me. If I'm wrong my understanding is keys are 32 bytes. But the value for instance could be something as small as I can, that's only like four bytes or something like, that. And so I think you're not getting as good cost savings as you think. If you just store the keys business set not to your Bloom filter
00:20:00.000 --> 00:20:59.999 align:start position:0%
question or. If there's a way to store the keys in a more efficient manner the issue with Bloom filters in particular is, that they don't it's very difficult to resize the bloom filter and so. If you say I have Unbound State and say okay we're going to pick a bloom filter, that's one gigabyte large. But in 10 years you need a larger Bloom filter. Because you're getting a lot of collisions and stuff it's impossible to just resize, that bloom filter without having all the values. Because whenever you change the size of the bloom filter you have to rehash everything, that you've added to the bloom filter, which would not be possible for the validators. Because they've thrown all those values away. And so essentially to resize your Bloom filter have to replay history from the beginning of time in order to resurface all the values, that need to be in, that blend filter and additionally there's still issues with Bloom filters. Because they're probabilistic in nature. And so you would still have key collisions sometimes or I guess or not Keyhole rather. Because they
00:21:00.000 --> 00:21:59.999 align:start position:0%
only return they don't return false Nexus I don't think. But then there'd be certain keys, that just based on the probabilistic nature of the bloom filter the validators would think they're in the archive even, though they weren't in the archive and so, that would also be an issue where just based on whatever your hash function is there'd be certain keys, that would essentially be impossible to create. Because the balloon filter thinks they already exist. When they really don't so, that answers your question about the the bloom filter issue in particular the issue in general as well I'm not entirely sure, that the resizing issue is such a huge problem. Because you have an archive where you have all the keys. So once in a let's say 10 years it's possible to organize a resizing using the archives as a source of all the
00:22:00.000 --> 00:22:59.999 align:start position:0%
existing keys and to make like a maintenance for all the validators as for the size of the bloom filter for example I just checked and one billion entries with a reasonable false positives like Point 0.1 percent probability of false negatives takes about two and a half gigabytes. So it's not the clutch and it will be enough for the first billion entries I think it's a reasonable trade-off for to avoid some other complexities as for the false positives case I think since you have
00:23:00.000 --> 00:23:59.999 align:start position:0%
the vehicle tree or some other structure. If it's not set. And so on yet. Then you can probably check whether their hive this is archive contains the given K give given key. If it's let's say it can be a resurrected or something like this maybe I'm missing something. Because I haven't think about this for quite a long time. But do you still think, that using some Bloom filters other probabilistic structures won't help to prevent collisions of entries in archives like maybe there is some other option. Because yeah this point looks like one of the most controversial things about
00:24:00.000 --> 00:24:59.999 align:start position:0%
the archives to me yeah this is a an interesting idea actually I think you mentioned, that perhaps use the bloom filter as like a almost a caching layer for efficiency. And then using the proofs as kind of like a back end in case you get a false positive I'm still not 100 sure. If the false positive case can be avoided right. Because say like from this would be very frustrating from a user standpoint say I have a key or like there's some like deterministic way to like Define cues right. So like I give my address. And then like the address is input, that generates the keys for entries associated with my account right like in a token contract. If one of those keys is a false positive. Then you just won't be able to create any entries based on your your the invoker address right. And so I don't know. If there's a way. If you get a false positive to somehow track the archive and say oh actually, that wasn't
00:25:00.000 --> 00:25:59.999 align:start position:0%
a false positive it really is it really doesn't exist I promise. And so I'm not sure. If there is a way to get around, that case. Because I think like even. If you say like a reasonable false positive rate like a 0.1 percent, that still means, that one out of I think a thousand Keys or maybe a ten thousand keys I might be off by zero or something there. But essentially like one of the Thousand Keys you'll think it's in the archive. When it really isn't, which means, that you are not allowed to create one of the thousand one out of a thousand Keys, which I feel like could be a really significant issue from the user interface perspective do you think, that the time of checking the existence of the key over the Merkel tree or the miracle trios or the structure you plan to use for archive proofs it's like
00:26:00.000 --> 00:26:59.999 align:start position:0%
a really huge time like seconds minutes or even more. Because if it's relatively small. Then checking checking the existence only for conflicting entries and you will be checking computer and entries only. When someone tries to create already existing entry, which actually I shouldn't be as often operation. So like what are the time requirements for checking against the try also I think one clarification here is, that you can't just track against the archive directly. Because the archivers don't participate in consensus
00:27:00.000 --> 00:27:59.999 align:start position:0%
and are not validators they are off chain right. And so you can't just like search through the tree brute force and trust, that the contents are correct. So there has to be some way for the archivers to give a proof to validators, that the validators can. Then validate themselves. And so I think, that's why part of the reason we're using this try model where we can get both proofs of explosion pros of inclusion. Because I think the difference between this case and the Ubuntu case is, that in the Ubuntu case you can trust the the archive in our case we can't trust the archive we have to have some proof via like a hash or something. And so I'm still I think it could be interesting this Bloom filter approach. If we can do the bloom filter and then. If there's a collision maybe say even, though there's a collision in the bloom filter I'm going to go and get a proof of exclusion and, that would perhaps make it more efficient and mean, that in most cases
00:28:00.000 --> 00:28:59.999 align:start position:0%
you can get away with creating entries without a perfect exclusion. But I'm still not sure what, that would look like I guess maybe what we could do is you create your entry you check the bloom filter and then. If you get an error on the bloom filter. Then you go try to get proof of exclusion and. If there's a valid proof of exclusion on chain, that can override the bloom filter, that might be an interesting optimization. But I guess, that's second point for explanation I don't want to steal the time it's definitely like I need to make more research on this and probably we should get back on this in, that sales matter to to instill the time on this question here thanks for the explanation yeah I think, that's a I think it's definitely an interesting idea, that's definitely worth pursuing I guess one question I would ask is
00:29:00.000 --> 00:29:59.999 align:start position:0%
under this proposal of the bloom filter and whatnot this would make all data unique data. And so I'm wondering. If there are use cases where a user might actually want data to be recreatable. So thinking of the balanced use case I'm thinking like is there ever a scenario where it's actually an advantage to have multiple different versions as opposed to only having this strict one unique key per archive per bucket list. So I'm thinking like for instance in the case of balances where the multiple different versions of the balance can just be summed together is, that a Advantage and is there like a do we still want to expose a repradable storage interface. So you can be even faster and I guess like not have to do this like Bloom filter check not. Because of exclusion all, that sort of stuff or is like a strict guarantee one key no collisions powerful and, that we shouldn't expose this interface at all
00:30:00.000 --> 00:30:59.999 align:start position:0%
to tell the truth from my experience there is probably a known cases. When you need several versions of the same major entry in most cases it's even a destructive problem right. Because creating an account or something else may be a huge problem I I'd say, that the use case for like optional recreating or maintaining several versions of the same Ledger entry is a rather and I haven't seen any use cases for this and just one question, that basically from the contract
00:31:00.000 --> 00:31:59.999 align:start position:0%
to host interface tend to point like there is no version in any Video Edit to kind of work around some issues in the current approach. But in general the interfaces, that like you put something into storage and it will stay there quote unquote forever or you know you put it into Temp Storage. Then it will be removed after a certain time period. But you know like even from the period is interface standpoint seriously no case for this multiple entry versions and I think the main reason they are thinking of hiding it is just to kind of save some time and cost just to be able to quickly create entries without proof of Proof Set exclusion. But it's more like an implementation detail, that unfortunately the contract writers would need to worry about in some cases versus like you know something is there things
00:32:00.000 --> 00:32:59.999 align:start position:0%
that can be used as a feature or the contractor correct like. If you need any sort of person you can build it yourself using just or as key. So what not. So I'm pretty sure there is no legitimate case for recreatable storage Beyond it like or requirements to the scalability yeah like maybe I can add, that the reason right. Now the the reason we are looking at this recruitable storage is, that we have token balances, that are interestingly one of the kind of primary use cases for fast at all right and. If we don't have recreatable storage we basically have either Temporaries right entries, which for
00:33:00.000 --> 00:33:59.999 align:start position:0%
balance is a no-go or going with those unique entries and for, that you need troops to create the balance. So the cost of an overhead of just kind of setting up your wallet becomes you know quite quite big for any new token, that's kind of the the problem here is, that I think the overhead of proofs is probably acceptable the first time you kind of create your own wallet on your on the network. But anytime you add a balance for any token it seems, that having this overhead is kind of too much. But yeah maybe we, that's kind of part of this discussion right to see you know are we wrong here yeah. So I guess the the trade-off and with the bloom filter approach where
00:34:00.000 --> 00:34:59.999 align:start position:0%
you know and let's just put aside like the resizing and migration issues for. Now but the bloom filter approach all data is unique. But the false positive rate is the percentage of the time, that you will need to provide a proof of exclusion for creating new entry. And so let's just I guess the trade-off is everything is unique and the interface is easier. But one out of a thousand Creations are very slow and require process of create precept explosion whereas. If we have unique data and recreatable data. Then the unique data is guaranteed to always be fast. And so I guess the trade-off is do we want all data to be fast most of the time or all data creation to be fast most of the time and sometimes to be really slow for the easier user interface or have a more complex user interface where one type of data is always slow to create and one type of data is guaranteed always fast to create I guess, that's the the fundamental trade-off at least in my mind
00:35:00.000 --> 00:35:59.999 align:start position:0%
and, that sounds about right like the thing about the blue tilt the other is, that. If in the context of like a balance right the ID the the key right of, that balance is actually deterministic. So as it's deterministic it becomes kind of attackable unless we can come up with like a cryptographic you know Broomfield of sorts it's it's very easy to basically you know cause certain keys to be to have complex in the boom filter. And then you're kind of back to, that you know even, though it's one in a thousand you know. If you're the one, that is always hit by the one it kind of sucks what. If we utilize B tree index or some other index or like database actually doing this
00:36:00.000 --> 00:36:59.999 align:start position:0%
and besides, that the index itself can reside on the disk and the Fast Cash can be implemented using the bloom filter and the actual track will be carried over the index for example B3 index database handle this I mean the issue with an index is we're getting to, that issue where. If we have any deterministic index like, that we need to store the keys right. And then we have, that same issue of unbalanced State growth and especially restore upon data where the keys can sometimes be significantly larger to the value. So I think any like deterministic data structure we can't get back into, that issue of we have unbalanced State versus defeats the purpose of the archive in the first place yeah I just want to remind way basically we still need to maintain
00:37:00.000 --> 00:37:59.999 align:start position:0%
consensus and we cannot just like randomly update boom shooter for example right it has to be a part of consensus. So it would need to come up with some way to Hash it quickly and add it to the CP values and make sure it's archived properly you know significant amount of work and I mean you could say, that the keys are stroke in The Ledger forever. And then you build some sort of index on them blockchain. But then you know yeah it kind of no longer fulfills the requirement of having limited Venture state cross, which we wanted to fulfill. So it's kind of an issue and yeah, that for what source like it was my first city as well like what. If we just throw keys in The Ledger. But yeah, that unfortunately kind of doesn't scale as well yeah. So I think the Bloom
00:38:00.000 --> 00:38:59.999 align:start position:0%
filter with a proof of exclusion fallback for false positives it's an interesting idea I think we probably have some technical homework to do there. So I think it's all right for everyone to move on to the second topic I'm not hearing any objection Celtic as yes. So kind of slipping away from the user interface. Now talking about how the archiver interface will be set up. And so currently there are kind of like two proposals one where we have an archive interface, that's a functions similar to capture how Horizon functions. Now where you go to a specific archiver you have some URL endpoint. And then you query, that endpoint with the keys you want to be archived. And so this is like a a model similar to what we have today with Verizon some of the pros there's pros and cons what are the cons, that you have to have like a personal relationship or at least know an archiver to go to. And then it's not super clear how we
00:39:00.000 --> 00:39:59.999 align:start position:0%
could incentivize or monetize this sort of interface perhaps you would have to like pay a monthly subscription to the archive or perhaps you'd have some relationship where you like pay your archive or per entry lookup or something like, that. But it's not super clear how we incentivize people actually brought in archivers in this setup. So the second scenario is where we have kind of what I'm referring to as the archive miners kind of stealing the minor terminology from Bitcoin. So essentially how this would work is instead of acquiring an archiver directly whenever you need a proof request on chain. Now this could either be implemented at the protocol level where proof request is an operation or this might also be able to be implemented by like a first party smart contract but, that's not really important right. Now but essentially you would just submit an operation, that requests an archival proof. And then by submitting this
00:40:00.000 --> 00:40:59.999 align:start position:0%
operation you'd submit meta information, that archivers could. Then adjust and, that's how the archivers would know about the requests and as part of this operation you would say the key you want to be proven the type of proof select proof of inclusion proof exclusion. And then also a reward and this reward would be variable and you would be the would be at the user's discretion as to what to set this reward to. And then this operation this request would go on chain and the metal would be a submit. And so an archival within Injustice meta. And then pick what requests they want to service. And so they could reservice the request, that has the highest reward first. And then they construct the proof with the information, that they store. And then they submit the proof another operation. And then the proof itself becomes an entry on The Ledger. And so once, that proof has been submitted and validated and the proof is on Ledger. Then the proof or the proof request is
00:41:00.000 --> 00:41:59.999 align:start position:0%
deleted and the reward is given to whoever submitted the correct archival proof first. And so this is I think a better interface. Because it has a clearly defined incentive structure and also doesn't require any personal relationships with an archiver. And so you don't have to have a URL, that you talk to or you don't have to have a relationship with some company, that you pay monthly to pay some subscription fee in order to access the archive it also allows archivers to freely enter and exit the network as they please and also by having a variable reward, that the user can set you can also have essentially like a built-in supply and demand Dynamics where, that price fluctuates over time depending on how many people want to restore archived entries and how many archivers want to service archives. And so I guess generally speaking what are your thoughts on the two approaches and kind of the leading approach being this I'm submitting proof request to the
00:42:00.000 --> 00:42:59.999 align:start position:0%
chain. And then having archivers read the chain. And then submit the proofs how do we feel about, that have you ever speak let's see. So like the the thing I'm thinking of right in terms of like meaningful viable product I'm thinking the having a way to talk directly to archives is is kind of a foolproof the approach where you use the on-chain state I think I mean it's I think there is like good potential there I think it's it's going to be fairly tricky to get this right the reason being, that basically your. So you have you're not creating like intrinsic
00:43:00.000 --> 00:43:59.999 align:start position:0%
value right to certain transactions, that are being published on the overlay and. Therefore a like a board of source right can can look at this overlay traffic and don't run take the data right, that is the primage right the proof and from the the archival, that actually did the work and benefit from the academy. So I think there is like an interesting problem there to solve in terms of like how can you safely disclose the the proof to the network without being from sinus right interesting some entities, that can be signing it refining well the issue, that was signing is how can like the mental not just
00:44:00.000 --> 00:44:59.999 align:start position:0%
like take your proof. And then sign with its own address. And then submit as. If it was the originator yeah, that's fair I'm not sure and I think there might be ways to do it right like it's a maybe what you you it's like a multiple multi-step thing right where you you. Because you were first to disclose let's say the harsh of the proof before you actually disclosure proof. Then you're the one you know. If it's a contract, that's doing the, that work. Then we can basically give the the first you know first one the benefit I mean at the same time like a yeah maybe a bot can I mean it becomes
00:45:00.000 --> 00:45:59.999 align:start position:0%
kind of a a cat and you know a nice game right like where you yeah how to do this safely yeah I think in the front room like. Because you know yours are like super nice people, that maybe on I mean not trying to game the system yeah I thought like the front running I did was just to like. If they're the same proofs in the same block just or randomly pick one. But I didn't think about this proof stealing case in front running by stealing proof. So this is definitely an interesting issue to think about. But I still like the model where you don't have to have a relationship with the archive for a couple of reasons. So first it's likely, that especially you know. If the archive systems run for a long time archivers will not store the entire archive I think it's a good idea to let the archive pick and choose how much or
00:46:00.000 --> 00:46:59.999 align:start position:0%
how little of his or of the archive history they want to store. And so I can see an archiver, that only stores the last five years one, that stores the last 10 years. And then one, that's like a more expensive on the stores like the last 50 years for instance I think. If you have to like individually query an archiver you have this weird interface where for things, that are three years old you can maybe query a cheap archiver. And then for things, that are older you have to change your URL or something like, that to Target like a different archive, that has more history state or something like, that. And so I think there's still some some interface issues with having to talk to the archiver directly. But at least on the top of my head I don't have a great solution to the stealing proofs thing I think, that's the challenges yeah I was going to say like the I think it's it's a we should definitely be looking into those mechanisms, that are like a little more distributed right from a you know Discovery Point of View
00:47:00.000 --> 00:47:59.999 align:start position:0%
I think all it means is, that we use we we have the proper semantics on network to allow for for doing this. So like I think for example like the thing where we have proofs, that are usable independently of using you know the entries yeah I think this is like a a key property, that we need to have right. Because I know like one of the earlier drafts was requiring people to submit proofs in the same transaction, that they were going to use the you know the actual and actually restore the entry and obviously this this would not work in you know would not enable the type of of scenarios yeah I definitely like having the or the proofs themselves be Ledger
00:48:00.000 --> 00:48:59.999 align:start position:0%
entries I just I think we just need to find the the best way to to make sure the system isn't getting it or gained yeah there's a. Because there's not a clear solution. Because I mean you could submit say like before you submit the proof. If you submit the hash to say hey I was here first. And then submit the proof of the next Ledger. But then you could open yourself up to Dos attacks for a malicious user could just generate a bunch of dummy proofs. And then submit them for every archive request. And then archivers would not want to service those requests. Because something's already spoken for it and issues like, that. So so I think we need to think about. But I definitely like I think we definitely should have the proofs on chain like you mentioned and we should see. If there's a way to solve this issue in a way, that makes sense. If you have a you know a few more minutes left was there like some other topic, that you wanted to cover as part of this yes I guess one other
00:49:00.000 --> 00:49:59.999 align:start position:0%
question, that's kind of, that we don't have a great solution for and this is one, that we need to figure out here pretty soon. Because it's required to launch for v0 is how to bump rent. And so right. Now whenever you create an entry it's created and initializes rent balance to some amount. But there doesn't seem to be a great way in order to bump rent and to actually increase, that rent balance. And so kind of the initial thought was Hey whenever you access something increase the rent and, that's way, that the things, that are accessed most commonly automatically have their rent increased and so. If you access something a lot it will most likely be on the buy policy and we'll have to unarchive it. Now for read write items it's easy. Because you have to rewrite the entry anyway. So you might as well bump the rent. However it's not clear how to bump rent on read only items. So for instance. If you have auth say like an auth record, that's almost never changed. But is read often
00:50:00.000 --> 00:50:59.999 align:start position:0%
you would want to bump the rent on reads so, that you wouldn't have to constantly unarchive it the issue is. Because the way the bucket list is structured there's no way to bump rent without rewriting the entry. Because essentially the way the bucket list is structured entries and buckets are immutable. And so in order to update an entry it's not like SQL you can just go to the entry. And then change a value you have to completely rewrite the entry. And so we wouldn't want to bump rent on every read. Because then we're implicitly at the systems level turning reads into read writes, which we don't want to do. And so I guess in the read write case we obviously want to bump rent. But I was wondering. If there are any ideas as to what to do for like read-only data and how to handle rent in, that regard well simple way I've been thinking about is well you just said with online. But you
00:51:00.000 --> 00:51:59.999 align:start position:0%
know I'm exposed just some contract function but, that's free tried access to the entries, that you want to bomb and nothing else, that will basically you know to call just as any other contract function to bounce around but, that maybe can be generalized to you know post functions, that allows you action arbitrary entries without accessing them so, that you know you don't need to maintain any invariants in terms of like only the contract can modify its own data I guess they can all agree, that one pins or end is always positive right. So anyone can do, that. And then you know just kind of cost functions it takes a bunch of majorities catches them at print right supposing pumpkins rent by whatever mechanism they come up this
00:52:00.000 --> 00:52:59.999 align:start position:0%
which again is not super pretty but, that makes it possible to do the pump without touching the contract code and increasing inside guys. And so on. So you know it's a basically a generic way to maintain your contracts I think it's maybe viable yeah I think the host function could be good I think the only issue with, that is key Discovery is still an issue. And then it might be difficult to determine what keys you need to bump around for in the host function. But perhaps one another thing is we could expose a explicit rent bump at like the storage interface. And then also make the current rent balance readable in the smart contract. So I could imagine something like maybe a common Paradigm for read-only data would be
00:53:00.000 --> 00:53:59.999 align:start position:0%
like. If you have an off entry whenever you read it you say you also have a check to see. If the rent balance has fallen below some level and. If so. Then you call Rent bump on, that value. So I guess or I don't know. If that would be possible then. Because or none of, that would be possible. And so then essentially there's a g path and an expensive path and so. If your rent is above the value. Then pre-flight would put, that entry in the read-only data set but. If your rent is below a value. Then PreFlight would put it in the read write set and bump the rent. And so I think, that might be also another possible solution for for often like read-only data I mean, that's kind of possible. But doesn't solve it for everything like imagine again your token right and it has the you know an admin and let's say you don't mean this token much. So you don't touch the admin entry frequently. And then it would still expire right you know the admin
00:54:00.000 --> 00:54:59.999 align:start position:0%
functions frequently enough. So you know for the important things you it sure needs some sort of manual tracking and you still need to understand, which entries need to be updated they I'm sure we can avoid this food just. Because you know some interest may be rarely accessible well I think, that's okay actually. Because if you don't access admin very much and whatever you access it you need to restore your admin entry I think, that's okay I think the case I'm talking about is. If you have some really value, that you access a ton, that still gets archived all the time yeah. But but my point is, that you know you can still forget to call it. So I mean rely on automatic bumps is not going to always work and another thing is, that imagine you know oh maybe it's not a super good example. But I I've been just thinking about who who pays for the bump right it's always a source account who pays for the bumps
00:55:00.000 --> 00:55:59.999 align:start position:0%
and it would be a bit weird like. If every once in a. While like some transaction to your contract suddenly becomes more expensive. Because you need to bounce around. But you may be bump into rent of some entries, that has nothing to do with your account specifically like for example let's say you have some found, that stores some sorry some contracted Source some State saying it's like never going to change right and it's always free download. And then there is no clear owner of the token paywrites they're not owned by any address or anything. So what would happen is like every once in a. While someone who trades with this liquidity pool will need to pay for their inbound I was a token payer record
00:56:00.000 --> 00:56:59.999 align:start position:0%
which is a little bit awkward right. Because I just want to trade why would it charge me more and you know I. Then draw all the incentives to kind of try to gain this and president not submit transactions until someone else has pumped you know I mean it's definitely viable on paper. But it just leads to distribute situations for you know you're in a bumping rent on the some interest you can respond to know about right I guess it also depends on the amount. Because it's well enough and it probably doesn't matter but. If it's high enough. Then suddenly it becomes pretty annoying for the user who ended up paying it yeah thank you I agree like it we probably need to think about those couple of angles like the people, that want to kind of maintain their run balance on those
00:57:00.000 --> 00:57:59.999 align:start position:0%
like read-only type of items, that and. When I say people here could be a contract, that tries to do like like an mm right, that wants to kind of ensure, that it keeps alive its own thing at the same time there are probably scenarios where you want to kind of do, that from the outside in some way right. Because you don't want every contract I mean yeah in the up cases where this is going to not work like. If nobody is using it or and you need to revalue it or I don't know anyways we have time. So thank you everybody let's continue those conversations actually we should probably create like explicit threads on the you know on the dev Channel about those topics. And then yeah we'll keep going thank you again