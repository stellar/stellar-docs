#!/usr/bin/env python3
import argparse
import json
import os
import re
import sys
from typing import List, Optional

from http.cookiejar import MozillaCookieJar
from requests import Session

from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
    VideoUnavailable,
    CouldNotRetrieveTranscript,
)

YOUTUBE_ID_RE = re.compile(
    r"(?:v=|\/)([0-9A-Za-z_-]{11})(?:\?|&|\/|$)"
)

# Global transcript API client (initialized in main)
TRANSCRIPT_API: Optional[YouTubeTranscriptApi] = None

def extract_video_id(url_or_id: str) -> Optional[str]:
    s = url_or_id.strip()
    if len(s) == 11 and re.fullmatch(r"[0-9A-Za-z_-]{11}", s):
        return s
    m = YOUTUBE_ID_RE.search(s)
    return m.group(1) if m else None

def sanitize_filename(name: str) -> str:
    name = name.strip()
    name = re.sub(r"[^\w\-. ]+", "_", name)
    name = re.sub(r"\s+", " ", name)
    return name[:180] if len(name) > 180 else name

def read_video_list(path: str) -> List[str]:
    vids = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            vids.append(line)
    return vids

def choose_transcript(video_id: str, lang: str, prefer_manual: bool):
    if TRANSCRIPT_API is None:
        raise RuntimeError("Transcript API not initialized")
    transcript_list = TRANSCRIPT_API.list(video_id)

    # Prefer exact language if present (manual > autogenerated if prefer_manual)
    # Fallback: try autogenerated if manual not available.
    manual = None
    auto = None

    try:
        manual = transcript_list.find_manually_created_transcript([lang])
    except Exception:
        pass

    try:
        auto = transcript_list.find_generated_transcript([lang])
    except Exception:
        pass

    if prefer_manual and manual is not None:
        return manual, "manual"
    if auto is not None:
        return auto, "auto"
    if manual is not None:
        return manual, "manual"

    # If exact lang not found, try any available transcript in that language family
    # or just pick the first available transcript.
    try:
        # best effort: first transcript
        first = next(iter(transcript_list))
        return first, "unknown"
    except Exception:
        raise NoTranscriptFound(video_id)

def init_transcript_api(cookies_path: Optional[str]) -> None:
    global TRANSCRIPT_API
    if TRANSCRIPT_API is not None:
        return

    session = Session()
    if cookies_path:
        jar = MozillaCookieJar()
        try:
            jar.load(cookies_path, ignore_discard=True, ignore_expires=True)
            session.cookies = jar
            print(f"Loaded cookies from {cookies_path}")
        except FileNotFoundError:
            print(f"Warning: cookies file not found at {cookies_path}, continuing without cookies", file=sys.stderr)
        except Exception as e:
            print(f"Warning: failed to load cookies from {cookies_path}: {e}", file=sys.stderr)
    TRANSCRIPT_API = YouTubeTranscriptApi(http_client=session)

def export_one(video_id: str, out_dir: str, lang: str, prefer_manual: bool, also_json: bool):
    try:
        transcript_obj, kind = choose_transcript(video_id, lang, prefer_manual)
        # If transcript is not in requested language, try translating if possible
        if transcript_obj.language_code != lang:
            try:
                transcript_obj = transcript_obj.translate(lang)
                kind = f"{kind}_translated"
            except Exception:
                # Keep as-is
                pass

        items = transcript_obj.fetch()
        text = "\n".join([i.get("text", "") for i in items]).strip()

        base = sanitize_filename(f"{video_id}_{lang}_{kind}")
        txt_path = os.path.join(out_dir, f"{base}.txt")

        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(text + "\n")

        json_path = None
        if also_json:
            json_path = os.path.join(out_dir, f"{base}.json")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(items, f, ensure_ascii=False, indent=2)

        return {"video_id": video_id, "status": "ok", "txt": txt_path, "json": json_path}

    except (TranscriptsDisabled, NoTranscriptFound) as e:
        return {"video_id": video_id, "status": "no_transcript", "error": type(e).__name__}
    except (VideoUnavailable,) as e:
        return {"video_id": video_id, "status": "unavailable", "error": type(e).__name__}
    except (CouldNotRetrieveTranscript,) as e:
        return {"video_id": video_id, "status": "blocked_or_rate_limited", "error": type(e).__name__}
    except Exception as e:
        return {"video_id": video_id, "status": "error", "error": repr(e)}

def main():
    ap = argparse.ArgumentParser(
        description="Export YouTube transcripts for one or many videos to .txt (and optional .json)."
    )
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--video", help="YouTube video URL or 11-char video ID")
    g.add_argument("--list", help="Path to a text file containing one video URL/ID per line")

    ap.add_argument("--out", default="transcripts_out", help="Output directory (default: transcripts_out)")
    ap.add_argument("--lang", default="en", help="Language code to prefer/translate to (default: en)")
    ap.add_argument("--prefer-manual", action="store_true", help="Prefer manually created captions if available")
    ap.add_argument("--json", action="store_true", help="Also export raw transcript items as JSON")
    ap.add_argument("--cookies", help="Path to a cookies.txt file for authenticated transcript access")
    args = ap.parse_args()

    os.makedirs(args.out, exist_ok=True)
    init_transcript_api(args.cookies)

    inputs = [args.video] if args.video else read_video_list(args.list)
    video_ids = []
    for x in inputs:
        vid = extract_video_id(x)
        if not vid:
            print(f"Skipping (could not parse video ID): {x}", file=sys.stderr)
            continue
        video_ids.append(vid)

    if not video_ids:
        print("No valid video IDs found. Exiting.", file=sys.stderr)
        sys.exit(2)

    results = []
    for vid in video_ids:
        r = export_one(vid, args.out, args.lang, args.prefer_manual, args.json)
        results.append(r)
        print(f"{vid}: {r['status']}")

    # Write summary
    summary_path = os.path.join(args.out, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nDone. Files saved in: {os.path.abspath(args.out)}")
    print(f"Summary: {os.path.abspath(summary_path)}")

main()
