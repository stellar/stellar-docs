WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:59.999 align:start position:0%
So welcome to the Stellar Open Protocol Discussion in these discussions we discuss and plan for changes to upcoming versions of the Stellar protocol right. Now we're focused on Project Jump Cannon, which will bring smart contracts to Stellar in addition to changes to the Stellar protocol it will also lead to the creation of a new smart contracts platform platform and all of the discussion, that we're having about this is being tracked and you can participate in it much of it is happening here on Discord in the Jump Cannon channel and the drum cannon dev channel there are also a series of Core Advancement Proposal or CAPs, that relate to changes, that would enable Jump Cannon or the new smart contracts platform platform CAPs 46 through 55 I believe at this point they're pretty modular. So each one sort of takes on an aspect of the changes, that need to be made in order to bring smart contracts to Stellar and we are working through those modules bit by bit and discussing sort of the segments necessary and the changes necessary to allow those segments to actually come to life
00:01:00.000 --> 00:01:59.999 align:start position:0%
life at some point all of the work, that we do will go through the normal process in other words CAPs are sort of put up in this github repository, that's linked to in the show notes they're discussed here they're discussed on a mailing list and they're discussed in Discord and after they sort of reach a point where they are stable they move from being a draft into a formal acceptance period finally they're accepted and implemented in a version of the Stellar protocol and before, that version of the Stellar protocol goes live validators actually vote to accept it. Now we are still fairly well we've actually made a lot of progress in the in sort of the Jump Cannon trajectory. But as of yet the CAPs, that we have in front of us have not been accepted there's still a lot of questions and today we will dig into some of those questions questions. If you who are listening actually have questions you can leave them as text in the live chat channel i'll try to keep an eye on, that we're certainly trying to move this discussion forward and have the substantive issues come to light
00:02:00.000 --> 00:02:59.999 align:start position:0%
so. So you know we may not be able to answer all the live chat questions. But we definitely will later. If we can't answer them in the course of this meeting. So I think everyone is here and I think we are ready to kick off today I know, that recently there was a new CAP 55 fee model in smart contracts I don't know. If it just hit the mailing list yesterday there were a few comments, that came in I guess I'm going to start by asking you nicola. If we're ready to discuss, that if, that's where we should start maybe yeah like we can maybe like just go over like a quick overview of like what's going on in, that cap. And then then you know like we don't have to go in details basically. So I'm not sure like we need to have like a lot of of a pre-reading basically as part of this
00:03:00.000 --> 00:03:59.999 align:start position:0%
this is, that good use of time of people yeah I think, that sounds good also Nico. If you can you know obviously a lot of these things are kind of like normal quote unquote in the world of crypto. But some of these are are a bit more contentious. So I would like emphasize specifically like the contentious bits so, that we can have a good old-fashioned argument all right let's see and yeah I mean. So do we want to start with this CAP or I think there were there was also like the events one, that cida opened, that is maybe a little more scope I don't know yeah I argue let's start with the Siddharth one. Because it's less contentious and we'll probably be arguing this okay great. So we'll start there we'll end up at CAP 55 sit down do you want to yeah I don't know. If everyone had time to read the CAP document I actually made a fix of this morning
00:04:00.000 --> 00:04:59.999 align:start position:0%
morning. But I can give a quick overview of it and and you can we can discuss after, that. So I made I added this change on to CAP 51, which is the host functions CAP CAP I just added the ability for contracts to log data. So I added a contract logs back to the transaction meta and as a part of this change I we also moved the transaction result into the meta as well both contracts. So both contract logs and transact transaction results are hashed and a hash of these hashes are stored in the transaction result pair, which is this is how you would cryptographically verify them and one change I haven't made to the document yet. But we talked about yesterday was, that we're gonna add another another contract log type where the logs are only emitted. If there's an error so, that's a very high level overview are there any questions
00:05:00.000 --> 00:05:59.999 align:start position:0%
I guess like those logs they are like the equivalent to events in Ethereum yeah. So they well. So the way it would work is you know, that the contracts allow whatever they want since it gets sent to core core or write some transaction meta and horizon can serve them up in any way you want. So I was I would imagine, that you know. If you want to listen to a specific event horizon would provide, that ability allowing you to you know write applications, that would hook on to specific events. So one thing, that I want to point out here is, that it basically means, that in order to sift through through data coming in from horizon as an ingestor you need to basically read everything
00:06:00.000 --> 00:06:59.999 align:start position:0%
which may become a lot as the network grows in capacity Ethereum has this concept of this bloom filter, that's included in every ledger header so, that you can actually actually get a like a strong indicator whether or not the smart contract, that you're interested in or the account, that you're interested in is actually included or has emitted events in, that specific block should we consider doing something similar yeah I don't think I can't think off top of my head why we wouldn't do optimize like, that that's, that area I'm sorry I can look into, that I got I don't think I don't see I don't think I see anything wrong with, that well actually I think it's kind of maybe like premature optimization type of situation
00:07:00.000 --> 00:07:59.999 align:start position:0%
like there are probably better ways to do it than doing this kind of arbitrary broom filter thing like you know I can imagine for example like we have just made a stream you could have horizon telco, which filters it wants to apply instead of kind of doing it after the fact. And then the mirror would be a subset of the meta like maybe like you're not interested in ledger changes let's say maybe you're not interested in classic transactions you know like all those things things well you're assuming a horizon here and I think, that's like part well there's always a consumer right of but. If you want something, that's like an application specific consumer, which you do see in other ecosystems quite a bit you know. If I'm developing a dApp and I want to have a
00:08:00.000 --> 00:08:59.999 align:start position:0%
you know like a stream coming in of my specific information why should I run a full-blown horizon rather than some sort of a light client, that could just you know share logs, that are specifically relevant to me and, that's again like a subscriber I don't see why this is like part of this gap well I'm not saying it's part of this cap. But I am saying like is there a place in the protocol to optimize for these use cases. Because we actually want more people to run nodes nodes and and what we're doing here right. Now is, that we're like really tying them down to like the horizon model, which is consume everything in just everything no no like horizon doesn't force you into consuming everything you're getting the entire meta
00:09:00.000 --> 00:09:59.999 align:start position:0%
and. Then you filter it yes like is the amount of meta being produced going to be a bottleneck in the you know even in the middle you know medium medium term I don't think. So like you know XDR is fairly efficient like I would like to see the actual performance problems before picking arbitrary type of filtering technology. Because I don't know what the use cases are like you're saying they are events yeah sure. Then you just keep advance it's a very small subset actually of the media stream is anything, that we're adding here in this CAP prevent us from adding a bloom filter or other sort of strategies in the future I don't think. So but yeah you can do any kind of filtering I mean mean I think the the place where I can see having an actual bottleneck in the future is the
00:10:00.000 --> 00:10:59.999 align:start position:0%
actual size of the meta may get too large for like. If you want to run a light lighter node. But then we are getting into custom logic in core to kind of you know filter, that somehow and I think the best way to do it is actually like. When you're producing it instead of trying to do it after the fact like with like like with the bloom filters something, that I don't see here. When it comes to filtering is any way to filter beyond the contract. So like like presumably the contract being out of filter up by the contract is there. Because that would probably be in the transaction matter. But but. If a contract wants to emit emit a whole lot of different logs
00:11:00.000 --> 00:11:59.999 align:start position:0%
how would an application filter on those specifically or is, that just too granular to micro well at the moment you know the body is an sc val so. If you wanted to do, that that you would add the filtering in there. But you know I think we would discuss this okay like maybe, that's not reasonable we should add a higher level filters above the sc bell something we can consider yeah like, that right. Now this structure would make sense here yeah like originally I thought we would do something like right. Now you have this block type right system or contract info and basically. If it's a I mean actually for both of them you probably want to have like an actual event name right like, which is like a short symbol of source right
00:12:00.000 --> 00:12:59.999 align:start position:0%
yeah I think, that makes sense I would honestly look at sort of what the subscription patterns, that you see in other smart contracting platforms are. Because they have explored this space fairly extensively and I think it's what a lot of the SDKs really lean on like. If you're writing a dapp it's fairly common for it to latch onto a bunch of subscriptions. So like. However they're normally doing it we kind of want to support those patterns is there more anyone else have thoughts questions suggestions is it sort of clear what the next move is for you here
00:13:00.000 --> 00:13:59.999 align:start position:0%
Siddharth you move on to catholic actually like there is something yeah, that I just thought about, that I think you know from what I think graydon was asking in terms of use cases like like are there expectations for example, that and, that's related to this filtering question, that you want to have proofs of events, that do not happen. So like positive proofs are easy like like with the proposal you have like you know basically like you can prove, that a given ledger had a specific event inside a space you know from a generated by a specific contract what. If you want to prove the negative, that is
00:14:00.000 --> 00:14:59.999 align:start position:0%
that a specific event yeah was not emitted in a ledger is, that like the type of things, that people people try to do in other systems. So yeah, that's a question for yeah to look into the what happened the use cases yeah 55. Let's move on to it all right yeah yeah. So 55 is basically like like trying to layer fees on top of the various like like resource metering, that metering, that started to get introduced in the system. So it's kind of a problem is it's a little bit ahead of, that. Because we didn't actually
00:15:00.000 --> 00:15:59.999 align:start position:0%
finish all this like I think we have a the beginning of like gas metering for example in CAP 46. But there are like other things, that are not covered yet. So yeah this is. So the test disclaimer this gap has a bunch of open-ended open-ended things things let's see and yeah. So like they are are where the CAP is covered there are like several aspects kind of maybe like more important to discuss I think there's the first one around the classification of resources and having market dynamics based on those resource types. So this is an area where. If you look at other blockchains it's actually a mix of things
00:16:00.000 --> 00:16:59.999 align:start position:0%
like some systems like historically started with just like Ethereum just like gas as being the yes I I'm also a gas in the context in this context meaning the the kind of this metric right, that allows to to kind of count the cost of a to execute a transaction and cost here is kind of a pretty loose in terms of definition like mostly computation but. When you do like. a ledger entry like. When you like ledger you also pay for gas. And then yeah like yeah. And then so you have like gas cost, which is this aggregate metric basically of multiple resource types and
00:17:00.000 --> 00:17:59.999 align:start position:0%
then more recently in Ethereum there have been discussions around other types of resources, that are kind of interesting such as bandwidth and any other like basically you have a kind of a funny funny crossroad there, that is do I want to have a market for for each of those resources or do I want to kind of generate like a composite market market for those. So like the you have like I think in polkadot what they do is they put a the aggregation with utilities with a polynomial function. So basically take all those resource types. And then you assign them a weight actually it's not even I think they are linear it's a linear thing
00:18:00.000 --> 00:18:59.999 align:start position:0%
thing. And then yeah you combine all those things and you get with your synthetic I don't remember how they call it weight I think in in over there. But like yeah like, that's a way to kind of compute this aggregate gas. So the challenge. So to talking about challenges, that comes with those aggregate models is, that it's actually very hard to discover price of things things like an example is. If you take a transaction, that does a lot of I o and very little compute, that is competing with a transaction, that does very little I o. But a lot of compute compute like like with those aggregate functions. If try to pay like 10 times more for example bid more
00:19:00.000 --> 00:19:59.999 align:start position:0%
right for one transaction you don't know. If you're signaling, that you're, that your I o is what you want to prioritize or. If it's your compute, that you want to prioritize. So so yeah. So it basically causes the overall overall prices to have like this uncertainty in terms of like what should I bid so, that's kind of one of the problems with those kind of aggregate aggregate metrics. So with, that said with John cannon like one of the things, that we are doing is we have a very clean separation between the different resource types. So io for example. When we read or write the ledger those are done basically outside of the main main execution like you can think of before applying a transaction before
00:20:00.000 --> 00:20:59.999 align:start position:0%
executing a contract we load all the ledger entries, that this contract needs. And then it does its thing. And then at the end it produces potentially side effects, that will be applied as like a post step, that's kind of a logically the way to we can you can think about this and the opportunity here for us is, that. Because we have those kind of completely separate we can actually dis and we also do it. Because of performance reasons for parallelism. But like. Because of, that we can we can actually express those markets like separately and we can. Therefore I think have like cheaper fees overall. Because you can price things properly. So you don't have to do like to kind of articulate inflate for
00:21:00.000 --> 00:21:59.999 align:start position:0%
example inflate the price of reading data from the ledger. Because compute happens to be expensive, which kind of would happen with the aggregate model. So so in the proposal what I'm doing is actually I have like three categories three really really brackets for fees. So one is for gas, which is the compute time exactly you can think of it as execution time in our model really. Because like I said earlier we have a full separation between I o and and execution. So this one you can bid bid second second market, that I have in the proposal is for reading and writing to the ledger. So here there is actually a competition for there's like there are. So many
00:22:00.000 --> 00:22:59.999 align:start position:0%
there's like a you have like constraints right in terms of a number like the bandwidth to the disk subsystem both in reason rights so. Because of, that you have to have like a market for, that and and it is separate right. Now and it is separate also. Because there is a a interesting fee model for rights, that I'm going to talk about later let's see and the third category is actually something, that is not a market it's not really a market there are dynamic fees for what I would consider like commodity on the network. So things like like producing meta or data, that ends up being stored in archives archives those do not there's no reason to have like really competition between transactions
00:23:00.000 --> 00:23:59.999 align:start position:0%
instead we have like limits per transaction like basically we say you can only produce I don't know like a 500k or something of meta right for a transaction and then, that's your limit. And then two transactions are actually not competing you know against each other. So there is no no need to you cannot have a market dynamics there. So so there are actually a few of those of those resource types and. Because there are there is no market you can actually aggregate them. So they end up in one big bucket of like like deterministic fees basically based on the current state of the ledger plus plus yeah like the actual transaction. So those are like the three three categories, that we have in this proposal
00:24:00.000 --> 00:24:59.999 align:start position:0%
proposal any question at this point on this i've been going back and forth actually on this like a should we or not separate piece we could go with one like I said one market. But I think it pushes price quite a bit too much for like cheap cheaper whichever resource will be cheaper, which is hard to predict do we anticipate, that this will be difficult for users to reason about to like understand these different types of fees and to think about how to set them. So so yes and no I think, that's actually one of the things, that's kind of interesting is, that. When we have already in the system a strong dependency on a pre-flight
00:25:00.000 --> 00:25:59.999 align:start position:0%
mechanism. So like before submitting most transactions to the network like they will have to go through a pre-flight endpoint the pre-flight is the thing, that basically will allow people to compute compute gas for example to estimate gas for transaction in addition to, that let's see in addition to, that yeah we have like I was saying like certain fees, that are like, that more like dynamic. Because they are based on the current ledger like for example like a bunch of those things, that I was saying are like the price of storage in archive this is voted by the or determined by the validators. So at before submitting a transaction you have to know basically what those parameters are
00:26:00.000 --> 00:26:59.999 align:start position:0%
and yeah. So as part of the pre-flight endpoint you basically get a an estimate for your the minimum fee for those categories well in the case of the non-market-based resources the minimum fee is basically equal to your or very likely to be equal to what you need in the case of well you do have markets it's more like today where you have to decide how much do you want to over bid based on, that. Because the minimum fee doesn't necessarily translate to to what the market is willing to pay. So you have to look more at historical data data. But but this is like something we can yeah, that that like endpoint scan like a horizon can
00:27:00.000 --> 00:27:59.999 align:start position:0%
can expose right can having yeah having them tracked as separate resources in terms of historical price allows you have actually something a little little more stable I would imagine than. If it was like an aggregate I think the yeah the complexity from multiple markets comes from I mean one of the implications is actually. When we we construct one and say we validators construct construct a transaction set it's going to be a kind of a multi-dimensional nexa problem, which is not great but, that's the but, that algorithm would not be part of the protocol it's more like like it's
00:28:00.000 --> 00:28:59.999 align:start position:0%
if you have like five seconds to produce a block here, that's there's. So much compute you can span in assembling the perfect transaction set yeah to answer original question Justin at the end of the day the you know the wallets should have an easy way to present an like an estimated cost in in an XLM currency, that they can understand understand and they can tell the user hey like you know this is how much more you can you can propose for, that like they don't need to actually understand the mechanics of how this works yeah it's true, that. When you over beat right like you're doing for example you say I want to spend like 10 more than whatever happened in last few lectures lectures, that 10 percent you can put it I mean I imagine pretty safely across like those different resource types like the ones with
00:29:00.000 --> 00:29:59.999 align:start position:0%
markets markets. Because the assumption there is like the they are priced accurately. But I imagine, that more yeah. If you want to really save like a it's hard to predict. But like like. If there are like some of those like a gas for example becomes very expensive expensive yeah you don't like maybe you don't want to be as aggressive on the other resource types can I jump in with a couple comments yeah. So I guess. So one it's I guess high level it's not clearly what this adds over for having like a multi-dimensional optimization problem over the one-dimensional one and the reason is
00:30:00.000 --> 00:30:59.999 align:start position:0%
that sorry I haven't thought about this all, that much. But the reason is, that like. When when at least in the current execution model sort of everything like everything executes everything in one lane is going to execute sequentially right. And so the main like resource, that's truly limited in like a block is time right and it's it's it like unless there's some sort of weird weird interleaving going on between like transaction executions, that's sort of the one-dimensional resource, that we have to optimize anyway. And so it's not clear to me why like like your example of like a transaction, that does lots of I o versus one, that does lots of compute well both of them are going to take a lot of time. If they're using a lot of one resource. And so it's not clear to me, that at least in the current execution setup we have, that it's we gain by sort of allocating some I don't know
00:31:00.000 --> 00:31:59.999 align:start position:0%
resource to like I o versus some to compute as opposed to just like looking at the whole picture of like the total end-to-end time of the transaction, that said it does seem like we it'd be good to like have some kind of like price discovery mechanism for different resources and certainly like you want like an overall limit perhaps on like the total number of ledger entries. And so I know I don't think sort of thinking off the top my head I don't think it's incompatible to have like a one-dimensional like like gas market. And then like sort of price markets on each resource in the sense of transactions could bid like you know for the amount of resources they want to use and like the fee per resource. And then you do some like filtering step but, that's sort of thinking very much off the top my head I don't think we necessarily I guess high level I don't think we necessarily have to go to the full multi-dimensional
00:32:00.000 --> 00:32:59.999 align:start position:0%
operation problem, that's. But yeah, that's possible like it just looked like from from historical historical kind of experience right like a like I o is a huge problem and trying to model, that as time is actually making it's actually a disservice in a way to the network. Because you have like very expensive from like this point of view right like something, that's going to suck your disk resources, that. Now stole all your calls right on in a multi you know parallel execution model like store as in you know. Because like I said we do all I o early on and. If you're actually maxing out out your drive. Then you're just stuck
00:33:00.000 --> 00:33:59.999 align:start position:0%
right I mean it makes sense to have perhaps a limit on overall io I guess. And then there's yeah the other aspect actually maybe. If we can you know like I don't think we're going to necessarily like close on this multi-dimensional thing you know. Now but like there's the other aspect of yeah ledger size and rights, that is actually another kind of key thing in there, that I guess makes io a little more special also Nico just to go back to jeff's point like I understand, that why I o needs to be you know priced significantly higher than you know the compute operations. But I don't necessarily understand why it needs to have like its own market. So the pricing right, that you have is the minimum price it's not
00:34:00.000 --> 00:34:59.999 align:start position:0%
the market price like. When you market prices is like like in the ideal like what is describing the CAP is trying to be closer to like the ideal situation where you can actually construct a transaction set, that's going to basically be like right at the edge in terms of the capacity, that you have on your actual you know underlying hardware. So like cpu and io for disk. If we lose, that visibility. Then you may actually allocate too many transactions to compute. When then like you know you don't have like basically like the the kind of natural way of of having a
00:35:00.000 --> 00:35:59.999 align:start position:0%
a transaction compete against other transactions, that are paying for expensive stuff, that's kind of what I'm getting to like like. If you have like what was it like a good example would be like yeah I don't know, which one of those resources would be more expensive. But they are not going to be in the same order of magnitude let's say like a compute is the one more expensive at a given time. So so you have to pay like 10 times more right or 100 times more than the minimum fee for compute to get into the ledger. But your your storage price is also kind of expensive and by bidding a hundred times you also bid a hundred times on storage storage and you're basically overshooting quite
00:36:00.000 --> 00:36:59.999 align:start position:0%
a bit compared to the ideal model model yeah I think the general point here is just, that you cannot, that in reality reality it's not the case, that there's just time. When a transaction is executing there are two different resources and there are different contention patterns on them and you can't trade one for the other the the system does not actually trade one for the other like. If I for example submit you know a hundred transactions every one of, which is doing incredibly cpu and expensive stuff, that doesn't saturate the I o system and there's still no contention on the I o system whereas. If I submit a 100 transactions, that are just doing I o and they're doing no cpu, that doesn't saturate the cpu. So they are really two separate resources and the point where one of them gets a limit and can no longer do transaction processing it doesn't doesn't represent a limit on the other and vice versa. And so you you can't trade
00:37:00.000 --> 00:37:59.999 align:start position:0%
between the two of them from a market perspective sorry I'm not quite following something didn't we say earlier, that we were going to do like all of the sort of disk reads first. And then do the executions right so, that. If we have a lot of disk reads. Then we have less time for execution. And so the vice versa I sort of understand, that there's not they're not like directly tradable. But they seem correlated or anti-correlated well they're they're different devices. So like right I'm using the disk. And then I'm doing the cpu right I'm not using it at the same time I mean like sure there's different offers and things. But yeah sure. But the execution characteristics of each of them are different. So you use everyone uses the same desk. And then everyone sort of farms out to multiple threats right I feel like we're talking past each
00:38:00.000 --> 00:38:59.999 align:start position:0%
other I mean yes one goes in order of the other the two of them do get added together together in order to represent the total time. But you can't trade time on one of them for time on the other, that's what I'm saying I think I'm not quite following but, that's okay Nico are there any other networks and fee systems, that introduce a split or, that work similarly those are like the two like the yeah execution time and like compute right and and disc are like the two big things, that I isolated like the right side I figured it would probably is probably not needed. So I kind of put a flat fee for the other ones like basically like in a proposal I'm basically saying like
00:39:00.000 --> 00:39:59.999 align:start position:0%
we're okay with you know. If you want to have like something, that like. If you have a transaction, that is very important, that happens to emit a lot of miller for example. But you have to get just over a bit like crazy on your compute even, though you're not really, that's not what it's about you have to find a way to get it prioritized let's see are we talking about this okay like I think I kind of wanted to talk about it here is in your proposal you're talking a fair amount about state expiry yeah yeah before we go into estate expiration, which I know is a big topic I'm still trying to reason about like the kind of the wallet experience and user experience of having these like multiple dimensions for for gas like
00:40:00.000 --> 00:40:59.999 align:start position:0%
what is the expected behavior here for for wallets to be clear right like tomorrow like single dimension or multi-dimension from a white point of view. If you want to estimate it's the same problem right like it's like in the single like a cost model like you aggregate everything into one you have to actually you have like a function right, that just aggregates. But you do have to estimate your bid for each things. So it's not it's actually like a funny thing, that you have except maybe the tools you have for discovering price are not as great great. Because it's all implicit okay. So a wallet can do a pre-flight can tell me like the expected cost cost I can
00:41:00.000 --> 00:41:59.999 align:start position:0%
you know bid bid bit over. But how do I know how to divide, that between like the compute and the I o well like it's the same in you know like I said like it doesn't, that question is not not a question around multi-dimension versus single dimension. Because that like. If you want to say like I want to pay 10 percent more for on top of the market rate right for storage let's say. Because that's where there is contention you have to know, that that's exactly what you have to have the yeah market price for storage. If you just layer like 10 flat on everything everything you're just going to above overbid
00:42:00.000 --> 00:42:59.999 align:start position:0%
which is maybe okay right like for some people. If the fees are relatively low you know what's the difference between you know you know half a lumen and two-thirds of aluminum or something I don't know like historically we've seen, that like in some situations people are getting are bidding very high on certain on certain for certain patterns it would be great to maybe and forgive me. If it's already in the cap. But just like understanding what is the like the what's the expected wallet strategy or client strategy here. When like in terms of user experience like what do they present to the user
00:43:00.000 --> 00:43:59.999 align:start position:0%
and what you know what kind of inputs do they expect from the users yeah sure okay let's talk about state exploration Nico Nico where is it well. So state exploration yeah goes kind of hand to hand with the model, that I have there for storing data on the ledger. So like the in the proposal it's basically like there are two parts to it there's the how do you model model a write and as in. And so writers can be a create a ledger entry or an update and how does it work like how do we have like the right price basically for the cost of storage. So in the proposal what I did is I basically used as an approximation for
00:44:00.000 --> 00:44:59.999 align:start position:0%
for the cluster storage the bucket list size. So like the some basically like the ledger is organized into those like 19 buckets I think it's 19. And then it you. If you do a an update or a create you basically append, that to the the to the very first bucket in the bucket list list so, that's how basically like based on the size of the the total size of the ledger I allocate like a price function, that kind of looks like an exponential from fall like basically it starts with a slow slope up to some number let's say you say oh like
00:45:00.000 --> 00:45:59.999 align:start position:0%
validators here kind of determining those parameters. But like you can think of it as the leaders say oh yeah right. Now we are running on drives with I don't know like 25 25 gigs or 50 gigs of space right and they're going to basically set parameters such, that they don't have to kind of buy new drives you know like. If there's too much traffic. So so the price function is basically looks in this case like you have like your normal slope, that goes to in like I don't know let's say you have 100 gig and it would be like I want to use maybe like the first 80 gig at a rate, that's going to be like a good rate. But not like overly aggressive. And then the last 20 gigs I want to
00:46:00.000 --> 00:46:59.999 align:start position:0%
really slow down like the growth so, that's right like from 5 looks like this hockey stick type of shape right like an exponential and, that's kind of the model for pricing growing the bucket list. So so you have, that for, that's four rights. Then the problem is, that this is only like this is like saying okay you can add to the bucket list. But then like like and by the way like. If you delete entries entries eventually those get collapsed into the buckets. And so the bucket is shrinks in, that model a delete you still pay for or delete actually. Because delete is actually adding a little bit of data to the bucket list so, that's like first thing to note here
00:47:00.000 --> 00:47:59.999 align:start position:0%
and. Then yeah what I wanted to get here is as a kind of more like a desired property is, that I want the price of storage for people to kind of be the same for everybody regardless. If they signed up for you know created an account like two years ago or you know in five years it should be over time same cost and there should be no way to do like to have like a free ride on the ledger right like. So you the on able to have like store store I don't know like NFTs like jpegs whatever on chain you pay for this. When storage is cheap. And then now you have like something, that is cheaper than than even storing in aws right like, that doesn't make any sense
00:48:00.000 --> 00:48:59.999 align:start position:0%
so this in with, that said there's. Then a need for having some way of of kind of resetting in a way the the price of of storage over time and the mechanism, that I use there is state expiration. So state expiration here means, that you have to basically pay for market price of storage to maintain a ledger entry on the like a live in the ledger. If you do not pay for this for your brands basically you get perched, that's kind of the choice, that I made in this cap
00:49:00.000 --> 00:49:59.999 align:start position:0%
there are a bunch of other ways, that can be done, that are actually mentioned in the recap in the other approaches approaches. But like the the reason this kind of works with the other mechanism is, that basically like. If you set a policy for example by default you have to pay rent like a refresh every year let's say say. And then you don't pay your your renewal after a year yeah your the data gets deleted and yeah. So there's this kind of constant churn I guess on the ledger, which is kind of a new pattern and, that construction is basically a way to guarantee, that everybody in the last year has been paying basically something, that is
00:50:00.000 --> 00:50:59.999 align:start position:0%
market rate how long do you expect the like the how far in the f like. When I trade a letter entry how far is the maximum expiration date, that I can choose or, that will get chosen for me in the future this stuff isn't as far as I can tell it's not specified in the CAP how, that works. So right. Now the CAP what it says is, that it do not. So you can renew indefinitely right like the renewal window is determined by validators so, that's why I said it's like a like you say every year you have to pay run run right. And then every time you write you do an append, that happen is valid for a year right, that's good, that's demo. Then
00:51:00.000 --> 00:51:59.999 align:start position:0%
isn't there a natural trade-off between like renewal time and like fluctuations in this price of storage or are we expecting like this storage cost to not increase too quickly well it depends like what we've seen on the current network is ledger size has been increasing actually rapidly over the last few months. Because of like some strange token activity, which is not entirely I mean it's not I mean there are a combination of factors like one is yeah like just price of in crypto assets go you know going down. But also like. When they were were still pretty high you had like an incentive to create more crypto assets. So it basically those things kind of cancel each other and the growth has been pretty significant
00:52:00.000 --> 00:52:59.999 align:start position:0%
so I would say like seeing a growth rate, that that takes you to yeah something, that will be you want the market basically to kind of get to an eco equilibrium right like where where where like you do not have like like those weird use cases appearing on the network. If they are like cheaper than right. Now I think on the network the the problem we have is yeah we are cheaper than aws fault in some situations isn't there a trade-off here between not necessarily a trade-off. But isn't there a consequence here, that people will have to go and touch their data from time to time and people are procrastinators and like
00:53:00.000 --> 00:53:59.999 align:start position:0%
let's say, that like the expiration date is you know. When you're in the future or something everybody at the end of, that year. Then has to go and touch all their data and there's gonna be a huge log jam to get it done well, that's there would be a large jump. If everybody creates their stuff at the same time. But you would, that's not the case it's going to be you know like basically like the thing, that expires in a year is the whatever happened today right like at the given date right. But I mean like imagine, that today you have a day with like a lot of activity like you can look back historically of Stellar's history and there are periods. When there were like lots of token creations and stuff you know there are days. When there's hundreds of thousands of blood draw entries entries created created. And then abundant well many of them are abandoned. But like you could imagine a world where they're not all abandoned right. And then what happens a year in the future well nothing well people are incentivized to come back some time between here. And then I don't
00:54:00.000 --> 00:54:59.999 align:start position:0%
see this as any worse than the fact, that we have to handle load spikes in general yeah I mean we have to handle load spikes and let's bikes may get replicated. But yeah like what. So what I sketched or what we sketched actually in the cup, which is you know just a more of a strowman type of thing. Because I'm sure we can do better than, that is is we actually are kind of ensuring, that you do not have like giant spikes. So like it like I think the spike would be not be. Because of situations don't like what you're what you mentioned. Because actually activity from today. If you have like a comp a a a linear like a translation right like an actual just shapes right like all this activity gets translated exactly a year from now I you don't have a problem I think the it's just like additional cost of running a validator right
00:55:00.000 --> 00:55:59.999 align:start position:0%
like it's let's you say okay I need to to. When I set my limits right the number of rights rights I actually have to think about well actually my capacity in rights is half of what I can add to the ledger. Because I also need to delete right. But what can happen is more of a like. If we have different expiration times, which I actually kind of briefly talked about there is, that. If you have different expiration times times you can have actually different dates, that that end up expiring at the same date and and, that's for, that for those type of situations you have to have an algorithm, that kind of smooths things out and, that doesn't actually cause the system to kind of create a gigantic spike you know
00:56:00.000 --> 00:56:59.999 align:start position:0%
at a specific dates. So just we don't have a lot of time. But I just want to ask what like the biggest question I think, that there is like what is the what's the expected behavior here like you know these ledger entries are representing financial instruments instruments let's say assets just for simplicity even, though we have like a standard asset contract and I'm you know paddling on shares of something is what is the expectations or what is what's expectations there's like am I supposed to like once a year like come and touch this is like the operator of this financial instrument supposed to do, that for me. If you know. If you look at you know various common immutable contracts like uni swap you know. And so and I'm holding on like these uni tokens tokens what's the expectation here like who's going to touch these for me
00:57:00.000 --> 00:57:59.999 align:start position:0%
right. So in the CAP I actually let this kind of flexible like there's a. When you there's actually a special host function to, that you can call, that is basically a rewrite equivalent to like a rewrite ledger entry so, that you refresh the, that expiration time anybody can do, that all right like there's no it's not I understand, that anyone can do, that. But who is who do you expect to do, that that well it depends on the type of users right like like power users probably don't want to do it themselves like other situations you know. If you're like you said like this very passive type of person maybe you should pay somebody to maintain your stuff. If that's what you really want in other situations I suspect. If people are not active they probably should just be using centralized infrastructure
00:58:00.000 --> 00:58:59.999 align:start position:0%
infrastructure like you know contracts or whatever, that are a little more centralized centralized I think trump's point is, that there's a free rider problem here like imagine, that all the people in this room are using a single contract right like like, which one of us can touch me all of us have an incentive to wait until the last second and play chicken and hope, that somebody else oh like yeah for a short contract I think, that yeah well it's. If it's a shared goodness like just just I, that's not what actually what I asked John I assumed, that like each of us will have like our own ledger entry within, that contract. So maybe did I not understand correctly I think I'm concerned about people's money vanishing into thin air, which is completely reasonable these are balances and we're just going to delete them, that's not super great I recall there being a proposal to like
00:59:00.000 --> 00:59:59.999 align:start position:0%
have. But what happened to this like. When you have a ledger engine deleted it like gets dumped into like some kind of merkle try and you store the hat root of, that try. And then like. When I want to bring it back I can like bring in a proof, that this is what the state was right. So this is actually in the appendix yeah the alternative section. So this there is actually a very detailed proposal in Ethereum foreign v2 about this. So the complications from this archive approach is is. When you want to. So like restoring is actually yeah like a trivial like I said like you know you have like a maybe a way to do like a to just to basically store, that entry in inside a merkle tri of source right. And then you just need to provide the proof for, that that the complexity comes from
01:00:00.000 --> 01:00:59.999 align:start position:0%
when you want to create an entry. Because you have to prove, that entry doesn't exist in historical data like, that was actually archived and, that gets really nasty very fast right sure. But like, that like not wanting to do, that doesn't address creating this question like what do you do. If your money gets deleted it's it's an event like like you know. If it's a like with an issue it's like today. If you're on the seller network. If you're sending back to the issuer you know you basically basically burn it you can ask the issuer like hey sorry I didn't mean to do, that. But I didn't burn it. But like what. If you're like what. If it's not like you know an off-chain issuer, that you can appeal to like what. If your unit swap lp tokens get deleted what do you do it's tough yeah like you know
01:01:00.000 --> 01:01:59.999 align:start position:0%
these those are the rules of the network wait but, that's not a great solution long term right like we're gonna have a lot of people people consider the alternative right, which is infinite growth of a ledger with infinite price, which one do you prefer I mean like objectively like. If somebody had a million dollars of like uniform lp shares get liquidated it would have been better to pay for a million dollars of storage so, that one person yes. But like what about everybody else and, that person with a million dollars like. If they have, that it's kind of like key management like you have procedures to make sure, that you don't lose your million dollars it is currently the case, that people with million dollar balances can in fact lose them. Because they can lose their keys. So there's there is something to appeal to here like it is actually possible for you to lose money just by
01:02:00.000 --> 01:02:59.999 align:start position:0%
misusing the system. But what about the other end of the spectrum, though somebody who doesn't have a million dollars they have a small amount of their balance and they're just constantly eating, that up by paying these fees to keep their balance alive I mean it sort of reminds me of like bank accounts where you're like bank accounts just disappearing. Because you're paying all these fees yeah they do eventually disappear. If you put five dollars in a bank account. And then wait for 20 years it'll go away I mean this just points to yeah you're not stirring your balance in the right place like this is shared infrastructure like. If you don't use it you lose it. But nikki you can't just like ignore the entire entire industry, that we're in and you know I'm not pretending, that this is not a problem like people are overlooking this problem problem. But but you know it's I think it will be really difficult to bring people into Stellar telling them oh this is the way it works
01:03:00.000 --> 01:03:59.999 align:start position:0%
in selena right. So it's a fader no it's not selena doesn't actually actually do any of this right. Now they have rant no they don't they literally don't charge it right. Now it's a to be done in the future feature, that no one wants. So they're never actually going to get around to doing it they have infinite ledger in memory they allow they charge you money to make to allocate space. But they never actually reclaim it there's no active garbage collection process. So we're over time at this point and I think, that means, that we have to stop I mean I know, that this is an interesting conversation and there's seems like there's a lot to say about the concept of expiration. But I think we'll push it to next week's meeting and hopefully have some of this discussion on the Stellar dev mailing list and here also in Discord in the various John cannon channels so. If anyone is watching and has thoughts about, that feel free to join the Stellar dev mailing list or to
01:04:00.000 --> 01:04:59.999 align:start position:0%
chime in on the Discord here we'll continue to share work and ideas and conversations conversations debates as they happen and we will see you here again soon thanks everybody