So welcome to the Stellar open protocol discussion in these discussions we discuss and plan for. Changes to upcoming versions of the Stellar protocol right now we're focused on project jump cannon which. Will bring smart contracts to Stellar in addition to changes to the Stellar protocol it will also lead to the. Creation of a new smart contracts platform and all of the discussion that we're having about this is being tracked and. And you can participate in it much of it is happening here on discord in the jump. Cannon channel and the drum cannon dev channel there are also a series of core advancement proposals or caps that. Relate to changes that would enable jump cannon or the new smart contracts platform. Caps 46 through 55 I believe at this point they're pretty modular so each one sort of takes on an aspect of the. Changes that need to be made in order to bring smart contracts to Stellar and we are working through those modules bit by. Bit and discussing sort of the segments necessary and the changes necessary to allow those segments to actually come to. Life at some point all of the work that we do will go through the normal process in. Other words caps are sort of put up in this github repository that's linked to in the show notes they're. Discussed here they're discussed on a mailing list and they're discussed in discord and after after they sort of. Reach a point where they are stable they move from being a draft into a formal acceptance period finally they're. Accepted and implemented in a version of the Stellar protocol and before that version of the Stellar protocol goes. Live validators actually vote to accept it now we are still fairly well we've actually made a lot of progress in the. In sort of the jump cannon trajectory but as of yet the caps that we have in front of us have not been accepted. There's still a lot of questions and today we will dig into some of those questions. if you who are listening actually have questions you can leave them as text in. The live chat channel i'll try to keep an eye on that we're certainly trying to move this. Discussion forward and have the substantive issues come to light so you know we may not be able to answer. All the live chat questions but we definitely will later if we can't answer them in the course of. This meeting so I think everyone is here and I think we are ready to kick off today I know that. recently there was a new cap cap 55 fee model in smart contracts I don't know if it just hit the mailing list. Yesterday there were a few comments that came in I guess I'm going to start by asking you. nicola if we're ready to discuss that if that's where we should start maybe yeah like we can. Maybe  like just go over like a quick overview of like what's going on in that cap and. Then you know like we we don't have to go in details basically so I'm not sure like. We need to need to have like a lot of of a pre-reading basically as part of this. Is that good use of time of people yeah I think that sounds good also nico if you can. You know obviously a lot of these things are kind of like normal quote unquote in. The world of crypto but some of these are a bit more contentious so I would like. Emphasize specifically like the contentious bits so that we can have a good old-fashioned argument. All right let's see and yeah I mean so do we want to start with this cap. Or I think there were there was also like the events one that cida opened that is maybe a little more scope. I don't know yeah I argue let's start with the siddharth one because it's less . Contentious and we'll probably be arguing this okay great so we'll start there we'll end up at cat 55 sit down. Do you want to yeah I don't know if everyone had time to read the cap document I actually made a fix of this. Morning but I can give a quick overview of it and you can we can discuss after that. So I made I added this this change on to cap 51 which is the host functions cap. I just added the ability for contracts to log data so I added a contract logs back to. The transaction meta and as a part of this change I we also moved the transaction result into. The meta as well both contracts so both contract logs and transact transaction results are hashed. And a hash of these hashes are stored in the transaction result pair which is this is how you would cryptographically. Verify them and one change I haven't made to the document yet but we talked about. Yesterday was that we're gonna add another contract log type where the logs are. Only emitted if there's an error so that's a very high high level overview are there any questions. I guess like those logs they are like logs that are like the equivalent to events in ethereum. yeah yeah so they well so the way it would work is you know that the contracts allow. Whatever they want since it gets sent to core or write some transaction meta and horizon can serve them up. In any way you want so I was I would imagine that you know if you want to listen to a specific event. horizon would provide that ability allowing you to you know write applications that. Would hook on to specific events so one thing that I want to point out here is that. It basically means that in order to sift through data coming in from horizon as as an. Ingestor you need to basically read everything which may become a lot as as the network grows in capacity . Ethereum has this concept of this bloom filter that's included in every ledger header so that you can. Actually get a like a strong indicator whether or not the smart contract that you're. Interested in or the the account that you're interested in is actually included or has emitted. Events in that specific block should we consider doing something similar yeah I don't think I can't think. Off top of my head why we wouldn't do optimize like that that's that area I'm sorry I can look into that I got i. Don't think I don't see I don't think I see anything wrong with that well actually I think it's kind of . Maybe like premature optimization type of situation like there are probably better ways to. Do it than doing this kind of arbitrary broom filter thing like you know I can imagine for example like. We have just made a stream you could have horizon telco which which filters it wants to apply instead of. Kind of doing it after the fact and then the mirror would be a subset of the meta like maybe like you're not. Interested in ledger changes let's say maybe you're not interested in classic transactions you know like all those. Things well you're you're assuming a horizon here and I think that's like part well. There's always a consumer right of but if you want something that's like an application specific consumer. which you do see in other ecosystems quite a bit you know if I'm developing a dap. And I want to have a you know like a stream coming in of my of my specific information why should i. Run a full-blown horizon rather than some sort of a light client that could just you know share. Logs that are specifically relevant to me and that's again like a subscriber I don't see why this is like part of. This gap well I'm not saying I'm not saying it's part of this cap but I am saying like is. Is there a place in the in the protocol to to optimize for these use cases because we actually want more people. To run nodes and what we're doing here right now is that we're like really tying them down. To like the the horizon model which is consume everything in just everything no like horizon doesn't force you into. consuming everything you're you're getting the entire meta and then you filter it yes like is. The is the amount of meta being produced going to be a bottleneck in the you know. Even in the middle you know medium term I don't think so like you know xdr is fairly efficient. Like I would like to see the actual performance problems before picking arbitrary type of. Filtering technology because I don't know what the use cases are like you're saying they are events. Yeah sure then you just keep advance it's a very small subset actually of the of the media. Stream is anything that we're adding here in this cap prevent us from adding a bloom. Filter or other sort of strategies in the future I don't I don't think so but yeah you can do any kind of filtering i. Mean I think the the the place where I can see having an actual bottleneck in the future is the. Actual size of the meta may get too large for like if you want to run a light lighter node. But then we are getting into custom logic in core to kind of you know filter that somehow. And I think the the best way to do it is actually like when you're producing it instead of trying to do it after the. Fact like with like like with the bloom filters something that I don't see here when it. Comes to filtering is any way to filter beyond the contract so like presumably the contract being out of. Filter up by the contract is there because that would probably be in the transaction matter. But if a contract wants to emit a whole lot of different logs how would an application filter on those. Specifically or or is that just too granular to micro well at the moment you know the. Body is an sc val so if you wanted to do that you would add the filtering in there. but you know I think we would discuss this okay like maybe that's not reasonable . We should add a higher level filters above the sc bell something we can consider yeah like that right now this structure. Would make sense here yeah like originally I thought we would do something like right now you have this. block type right system or contract info and basically if it's a I mean actually for both of them you. Probably want to have like an actual event name right like which is like a a short symbol of source. Right yeah I think that makes sense I would I would I would honestly look at sort of what the subscription patterns. That you see in other smart contracting platforms are because they they have explored this this space fairly. Extensively and I think it's what a lot of the sdks really lean on like if you're writing a dapp it's fairly common. For for it to to latch onto a bunch of subscriptions so like however however they're normally doing it we kind of. Want to support those patterns is there more anyone else have thoughts questions suggestions is it sort of. Clear what the next move is for you here siddharth you move on to catholic actually like there is something yeah. That I just thought about that I think you know from what I think graydon was asking in terms of use cases like. Like are there expectations for example that and that's related to this filtering. Question that you want to have proofs of events that do not happen so like positive proofs are easy like . Like like with with the proposal you have like you know basically like you can prove that a given ledger had a. Specific event inside a space you know from a generated by a specific contract what if you want to prove the negative. That is that a specific event yeah was not emitted in a ledger is that like the type of things that. People try to do in other systems so yeah that's a question for yeah to look into the. What happened the use cases yeah 55. Let's move on to it all right yeah yeah so 55 is basically like like trying to layer. Fees on top of the various like resource metering that started to get introduced in the system. So it's kind of a problem is it's a little bit ahead of that because we didn't actually. Finish all this like I think we have a the beginning of like gas metering for example . In cap 46 but there are like other things that are not covered yet so yeah this is so the test. Disclaimer this gap has a bunch of open-ended things let's see and yeah so so like they. Are where the cap is covered there are like several aspects kind of maybe like more important to discuss . I think there's the the first one around the classification of resources and having. Market dynamics based on those resource types so this is an area where if you look at other blockchains it's. Actually a mix of things like some systems like historically started with just. Like ethereum just like gas as being the yes I I'm also a gas in the context in this context meaning. the kind of this metric right that allows to to to kind of count the cost of a to to execute a. Transaction and cost here is kind of a pretty loose in terms of definition like mostly computation but when you do. Like when you load like a like a ledger entry like when you access ledger you also. Pay for for gas and then yeah like yeah and then so you have like gas cost which is. this aggregate metric basically of multiple resource types and then more recently in ethereum there. Have been discussions around other types of resources that are kind of interesting. such as bandwidth and and any other like basically you have a kind of a. Funny crossroad there that is do I want to have a market for for each of those resources or do I want. To kind of generate like a composite market for those so like the you have like I think in. Polkadot what they do is they put a the aggregation with utilities with a polynomial function so basically take. All those resource types and then you assign them a weight actually it's not even I think. They are linear it's a linear thing and then yeah you combine all those things and you get with your synthetic. I don't remember how they call it weight I think in in over there but like yeah like that's. That's a way to kind of compute this aggregate gas so the challenge so to talking about. Challenges that comes with those aggregate models is that it's actually very hard to discover price of. Things like an example is if you take a transaction that does a lot of I o and. Very little compute that is competing with a transaction that does very little I o but a lot of. Compute like with those aggregate functions if try to pay like 10 times more for example bid more. Right for one transaction you don't know if you're signaling that you're that your I o is is what you want. To prioritize or if it's your compute that you want to prioritize so yeah so it basically causes the. Overall prices to have like this uncertainty in terms of like what should I bid so that's kind of one of the. The problems with those kind of aggregate metrics so with that said with john cannon like one of the things that. We are doing is we have a very clean separation between the different resource types. So io for example when we read or write the ledger those are done basically outside of the. Main execution like you can think of before applying a transaction before executing a contract. We load all the ledger entries that this contract needs and then it does its thing and then at the end . It produces potentially side effects that will be applied as like a post step. That's kind of a logically the way to we can you can think about this and the opportunity here for us is. That because we have those kind of completely separate we can actually dis . And and we also do it because of performance reasons for parallelism but like because of that. We can we can actually express those markets like separately and we can therefore. I think have like cheaper fees overall because you can price things properly so you don't have to. Do like to kind of articulate inflate for example inflate the price of of reading data from the ledger because. Compute happens to be expensive which kind of would happen with the aggregate model. So in the proposal what I'm doing is actually I have like three categories three really. Brackets for fees so one is for gas which is the compute time exactly you can think of it as execution time in. In our model really because like I said earlier we we have a full separation between I o and and . And execution so this one you can bid  second market that I have in the proposal is for reading and. Writing to the ledger so here there is actually a competition for there's like there are so many. there's like a you have like constraints right in terms of a number like the bandwidth to the disk . Subsystem both in reason rights so because of that you have to have like a market for that. And it is separate right now and it is separate also because there is a interesting fee model for rights that. I'm going to talk about later let's see and the third category is actually something that is not a. Market it's not really a market there are dynamic fees for what I would consider like commodity. On the network so things like like producing meta or data that ends up being stored in. Archives those do not there's no reason to have like really competition between transactions . Instead we have like limits per transaction like basically we say you can only produce. I don't know like a 500k or something of meta right for a transaction and then that's your limit and then two. Transactions are actually not competing you know against each other so there is. No need to you cannot have a market dynamics there so there are actually a few of those . Of those resource types and because there are there is no market you can actually aggregate them so they end up. In one big bucket of like like deterministic fees basically based on the current state of the ledger plus. Plus yeah like the actual transaction so those are like the three three categories that we have in this. Proposal any question at this point on this i've been going back and forth actually. On this like a should we or not separate piece we could go with one like I said one. Market but I think it pushes price quite a bit too much for like cheap cheaper. Whichever resource will be cheaper which is hard to predict do we anticipate that this will be. Difficult for users to reason about to like understand these different types of fees and to think about how to set them. So yes and no I think that's actually one of the things that's kind of interesting. Is that when we have already in the system a strong dependency on a pre-flight. Mechanism so like before submitting most transactions to the network like they will have to go through a. Pre-flight endpoint the pre-flight is the the thing that basically will allow people to. Compute gas for example to estimate gas for for transaction in addition to that. let's see in addition to that yeah we have like I was saying like certain. Fees that are like that more like dynamic because they are based on the current ledger like for example like. A bunch of those things that I was saying are like the price of storage in archive. This is voted by the or determined by the validators so at before submitting a transaction.  you have to know basically what those parameters are and yeah so as part of the pre-flight. Endpoint you basically get a an estimate for your the minimum fee for for those categories. Well in the case of the non-market-based resources the the minimum fee is basically equal to your. Or very likely to be equal to to what you need in the case of well you do have. Markets it's more like today where you have to decide how much do you want to over bid based on that. Because the minimum fee doesn't necessarily translate to to what the market is is willing to pay. So you have to look more at historical data but this is like something we can yeah that that like. Endpoint scan like a horizon can can can expose right and having yeah having them tracked as. Separate resources in terms of historical price allows you have actually something a. Little more stable I would imagine than if it was like an aggregate I think the yeah the complexity from. Multiple markets comes from I mean one of the implications is actually when. We construct one and say we validators construct a transaction set it's going to be a kind of a. Multi-dimensional nexa problem which is not great but that's the but that algorithm would. Not be part of the protocol it's more like it's if you have like five seconds to produce. A block here that's there's so much compute you can span in assembling the perfect. Transaction set yeah to answer original question justin at the end of the day the you know. The wallets should have an easy way to present an like an estimated cost in an xlm currency that they can. Understand and they can tell the user hey like you know this is how much more you can you can. Propose for that like they don't need to actually understand the mechanics of how this works yeah it's true that. When you over beat right like you're doing for example you say I want to to spend like 10 more. Than whatever happened in last few lectures that 10 percent you can put it I mean I imagine pretty safely across. Like those different those different resource types like the ones with markets because the assumption there is like. The they are priced accurately but I imagine that more yeah if you want to really save. Like a it's hard to predict but like like if if there are like some of those like a gas for example becomes very. Expensive yeah you don't like maybe you don't want to be as aggressive on on the other resource types. can I jump in with a couple comments yeah so I guess so one it's I guess high level it's not clearly what. This adds over for having like a multi-dimensional optimization problem over the the. One-dimensional one and the reason is that sorry I haven't thought about this all that much but the reason is. That like when when at least in the current execution model sort of everything like everything executes . Everything in one lane is going to execute sequentially right and so the main like. Resource that's that's truly limited in like a block is is time right and it's. It like unless there's some sort of weird interleaving going on between like transaction executions . That's sort of the one-dimensional resource that we have to optimize anyway and so it's it's not clear to me why . Like your example of like a transaction that does lots of I o versus one that does. Lots of compute well both of them are going to take a lot of time if they're using a lot of one resource and so it's. Not clear to me that at least in the current execution setup we have that it's we gain by sort of. Allocating some I don't know resource to like I o versus some to to compute as opposed to just like looking. At the whole picture of like the total end-to-end time of the transaction that said it it does seem like. we it'd be good to like have some kind of like price discovery mechanism for different resources and. certainly like you want like an overall limit perhaps on like the total number of ledger entries . And so I know I don't think sort of thinking off the top my head I don't think it's incompatible to have. Like a one-dimensional like gas market and then like sort of price markets on each resource in the sense of. transactions could bid like you know for the the amount of resources they want to use and like the fee per. Resource and then you do some like filtering step but that's sort of thinking . Very much off the top my head I don't think we necessarily I guess high level I don't think we necessarily. Have to go to the full multi-dimensional operation problem that's but yeah that's possible like . It it just looked like from historical kind of experience right like a like. I o is a huge problem and trying to model that as time is actually making it's actually a. Disservice in a way to the to the network because you have like very expensive . From like this point of view right like something that's going to suck your your. Disk resources that now stole all your calls right on in a multi you know parallel . Execution model like store as in you know because like I said we do all I o early on and if you're actually maxing. Out your drive then you're just stuck right I mean it makes sense to have . Perhaps a limit on overall io I guess and then there's yeah the other aspect actually maybe if. We can you know like I don't think we're going to necessarily like close on this. Multi-dimensional thing you know now but like there's the other aspect of yeah ledger size and rights that is. Actually another kind of key thing in there that I guess makes io a little more special also. Nico just to go back to to jeff's point like I understand that why I o needs to be you know priced. Significantly higher than you know the compute operations but I don't necessarily understand why. It needs to have like its own market so the the pricing right that you have is the minimum price it's not. The market price like when you market prices is like in the in the ideal like what is describing the cap is is. Trying to be closer to like the ideal situation where you can actually construct. A transaction set that's going to basically be like right at the edge in terms of the capacity that you have on. Your actual you know underlying hardware so like cpu and io for disk if we lose that visibility. Then you may actually allocate too many transactions to compute when then like. you know you you don't have like basically like the the kind of natural way of of of having a. a transaction compete against other transactions that are paying for expensive stuff. That's kind of what I'm getting to like like if you have like what was it like a good example would be. Like yeah I don't know which one of those resources would be more expensive but. They are not going to be in the same order of magnitude let's say like a compute is the one more expensive at a. Given time so you have to pay like 10 times more right or 100 times more than the minimum. Fee for for compute to get to get into the ledger but your your your storage price. Is also kind of expensive and by bidding a hundred times you also bid a hundred times on. Storage and you're basically overshooting quite a bit compared to the ideal model. Yeah I think I think the general point here is just that you cannot that in reality. it's not the case that there's there's just time when a transaction is executing. there there are two different resources and there are different contention patterns on them and you. Can't trade one for the other the the the system does not actually trade one for the other like. If I if I for example submit you know a hundred transactions every one of which is doing incredibly cpu and. Expensive stuff that that doesn't saturate the I o system and there's there's still no. Contention on the I o system whereas if I submit a 100 transactions that are just doing I o and they're doing no cpu. That doesn't saturate the cpu so they are really two separate resources and the point where one of them gets a limit. and can no longer do transaction processing it doesn't represent a limit on the other and vice. Versa and so you you you can't trade between the two of them from from a market perspective. Sorry I'm not quite following something didn't didn't we say earlier that we were going to do like all of the sort of. Disk reads first and then do the executions right so that if we have a lot of disk reads then we have less time. For execution and so the vice versa I sort of understand that there's not they're not like directly tradable but. They seem correlated or anti-correlated well they're they're they're different devices so like right I'm using the disk. And then I'm doing the cpu right I'm not using it at the same time I mean like sure there's different. Offers and things but yeah sure but the the execution characteristics of each of them are are. Different so you you use everyone uses the same desk and then everyone sort of farms out to multiple threats. Right I feel like we're talking past each other I mean yes one goes in in order of. The other the two of them do get added together in order to represent the the total time. But you can't trade time on one of them for time on the other that's what I'm saying. I think I'm not quite following but that's okay nico are there any other networks and fee systems that . Introduce a split or that work similarly those are like the two like the yeah execution time and and . Like compute right and and disc are like the two big things that I isolated like the right side. I figured it would probably is probably not needed so I kind of put a flat fee for the. Other ones like basically like in a proposal I'm basically saying like we're okay with you know if you want to. Have like something that like if you have a transaction that is very important that happens to emit a. Lot of miller for example but you have to get just over a bit like crazy on your compute even though you're. Not really that's not what it's about you have to find a way to get it prioritized. let's see are we talking about this okay like I think I kind of wanted to talk about it. Here is in your proposal you're talking a fair amount about state expiry yeah before we go into estate expiration. Which I know is a big topic I'm still trying to to reason about like the kind of the wallet experience and user. Experience of having having these like multiple dimensions for for for gas like. What is the expected behavior here for for wallets to be clear right like tomorrow like. Single dimension or multi-dimension from a white point of view if you want to estimate it's the same problem. Right like it's like in the single like a cost model like like you aggregate everything into one you have. To actually you have like a function right that just aggregates but you do have to estimate your your. Bid for each things so it's not it's actually like a funny thing that that you have except maybe the tools you. Have for discovering price are not as great because it's all implicit okay so so a wallet can can do a. Pre-flight can tell me like the expected cost I can you know bid bit over but how do I know how to divide that. Between like the the compute and the I o well like it's it's the same in you know like like i. Said like it it doesn't that question is not a question around multi-dimension versus. Single dimension because that that like if you want to multi like if you want to say like i. Want to pay 10 percent more for on top of the market rate right for storage let's say. Because that's where there is contention you have to know that that's exactly what you you have to have. Like you have to have the yeah market price for for storage if you just layer like 10 flat on. Everything you're just going to above overbid which is maybe okay right like for some. People if the fees are relatively low you know what's the difference between you know. you know half a lumen and two-thirds of aluminum or something I don't know like historically we've seen that . Like in some situations people are getting are bidding very high on on certain . For certain patterns it would be great to maybe and forgive me if it's already in the cap. But just like understanding what is the like the what's the expected wallet strategy. Or client strategy here when like in terms of user experience like what do they present to. The user and what you know what kind of inputs do they expect from the users yeah sure. Okay let's talk about state exploration nico where is it well so state exploration. Yeah goes kind of hand to hand with the model that I have there for storing data on the ledger so like the in the. Proposal it's basically like there are two parts to it there's the how do you model. A write and as in and so writers can be a create a ledger entry or an update and how does it work . Like how do we have like the the right price basically for the cost of storage . So in the proposal what I what I did is I basically used as an approximation for. for the cluster storage the bucket list size so like the some basically like the. The ledger is organized into those like 19 buckets I think it's 19 and then . it you if you if you do a an update or a create you basically append that to. The to the very first bucket in the bucket list so that's how basically like based on the. Size of the the the total size the or total size of the ledger I allocate like a a price function . That kind of looks like an exponential from fall like basically it starts with a slow slope. Up to some number let's say you say oh like validators here kind of determining. Those parameters but like you can think of it as the leaders say oh yeah right now we are running on. Drives with I don't know like 25 gigs or 50 gigs of space right and they're going to basically set. Parameters such that they don't have to kind of buy new drives you know like if. There's too much traffic so so the price function is basically looks in this case like you have like your. Normal slope that goes to in like I don't know let's say you have 100 gig and it would be like. I want to use maybe like the first 80 gig at a rate that's going to be like a good rate but not. Not like overly aggressive and then the last 20 gigs I want to really slow down like the the growth. So that's right like from 5 looks like this hockey stick type of shape right like an exponential. And that's kind of the model for pricing growing the bucket list so you have that for that's four rights. Then the problem is that this is only like this is like saying okay you can add to the bucket list but then . Like and and by the way like if you delete entries eventually those get collapsed into the. Buckets and so the bucket is shrinks in that model a delete you still pay for or delete actually because delete. Is actually adding a little bit of of data to the bucket list so that's like first thing to note. Here and then yeah what I wanted to get here is as a kind of more like a desired property is that i. Want the price of storage for people to to kind of be the same for everybody regardless if they. Signed up for you know created an account like two years ago or you know in five years. it should be over time same cost and there should be no way to do like to have like a free ride on the on on. The on the ledger right like so you shouldn't be able to have like store I don't know like. Nfts like jpegs whatever on chain you pay for for this when storage is cheap. And then now you have like something that is cheaper than than even storing in aws right like that. That doesn't make any sense so this in with that said there's then a need for having some way. Of kind of resetting in a way the the the price of of storage over time and the mechanism that I use there is. State expiration so state expiration here means that you have to basically . Pay for market market price of storage to maintain a ledger entry on on the like a live in the in the ledger. If you do not pay for this for those for this for your brands basically you get perched that's kind of the. The choice that I made in this cap there are a bunch of other ways that can be done that are actually. Mentioned in the recap in the other approaches but like the the reason this kind of works with the other. Mechanism is that basically like if you if you set a policy for example by default you have to. Pay rent like a refresh every year let's say and then you don't pay your your your renewal after a year. yeah your the data gets deleted and yeah so there's this kind of constant churn I guess on the ledger. Which is kind of a new pattern and that construction is basically a way to to guarantee that everybody. In the last year has has been paying basically something that is market rate how long do you expect the. Like the how far in the f like when I trade a letter entry how far is the maximum expiration date. That I can choose or that will get chosen for me in the future this stuff isn't as far as I can tell it's not. Specified in the cap how that works so right so so right now the cap what it says is that it do not. So you can renew indefinitely right like the the renewal window is determined by validators. So that's why I said it's like a like you say every year you have to pay run right and then every time you write you. Do an append that happen is valid for a year right that's good that's demo then. Isn't there a natural trade-off between like renewal time and like fluctuations in this price of storage or. Are we expecting like this storage cost to not increase too quickly well it depends like what we've seen. On the on the current network is ledger size has been increasing actually rapidly over the last few months. Because of like some strange token activity which is not entirely I mean it's it's not. I mean there are a combination of factors like one is yeah like just price of in. Crypto assets go you know going down but also like when they were still pretty high you had like an incentive to. To create more crypto assets so it basically those things kind of cancel each other and the growth has been . Pretty significant so I would say like seeing a growth rate that takes you to. yeah something that will be you want the market basically to kind of get to an eco equilibrium right like. Where where like you do not have like like those weird use cases appearing on the network if they if they. Are like cheaper than right now I think on on the on the network the the the problem we have is. yeah we are cheaper than aws fault in some situations isn't there a trade-off here between. Not necessarily a trade-off but isn't there a consequence here that people will have to go and touch their data. From time to time and people are procrastinators and like let's say that like the expiration date is you know when. You're in the future or something everybody at the end of that year then has to go and touch all their data and. There's gonna be a huge log jam to get it done well that's there would be a large jump. If everybody creates their stuff at the same time but you would that's not the case it's going to be. You know like basically like the thing that expires in a year is the whatever happened today right like. At the given date right but I mean like imagine that today you have a day with like a lot of. Activity like you can look back historically of Stellar's history and there are periods when there were like. Lots of token creations and stuff you know there are days when there's hundreds of thousands of blood draw. Entries created and then abundant well many of them are abandoned but like you could imagine a world where they're. Not all abandoned right and then what happens a year in the future well nothing well people are. Incentivized to come back some time between here and then I don't I don't see this as. Any worse than the fact that we have to handle load spikes in general yeah I mean we have to handle load spikes and. Let's bikes may get replicated but yeah like what so what I sketched or what we. Sketched actually in the cup which is you know just a more of a strowman type of thing because I'm sure we can do. Better than that is is we actually are kind of ensuring that you do not have like. Giant spikes so like it like I think the spike would be not be because of situations don't like what you're. What you mentioned because actually activity from today if you have like a comp a a. A a linear like a translation right like an actual just shapes right like all this activity gets. Translated exactly a year from now I you don't have a problem I think the it's just like additional. cost of running a validator right like it's let's you say okay I need to when I set my limits right the number of. Rights I actually have to think about well actually my capacity in rights is half. Of of what I can add to the ledger because I also need to delete right but what can happen is more of a. Like if we have different expiration times which I actually kind of briefly talked about there. Is that if you have different expiration times you can have actually different dates. That end up expiring at the same date and and that's for that for those type of situations you have to. Have an algorithm that kind of smooths things out and that doesn't actually cause the system to. Kind of create a gigantic spike you know at a specific dates so just we don't have a lot of time but. I just want to ask what like the biggest question I think that there is like what is the what's the expected. Behavior here like you know these ledger entries are representing financial instruments. let's say assets just for simplicity even though we have like a standard asset contract. And I'm you know paddling on shares of something is what is the expectations. Or what is what's expectations there's like am I supposed to like once a year like come and touch this. Is like the operator of this financial instrument supposed to do that for me if. You know if you look at you know various common immutable contracts like uni swap you know and so. And and I'm holding on like these uni tokens what's the expectation here like who's. Going to touch these for me right so in the in the in the cap I actually let this kind of. Flexible like there's a when you there's actually a special host function to that you can call that is. Basically a rewrite equivalent to like a rewrite ledger entry so that you refresh the. That that expiration time anybody can do that all right like there's no it's not I understand that anyone can do. That but who is who do you expect to do that well it depends on the type of users. Right like like power users probably don't want to do it themselves like other situations you know if you're if. You're like you said like this very passive type of person maybe you should pay somebody to maintain your. Stuff if that's what you really want in other situations I suspect if people are not active they probably. Should just be using centralized infrastructure like you know contracts or whatever that. Are a little more centralized I think trump's point is that there's a free rider problem here like imagine. That all the people in this room are using a single contract right like which one of us can touch me all of us. Have an incentive to wait until the last second and play chicken and hope that somebody else oh like yeah for a short. Contract I think that yeah well it's if it's a shared goodness like just. I that's not what actually what I asked john I assumed that like each of us will have like our. Own ledger entry within that contract so maybe did I not understand correctly I think I think I'm concerned about. People's money vanishing into thin air which is completely reasonable these are balances and we're just going to delete. Them that's not that's not super great I recall there being a proposal to like have . But what happened to this like when you have a ledger engine deleted it like gets . Dumped into like some kind of merkle try and you store the hat root of that try and then like when I want to bring it. Back I can like bring in a proof that this is what the state was right so this is actually in the appendix yeah the. Alternative section so this there is actually a very detailed proposal in ethereum foreign v2. about this so the complications from this archive approach is is when you want to. So like restoring is actually yeah like a trivial like I said like you know you have like a maybe a way to do like a. To just to basically store that that entry in in inside a merkle tri of source right and then. You just need to provide the proof for that the complexity comes from when you want to create an entry. Because you have to prove that that entry doesn't exist in historical data like that was actually archived. And that gets really nasty very fast right right sure but like that that like not wanting to. Do that doesn't address creating this question like what do you do if your money gets. Deleted it's it's it's an event like like you know if it's a like with an issue it's like today if. You're on the seller network if you're sending back to the issuer you know you basically. Burn it you can ask the issuer like hey sorry I didn't mean to do that but but I didn't burn it. But like what if you're like what if it's not like you know an off-chain issuer that you can appeal to. Like what if your unit swap lp tokens get get deleted what do you do it's tough yeah like you know. These those are the rules of the network wait but that's not that's not a great solution long term. Right like we're gonna have a lot of people consider the alternative right which is. Infinite growth of a ledger with infinite price which one do you prefer I mean like. Objectively like if somebody had a million dollars of like uniform lp shares get liquidated. It would have been better to pay for a million dollars of storage so that one person yes but like what. About everybody else and that person with a million dollars like if they have that it's kind of like. Key management like you you have procedures to make sure that you don't lose your million dollars it. Is it is currently the case that people with million dollar balances can in fact lose them because they can lose their. Keys so there's there is there is something to appeal to here like it is it is actually possible for you to lose. Money just by misusing the system but what about the other end of the spectrum though somebody who doesn't have a. Million dollars they have a small amount of their balance and they're just constantly eating that up by paying. These fees to keep their balance alive I mean it sort of reminds me of like bank accounts. Where you're like bank accounts just disappearing because you're paying all these fees. Yeah they do eventually disappear if you if you put five dollars in a bank account and then wait for 20 years it'll. Go away I mean this just points to yeah you're not stirring your your balance in the. Right place like this is shared infrastructure like if you don't if you don't use it you. Lose it but nikki you can't just like ignore the entire industry that we're in and you know I'm. Not pretending that this is not a problem like people are overlooking this problem. But you know it's I think it will be really difficult to bring people into Stellar. Telling them oh this is the way it works in selena right so it's a fader no it's not selena doesn't. Actually do any of this right now they have rant no they don't they literally don't. Charge it right now it's a to be done in the future feature that no one wants so they're never actually going to get. Around to doing it they have infinite ledger in memory they they allow they charge you money to. Make to allocate space but they never actually reclaim it there's no there's no active garbage collection process. So we're over time at this point and I think that means that we have to stop I mean I know that this is an interesting. Conversation and there's seems like there's a lot to say about the concept of expiration. but I think we'll push it to next week's meeting and hopefully have some of this discussion. On the Stellar dev mailing list and here also in discord in the various john cannon channels so if anyone is watching. And has thoughts about that feel free to join the Stellar dev mailing list or to chime in on the discord here we'll. Continue to share work and ideas and conversations debates as they happen and we will see. You here again soon thanks everybody.
