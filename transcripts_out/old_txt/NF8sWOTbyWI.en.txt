okay so I think we're gonna
get started and hopefully david will
soon join
so hey everyone welcome to another
protocol meeting I'm filling in for
justin today
I am seeing this so in these meetings
we discuss potential protocol changes
these changes are outlined in these
documents called caps or advancement
proposals
and
the big change we're working on right
now is project jump cannon which are
which is a feature to introduce native
smart contracts on Stellar
so we've divided this massive change
into a set of composable caps
and the agenda specifically for today
is we're going to talk about cap 53
smart contract data this was recently
published by graden
we're going to talk about cap 52
smart contract interactions
minimal which was recently introduced by
john
and we're going to talk about the smart
contract life cycle cat 47
which was recently updated by siddharth
and
requires some
for the discussion so let's do this
graded can you kick this off with a
review of cap53
yeah just give me one second all right
got it yeah sorry I just I had the
pull request open and then I actually
wanted to switch over to the merged
version of it
so this is a fairly straightforward cap
it's not it's not really introducing
anything that
probably will be a surprise to anyone
here it's just formalizing something
that was left out or left sort of for
future caps in the modularization that
we've been doing
splitting off conversation into
different pieces so that we can work on
them separately and land them separately
but it's fairly tightly related to the
data model that was presented in cap 46
so a lot of the motives in cap 46
around the data model
implicitly talked about how
how that data would be stored
permanently so there's there's
there's concerns that that bear on the
data model that are sort of
interacting with it while it's in memory
and then there are concerns more
related to its long-lived
accessibility over over multiple
invocations of a smart contract well
it's stored on the blockchain so
some of the requirements that are
sort of
rephrased and and brought into the
foreground here
have to do with interoperability
where we we want there to be something a
little bit
a little bit more general
or I say sort of a less general more
interoperable more generally understood
more widely understood structure to the
data than just a byte buffer a lot of
smart contract platforms essentially
only provide a byte buffer storage
service to smart contracts which means
that nobody really accepts that exact
version of the smart contract code can
necessarily read any of the data
that's stored there and that that
produces interoperability problems if
other third parties want to access it
offline you know browsers that want to
take a look at the data it also creates
versioning problems because it means
that you're
originally locked to the
schema language
or serialization format that the
contract used it means the contract
wants to pass data it has to from one
contract to another it has to transform
it so there's a whole interoperability
angle here which we wanted to address
in in the cat 46 data model and we're
carrying that forward here and I think a
lot of the the concerns
only make sense when considered in
terms of persistent data but this is the
persistent data
cap so so here we're just talking
about basically what the ledger entries
is gonna store things looks like and
a handful of host functions for for
accessing it they're very basic
functions they're just key values to our
access functions they're point access
functions they're not range range
functions they don't include
iterators or range queries or anything
like that they're just get put
does do we have a key and delete the key
very simple host functions the
interesting thing really is
the choice of granularity
which is left to the user so this is a
little bit different in in that
many
blockchains
provide a
this is this is different than many many
smart contract systems you'll see many
smart contract systems provide a key
value store which is keyed by a byte
string or some some kind of a prefix
that goes into a merkle tree or
something like that

we do not have
we essentially don't expose any interior
nodes of the mercalli storage that we
use to use this anyway
we just provide a single bucket list
hash the fact that our data structure
has internal localization doesn't really
apply here anyway
and structurally it wouldn't make
sense
but


what we're what we're doing here is also
allowing structured values as keys
rather than
rather than byte string so of course you
know you can serialize any value and
you will in the case of using it as a
string here but the api is encouraging
users to have fairly structured values
so they can have fairly rich keys

and
implicit to all of this is that
there's there's a parallel access and
consistency model
that's discussed in this cap

which is that we're we're trying to
encourage the possibility of executing
smart contracts in parallel and if you
have parallel access to
a data store
you have to talk about what the
consistency model is what is it what
does it mean when two different
users access that that model
in parallel so we're specifying here
that it's a serializability consistency
model which is the strongest possible
it says you know equivalent to the exact
order that the transaction sets
specified the transactions executing in
that has to be the observable
side effect model
and
parallel models parallel consistency
models imply the existence of some
kind of a concurrency control mechanism
how how you actually enforce that
and
in in this
cap we're talking a little bit about
a very strong mechanism for
concurrency control it's what's it's
what's typically called deterministic
scheduling or non-conflicting
concurrency control
the idea is that every transaction
that enters the system will pre-declare
a footprint so there's this thing called
the footprint which is the the set of
keys that a transaction is going to
touch whether it's going to read them or
write them it actually marks whether it
reads or writes each key in
its footprint and the footprint is is
static information that accompanies a
transaction so
this cap doesn't describe exactly how a
footprint is encoded or accompanies a
transaction location because we don't
even have a cap open right now that has
transaction invocation
or at least we haven't settled on one we
have several caps open right now
but when transaction invocation
occurs it it's going to need to provide
a footprint in in this cap this cap
is asking that that
footprints are available and the
footprint defines

the keys that are
that the transaction is allowed to
perform these data access operations on
so if you try to perform a get again
something that's not in your footprint
the get will fail even even if the
the value is there if it's not
in the transaction's implicit footprint

it's defined as failing similarly for
a put or even has a point query anything
like that
you have to have it in the footprint so
so for simple transactions this is
fairly straightforward you can tell what
they're going to read or write and so
you just put things in the footprint
that they're going to read or write
that's fine
for
complicated transactions that have a
highly dynamic behavior maybe maybe it's
not even clear what they're going to
read or write because it's it's you know
subsequent to a transaction
it's it's determined by an earlier read
in the transaction these are what are
called
transactions with dynamic footprints
the
recommendation in this in this gap
and what we what we're prototyping is a
fairly standard technique from the
literature which is often called
reconnaissance queries I think I'm
using them here I'm using the term
recording footprint recording here which
is that you just run the transaction
offline before you submit it on on a
read snapshot and that gives you
a a fairly good guess and an approximate
footprint that you can then staple to
the real transaction when you submit it
and it will succeed
if that footprint still matches so it
essentially pushes concurrency control
out of the the transaction processing
loop and into the user's lap and so
the the user is now racing
on divergence between read snapshots
that they use to construct their
footprint and the footprint that they
actually submit a transaction with and
so theoretically if there's a very very
highly contended key and it's a very
different query they may have to retry
multiple times because if they get if
there's any significant divergence
between the the recorded footprint and
the footprint they submit their
transaction could fail
but the
database itself doesn't have to actually
perform the concurrency control so in
some ways this is shedding load from a
concurrency control mechanism inside of
a database out to the users

and
that has
turned out to work very well for for
maintaining a very even high throughput
on
existing databases that adopt this
technique so we're trying to adopt that
technique as well
so those are the the two sort of main
topics in here that the fact that
the user has control over granularity i
should go back and talk a little bit
more about granularity just for a second
which is that the the granularity
control that exists here
it has a natural tension in it so it's
so
doing a point read on a key value store
necessarily has some overhead it has it
has data framing overhead it has
serialization overhead it involves going
to the I o system at all it involves
touching the disk doing a seek doing a
read
all of that overhead is


potentially quite high and and so it's
it can be worth trying to amortize that
overhead and read more than one item if
you are going to access more than one
item you don't necessarily
want to to pay that on on a bit by
bit or bite by bite basis you want to
bring in a bunch of bytes at a time when
you do an I o and so that amortization
tension pushes you towards
larger ios
in a larger granularity of storage
but then the flip side of that of course
is that if you read or write data that
you don't actually need if it's if it's
wasted and you actually only wanted to
change one byte in the middle of a large
data structure that's waste and you're
paying for that waste in terms of you
know fees or cpu time or io or whatever
so that that pushes you in the opposite
direction of having
fine grained
data and

that problem actually just magnifies
itself when you start talking about
parallelism because again your footprint
is a unit of contention and so if two
transactions contend on the same data
value
they can't execute in parallel
basically that's that's that's what the
footprint is is doing is that it's
giving a static scheduler the
opportunity to
partition execution into separate lanes
and then those lanes will run with no
coordination
but
those lanes necessarily are serial
themselves you only get parallels
between them and so if you have
a whole lot of transactions for example
that all touch
some common
data value in their footprint they will
all be scheduled to run in serial
and
so to exploit parallelism it is it is in
the favor of the user to have a
finer grained footprints so
so you have this sort of
two different directions of pressure
it's a natural trade-off between
fine-grained and coarse-grained
data access and so we don't specify what
what the granularity is here we try to
be very open about that and and so
that's why the key type is is
literally just an arbitrary value

I think that's all I really had to say
about that there's not a lot in this in
this gap it's actually quite small and
it it kind of just does exactly what
you're expected as a key value type
awesome can you
just quickly talk about the rationale
for why there is point access only
yeah so

range queries essentially aren't
compatible with static footprints
because you know we don't we don't know
how far they go
that's that's the simple version
we we could theoretically but we would
lose parallelism so
yeah
awesome so we are
kind of like actively trying to deter

you know contract developers from
creating these like
you know creating like a need for
rangers with some you know like for
example like a classic or the book is
probably like not a great fit for this
which is
okay with us
yeah and I mean there
it's a good point which is that in in in
in a broader sense
a static footprint actually bounds the i
o you're going to do it allows us i
mentioned this in in the contract it it
it allows us to
essentially have no surprises the
contract is not going to be interrupted
in the middle of the contract in order
to actually go touch the disk
dynamically everything that it's going
to read
it says upfront and therefore we can
just do a bulk read at the beginning of
the contract

just in fact we'll integrate into a
single pass through the the storage
system all all of the reads from all
of the contracts in a given
parallel execution lane
will just read all their data at once at
the beginning of a transaction set
execution and then write it back at the
end so that

that kind of thing is is naturally
incompatible with something like dynamic
range queries
but that said
because you can store you know arbitrary
values if you want to store a map
that's that's completely reasonable one
of the values that you store it doesn't
have to be just a small string or a
number or something you can store
a map that has a bunch of stuff in it
and then do a range query on that map
it's just that when you do an I o you're
going to get the entire map is going to
come off the disk so you have to sort of
navigate that trade off yourself maybe
shard your map into a bunch of different
sub maps or something like that if
you're interested in not loading and
storing the whole thing every time
when I tried to to use a very early
version of this design
like a month ago or something
one of the kind of like
annoyances that I quickly encountered
was like
when I wanted to
you know partition my namespace i
basically was like okay well
I need some kind of key that is a tuple
and so I just used like the sc valve
option
and I just like
pumped a vec full of stuff and then use
it as my key that's I got like a
petition namespace the thing was like
doing that seemed like kind of
inefficient
because it's like okay like I need a
three like you know a three tuple as my
key so I like go I create a vector
host function
I push into it post function I push into
it again host function I push into again
host function
and then I call
the like you know get function host
function again
and it just seems like a ton of work to
get a single piece of data
so do you have any thoughts about that

off hand I guess I'm I'm
I'm not sure that it is a ton of work
like it would would be my first reaction
in the sense that


I don't
know
so so so for example you know we could
we could make a
contract put one contract put two
contract put three that takes three
values as inputs
three three keys four keys five keys you
know we could we could
reflect those usage patterns in function
signatures as conveniences but I'm not
sure they would do any less work
and I don't think the calls in and out
of the vm are actually all that
expensive I think you're only talking
about one extra op code and
a couple of like like a push and a call
so from a user perspective I think you
have a good point and
I think
if the sdk can't make that pattern
fairly convenient in terms of
putting you know
sort of a superficial
porcelain on top of it that makes it
makes it look nice
then perhaps we should expand the
functional repertoire to provide
additional support for that
I think one thing you might be able to
do
just responding to your comment about
the sdk is
you know make it easy to use things like
tuples like
trying to make it easier to use things
like vex already
and
maps
you know tuples might just that just
might be one thing we could we could
have
yeah that's that's kind of what I'm
expecting is that you can you can do the
the kind of thing that that
you know i
hate to use this as precedent but but
the the raw standard library does
something
similar here where it just says you know
the people use tuffles up to about five
or seven or twelve or whatever so like
it just you know macrogenerate enough
support for all the basic temple types
that anyone's like likely to use and and
just have them as conveniences and then
you only wonder how to use this
arbitrary vector sort of approach if
you're if you're doing something weird
yeah I mean as as long as the cost of
doing all the push functions isn't
particularly high
then it it doesn't really matter to me
because you could always put sdk support
to do this indeed I just built my own
thing that like I could pass the
functions to and inside the parameters
too and it would through the vect
back out at me so it would look a lot
less disgusting
we could generalize that of course as
long as the cost isn't high at the
protocol level so
I think so I think the cost of a
function call is is fairly
small function call and I again
absolutely it's the case that if if we
measure this and it's miserable I mean
the other thing is that I don't I don't
honestly think there's
masses of I o operations in the normal
contract path right I think you're only
talking about a couple of
point accesses per contract call anyway
so
I'm I'm not super concerned about that
path but if if if we measure it it turns
out to be expensive we can absolutely
revisit this and try to you know add
fast pass or optimize versions for this
braden d do you think they could be
contracts that will be vulnerable to
moving footprints so you mentioned about
the situation where dynamic footprints
is an inconvenience or you know you have
to do reconnaissance queries and they
potentially could be out of date but I'm
wondering if there's an angle here where
that actually makes the contract
vulnerable in the sense that you know
one participant of that contra contract
could prevent another participant from
interacting with it
yes absolutely this is
as far as I can tell this is basically
always the case with concurrency control
if you have any kind of concurrency
control mechanism somewhere
you can create a
starvation you can even you can survive
one party by by just hammering on a
contended resource
in this particular case the user has the
contract developer has a fair amount of
control over it because they can change
the granularity so if a contract
developer
feels that this is a risk or sees this
happening or something like that
they can re-architect the contract to
essentially sacrifice concurrency


to get rid of the ability to
have that kind of concern so so you know
at the extreme end your footprint is
the contract data there's only one
contract data
everyone who talks to this contract
always accesses the exact same contract
data and that means that everyone knows
exactly what their footprint should be
it's always just the contract data
there's only one element
everyone specifies the same thing and
they all get serialized and so there's
no you can never you can never have to
to
have your footprint invalidate because
your footprint is always correct
so you can do that if you
find that's happening
it's just the worst case right so you
you move away from that if you want more
more concurrency but if if you're seeing
that people are able to

able to and and actively exploiting you
know some kind of of starvation
concurrency starvation situation then
then you may have to move back towards
that and
I don't
I don't personally know a way to avoid
that I think if we did any kind of
dynamic locking we would be in exactly
the same situation where someone could
just flood the system with with
transactions that take a lock and and
deny anyone else the ability to make
progress
and
we would be in a worse situation because
dynamic and currency the the thing
that's really good about static and
currency control as an approach
is that you you have a guaranteed
sort of throughput that the the
things that you have scheduled
will complete one way or the other
within their allotted time slice right
they will they will either finish or
they will abort and so your abort
rates go up
but the system continues to run
in in this particular
strategy the other strategy would be
more like we would give people the
ability to drag the entire system down
so a transaction set would would
potentially slow down dramatically
because people are contending on a hot
resource so
it's there's there's there aren't a lot
of free lunches in concurrency control
and and I kind of feel like that's a
natural trait of you
that answer the question I know it's not
like a fun answer
no yeah I think I think that makes a lot
of sense
the fact that the proposal really gives
the contract developer a lot of control
and doesn't define the level of
granularity that they have to use
I like yeah it's good

well great it looks like you've created
one of the least contentious caps that
has ever
came to life thanks for that
great
cool so let's let's dial up on the
contentious or the contention levels
john I think this is probably the third
iteration of smart contract
interactions
so for those of you who are here at last
week's meeting you might recall that we
had a pretty big debate about this
and
I was in the interest of actually
agreeing on something I decided to just
remove all the functionality from the
proposal which sounds a little backwards
but
in the context of smart contracts you
can kind of
put all of these authorization questions
down to contracts and let them do them
do everything themselves
and we had been kind of moving in that
direction on cap 50 anyway
with the introduction of like the
invoker signature none
option and stuff like that
so basically at a high level of what's
in this proposal
there is a just like in the old proposal
there is a new operation sorry a new
transaction type and a corresponding
envelope type called
invoke contract transaction
or invo contract transaction envelope
and this and this contains the normal
stuff like before source account
sequence number fee
it contains the contract that you're
invoking you know the id
the parameter the symbol the parameters
it contains the read write set that you
would need for
cap 53 as grading was just talking about
and then

that's pretty much it
so how does one actually use this there
are some examples in the cap which were
pretty instructive about what what the
universe would look like if we actually
did this
there's a whole example section where
I hacked up
like two versions of an erc20 type
contract
but they look quite different from your
normal erc20
because there's not really like a
reliable message.sender that you can use
in this context

but
basically
the only other thing that's here
is just a few
a few host functions
that are useful for actually doing some
of the things that we discussed last
week
so there's some access to thresholds
from accounts there's access to
getting the signer weight by account key
by signer key the signer weight for an
account by signer key kind of hard to
say
I added a verified 25 519 function
which I think is also in cap 51 which is
being written at the same time
and that's that's pretty much it
so any questions about this
so I'm I'm just
just trying to
get into this can you talk a bit about
like the implications around like
accounts and you know we were talking
about about this a bit last week like
what does this mean for like you know
classic multi-sig accounts on the on the
smart side
the beauty of it is that the proposal
means basically nothing for those things
because the contracts get to make
their own decisions so
for some context like if you scroll down
almost all the way to the bottom
the last example is like a sim a simple
token based on account signatures
and this builds on the classic Stellar
multisig
mechanism

basically works exactly the same
with two exceptions no pre-signed
transactions here oh sorry no pre-auth
transactions
and no hashtag signers
but it works exactly the same and it
works at medium threshold and everything
kind of is exactly what you would expect

but right above that there's another
example that uses the like the single
key version that I was proposing as the
invoker signature last
week
and so this this framework basically
lets contracts build whatever they want

if you want some kind of
you know support for seller multisig
that'll really be up to contract
developers and ecosystem standards and
stuff like that
my intuition is that those things won't
really materialize
because they're not efficient structures
on the blockchain but they might they
might materialize from case to case
something that immediately jumps out at
me with these examples is that it may be
difficult to write these functions
some of these sorry I'm looking at
the simple token based on account
signatures example you were just
referencing
and the second code block has a check
function it says internal function is
that something that
the network is providing or that's an
internal function that the contract
provides
that's a contract function that's not
exported
so I can imagine
so i've had to write code like this
for
for our sdks when we were
implementing sept10 and
I was yeah I think it was septum
and one thing that
is quite difficult to get riders
iterating over a set of signatures
for a message
and determining a set of weights
because there's different things you
have to do like you have to make sure
that you don't
if somebody includes the same signature
twice
you don't use it twice to get you know
double the weight
different things like that
so do you think by going this approach
expecting people to implement their own
authorization
we're increasing the chance of foot guns
where people are going to implement what
they think is Stellar maori sig
signature authentic authorization
verification but it doesn't actually
exactly line up with it
that that's definitely possible

I mean my like kind of ideal universe
here
in the sense of like what I hope people
would do is probably somebody would
deploy one contract that has like a
unified key structure a unified signer
scheme basically you pass it some kind
of opaque blob the beginning of the
opaque blob is a discriminant saying
like hey what kind of signature is this
is it a single key ed-25519 is it a
single key ecdsa
is it a
is it like a Stellar multi-sig is it
some other scheme that I'm not thinking
of like some kind of like quantum reset
resistance scheme who cares
and basically like the entire
ecosystem relies on this contract or
sorry another example would be like i
know lee you had requested like aliases
we could have like a single ed 25 519
alias version all of that implemented in
one contract that everybody kind of
relies on as an ecosystem standard you
don't have to rely on it but if you do
you kind of get compatibility across the
entire universe for free
that's what I would hope would happen
instead of everybody rolling their own
but like yes at the very worst case
everybody rolls their own and if you
don't know how to roll your own for
example like I didn't account for the
for the repeated keys in this example
working too fast
you can get yourself in trouble
and just just to follow up on that
part of why I'm a big proponent of
single key signatures and doing
everything else is like you know secure
multiple-party communication
computation
is because there's a lot less ways to
blow yourself up on chain a single
signature is easy to verify
in fact my argument would generally be
that if you want to write a good
contract that is really safe and easy to
audit you should use the simplest
authorization scheme possible
which is that
all right can I ask a follow-up question
is that I'm I'm a little bit
well i'll be honest I'm a little bit
behind on on this entire aspect of the
interactions

if you're dealing with a case where
people do use the simplest and safest
approach
but you know suppose you're a smart
contract author who's trying to be
conservative and you don't want to do
anything too elaborate
and you're using this interface am i
correct in reading this that you are
probably
you're probably not going to have to
include an awful lot of code in your
contract to make this work right
is that correct that the the sort of the
number of calls that you have to make to
host functions is not particularly huge
you're talking about in the case of like
a single 80 25 590 right that's right
yeah in that case like it's very very
simple
that example is in the first one
and like the code is basically like
check the nonce
hash your hasher you know message do a
do a 80 25 5 19 post function call
that's pretty much it
and everything just traps if the wrong
thing happens and
you could probably write this in like
five lines three host function calls or
four host function calls pretty
pretty lightweight all told
and it's pretty hard to escape
all of like some of these parts no
matter what you do like at some point if
you're going to do the authorization
on-chain you probably need to do at
least one 80 25 5 19 signature
verification or ecdsa signature
verification
so I it probably could be like a little
lighter than this but probably not
significantly lighter than this
well I guess this
I'm I'm just being my typical trying
to shave things down approach
this feels to me like
even the minimal version is a
blob of code that will have to get
stapled onto every single smart contract
and they will all run
you know even even if they all wind up
being conservative and they'll take your
advice and be conservative
this is all in vm rather than extra
vm there's no there's no way for them to
say
fastpath may just do this conservative
thing
so
yes and no
because like
even though I think that everybody will
be conservative I still think the ideal
universe of everybody being conservative
is them all using a single
you know contract that implements this
all so that you don't end up with the
same code cloned everywhere you just
have a cross-contract call you probably
think that's worse than it is from a
performance perspective
but it doesn't end up with like 10 000
or a million copies of the same code
everywhere on the blockchain so it's the
plus
the the second part of it though is
that if there is a lot of ecosystem
adapt adoption around some kind of
standardized
signature verifier we could always
deploy a native version of that that's
super fast
I see I see so so so you are
I'm not I want to make sure I'm not
promising you that that's ever going to
happen no no no no I'm not I'm not
hearing your promise I'm just trying to
understand
what level of code reuse you're assuming
is going to work and also
so you know calling calling a third
party to do your authentication for you
definitely gets us into the question
that is the other thing we're going to
be discussing today which is mutability
of contracts and like you know
versioning your dependencies like i
think if there's anything someone is
going to not want to trust to a third
party it's it's the authentication path
unless they're 100 sure that that
third party is
you know
immutable and like the code that they
audited the last time they read it
I think another option is you know
you can it could be a cross-contract
call to this one contract that's living
in one place so we're talking about
reduced wasm size
it could also just be the library
code that everybody's sharing
so
people understand
there is no mechanism for library
sharing besides stapling this like
including the code into the contract
yeah yeah it's all right so obviously
there's no
space saving like all these contracts
are going to have the same code within
them
but addressing the the concern of people
implementing these things correctly
if everyone's using this common piece of
library code

that has either been audited or
people generally have more trust in
then then
then they don't really you don't really
have to worry so much about the
mutability concern because they're
choosing to build that into their
contract at build time
there are some trade-offs there like
I I think it would generally be a
thing that would would would occur that
people would probably provide some of
these like very standardized off
functions like single key or you know
based off of seller accounts or stuff in
a library that you can use
but if you want like a stateful system
like lee you were talking about aliases
and that's why I keep coming back to
this like the stateful system is only
really useful if your state lives in a
centralized place that
that people can rely on you know like
it would be really annoying for me to
have to go and set my alias in every
single contract that I use
like I could
but it just that just seems really
irritating
I think people would much prefer a
system where if that's an option
there is some global contract for that
state limits basically
I guess well I guess people could you
could have a really simple contract it's
just like an aliasing contract and you
could have library code that uses that i
don't know there's a lot of options here
when yeah like it is to me super
interesting like is that
so like what how do we think about that
like when you say like
when you make so this is like the smart
wallet type of
like situation where the smart wallet is
is kind of shielding
or separating the
whatever key you're using at that time
from your
persistent id on the network

I wouldn't necessarily call it like a
smart wallet
it's kind of tangentially related i
guess it would be what I'm really saying
is like I sign using key x
but my public key always stays as y and
I can change x to x prime or x double
prime or x triple prime but my public
key always stays y
yeah so for context I think the aliasing
came about because we were talking about
how do we replicate what exists on
Stellar today in the smart world
and what we have today with mo like we
often talk about Stellar multisig but
actually the other component of stella's
multisig that we get is aliasing because
you can have an account
identifier and then you can attach other
keys to it
and so Stellar accounts provide these
two concepts aliasing and then multisig
and I think

you know there's been the concern
address
presented that you know we shouldn't
just implement the multi-sig that exists
on classic
over on smart because
there's a lot of trade-offs with doing
that but the aliasing
alone is is like a feature that I think
that's worth
worth us exploring like what will that
look like because it allows people to do
things like rotate their keys
or
have multiple
devices or using the same key
or using the same address
and if I understand correctly john
you're saying that that aliasing
capability could actually just be a
contract
yeah that's exactly what I'm saying
any other questions on cap 52
I think because it's so fresh we
probably need a bit more time to
get into the weeds
john could you elaborate a little bit on
how you see replay prevention happening
so I see in the cap that there's this
announce
that that concept exists
yeah could you elaborate a little bit
with how you see contracts would
typically do that
yeah I'm happy to do that
so this whole replay prevention thing
gets kind of annoying in this proposal
that's one of like the big downsides
of this approach that I pointed out in
cat 50 when I said like why we shouldn't
do this
which is basically like every contract
ends up implementing their own replay
prevention when wherever it's needed

and this means that like things get
pretty annoying fast for example like
on Stellar you like consider stuff
center today
you might like submit a transaction and
you know you have replay prevention on
it because the sequence number and you
also have like you know
a
a deadline effectively
you know the max time
and
you know that if you get to that point
everything is done it can't execute or
it has already executed

but like if you want that functionality
here you also have to implement the
deadlines in your contract and all these
other things
and everything just gets kind of
annoying fast basically
now again same kind of thing that i
was just talking about you could
actually because like in this approach
because everything is done by signed
messages
you can actually delegate all of this
to to like some other contract that
deals with it so you can imagine
implementing
Stellar's times you know time downs
and ledger bounds and all that other
stuff in a contract and reusing it
if you still want it or you can rebuild
in your own contract as well
but
basically there's no
generic
nonce here
and the cap goes into a little bit of
detail about like why you can't use the
sequence number
and they're they're like my original
proposal here actually
had an example where the sequence number
was like the transaction sequence number
was used as an us
but it had a couple like kind of
annoying details about it specifically
like such a contract is like really
vulnerable or such a design is really
vulnerable to
what's it called confused deputy
problems and if you try to fix the
confused deputy problems then it becomes
impossible to use the sequence number
as a replay prevention tool
so there's a lot of trade-offs here
basically like I can imagine an argument
where we just say like hey like
people should be cognizant of their
confused deputy problems etc
and we make that an option again I don't
know if I would personally feel good
about that
because confused deputy problems are
like a very difficult foot gun to deal
with I think like they're an easy thing
to overlook
so
I don't know but basically yes every
every contract is building their own
replay prevention or relying on it from
somewhere else on chain
got it I think
one nice side effect of

not exposing the transaction source
account on sequence number two the
contract is that contracts are getting
really set up for that common relay
pattern that we do see in other
ecosystems
where people design their contracts so
that
the message that's getting
signed to be used like the contract
call that's getting signed to be used on
chain
is independent of
the
participant who's actually submitting it
and paying the fee
and that that that participant could be
like a third party
that has
you know is playing that role of
relaying making sure that the
transaction is on network so
in some ways it's nice it sort of sets
up contracts to
to really work well with that because if
a contract is written to use the source
count
you don't get like you would have to
then modify the contract to make it work
with a relay
yeah definitely you mentioned this to me
like I don't know a week ago or
something and that idea really kind of
stuck in my head when I was writing this
so I totally agree with you that that's
a huge advantage of this design
I think something that i've i've heard i
think maybe grading like raises a
concern is
if we encourage people to write their
own replaying mechanisms which I don't
think we can actually really get away
from so maybe it's not worth having this
conversation but

if we encourage people to write their
own replay mechanisms people may write
replay mechanisms that are really
inefficient say is storing data on chain
forever type of
inefficient
do you think there's anything that we
can provide that'll like maybe some
utilities that we can provide in the sdk
or even in the hose functions that might
help people write
replay prevention mechanisms that are
more efficient that you know use the
ledger in a less
aggressive way
I haven't given that too much thought
honestly
but like
I I mean
my general kind of perspective on this
is you know it costs money to use the
blockchain and people will be
incentivized to do things that cost less
money
so
basically if there's a reason to do a
really inefficient replay prevention
mechanism
because it makes the rest of your
contract much simpler or maybe it's the
only way to even do it
then I think people will do that but in
the absence of that need I think people
will favor the super simple mechanism
that's
cheap whether I can guarantee that i
don't know and whether we can provide
some utilities
I'm not really sure I mean like a really
simple replay mechanism like a sequence
number is basically like you have a map
you look it up you check you increment
that's it
it it could be hard to make it much
simpler I mean like we could provide
some like library functionality that
literally does that exact thing
but the thing is if you have an account
that already has per user data you would
probably want to of like wrap the
non-sin with the other per user data
and then the helper is not actually
helpful in that case
so
that's kind of the main perspective
there but I have been thinking in
general this is like a bit of an aside
that it would be really helpful if the
sdk provided some
types like for example like there's the
sc val map type which is like a map
in the sense of like a conventional map
but i've also been thinking like
sometimes it's like you want to look at
the data as in like I have a bunch of
data stored in different ledger entries
and be cool if there was a like a map
type that did that very easily instead
of having to use like the contract put
contract gap etc in cap 53.
so maybe there's some interplay between
that and a replay prevention mechanism
that we could learn from
yeah I mean I think that
yeah all those things are going to
like as we develop like even basic
applications will factor you know like
this type of
basic functionality in some
traits right that that people would just
use
I'm not actually too concerned about
yeah people having to write it because
we are going to write it
I think for more like maybe like
different type of like replay
prevention
like
I think that's kind of the nice thing
about this proposal is that I know in
the past we discussed like
you know potentially doing
very different things like where you
have like those

like ephemeral type of you know like
things that only work in a specific
time
period right so that you

you could in theory like replay in a
specific
window but in some design it's actually
acceptable
and then you end up with like much
simpler client-side code
so
yeah
and can I ask
it don't question but you have a
function in here called knots of
how does that work what does that do
it's just a contract internal
function I don't know if the com
comments emphasize that they don't
it's not a host function
basically it just it just reads the data
I just read the larger entry and find
the nonce in that
would probably just be a single you know
integer sort in a lever entry
non-sub in those cases is user
maintained data associated with an
address
yeah exactly contract maintain did i
want to say but yeah
okay in in general I mean I i
I'm a broken record I don't want to
waste too much time with this but I'm i
am I am extremely nervous about
suggesting that users roll their own
authentication mechanisms I think this
is just this is just asking for disaster
but
I understand that we've been around this
like
a lot so

you don't need to convince me I mean
providing the really really simple
authorization that mechanism is the only
one
I would strongly favor that approach
this just this just feels like it's
going to be a disaster you're going to
have people who completely fail to
because this this data this code path if
you get it wrong is came over for
everything and it's so easy to get it
almost right and
your tests pass and you deploy it and
you think everything's fine and then
it's not
so I'm
I would love to not have users writing
this code but
okay so
you know obviously I think we need to
dispense the value in this proposal
in the meantime we have 10 minutes
remaining and i'd love to hand it over
to
siddharth to talk a bit about the
changes to the smart contract life cycle
and potentially any
remit open questions that we need to
answer
yeah so the most recent change was a
a small one about how the
contract id which is which is now a hash
is calculated
and you can we can look at that
change it's pretty simple where the
it's created from a transaction
you hash source account a user provided
salt
and if it's a contract created within
another contract you hash the parent
contract id
and
assault provided
by the contract

I mean if there are any questions there
we can talk about it but I think the
more interesting thing are these two
other points I want to bring up one is
mutability
which is do we plan on adding
like initially the cap right now does
not have mutable contracts but the
question is
should we
leave that question open for the future
or should we just say
contracts will always be immutable
and
the second thing is
we allow contracts to be removed so the
cap has a host function
to remove the contract code entry
so I think we can start with
the immutability question
right if so if we do allow mutable
contracts in the future a big question
was how do we deal with versioning
and I was taking the approach that
let the contracts deal with it so for
contract a calls contract b and contract
b is mutable
contract may just trust that
just trust contracts bees creator
or right so
I think grading
had some issues with this

graham you want you want to talk about
this
well I think they're just
I I keep coming back to the
general sense I have that
cross-contract calls are
something like dynamic linking or or
package dependencies in software in
general right that smart contractor
software and this is this is a general
software versioning problem and in
general software
has like natural tensions around
versioning that
people frequently want to
lock to particular versions but they
also frequently want to get the newest
latest and
there's a concept of newest latest that
is compatible and a concept of newest
latest that is not compatible that is
often expressed in
major version numbers or or separate
apis or separate names for things
and
I'm concerned that we are
not

reproducing any of the infrastructure
that would be normal to
support
points on that
natural tension so I think I think it is
worth trying to provide some of the
building blocks that people are going to
provide themselves anyway because
it's it's bad enough to have like
versioning
it's worse if there are multiple
versions of the versioning system and
you have to like opt into different
versioning regimes depending on which
ecosystem you're
adhering your contract to like I i
understand there's just there's this
natural tendency in our conversations
here to try to push everything to the
ecosystem and let the ecosystem figure
stuff out let them let them develop
patterns in the smart contract space
that just
solved the problem however contract
users would like them to but that
that is not actually as much of a
solution as I think it sounds that
that really strongly introduces the
possibility of totally incompatible
regimes developing in parallel or
inadequate regimes that
miss some important aspect of the design
because they were
cobbled together in a hurry
so I wouldn't
mind us spending enough time to
be able to provide the basics which is
like
I want to pin to a version
I want to pin to

a major version and only get security
updates or I want to follow
you know any new features and additions
that people had including modifications
upgrades whatever
I feel like there's there's got to be
a future where those are those are
things that someone's going to provide
and it's
it's our position to potentially furnish
them maybe it's not but that's what
it feels like to me
but like at our layer like is it
is it just having a way for to have
like committable versus mutable
contracts and like the the versioning is
metadata basically and it's up to
when you write a contract you know to
decide how you want to use this metadata
okay like when you make a cross-contract
call at the moment a cross-contract call
only identifies a contract id
right right and it does not it does not
say
call this
but give me version five or give me
version seven or whatever like there's
there's no version information in
cross-country calling right now which
means we're essentially always
dynamically linking to either
immutable exactly the same thing or
immutable
whatever the person up to updated that
contract with
the thing is that I mean you have a
bunch of
of things that feel like if we if we do
that at the kind of protocol layer we're
kind of
printing those like for example like if
you're you're talking about different
versions right of a specific contract

like you have what is
like there are a bunch of questions
around like you know you have a 1.0 but
maybe the 1.1
is actually deployed by some somebody
else right like it's not even the same
author
like how do you
deal with those type of situations
like
who do you trust to be 1.1
I don't think we have the notion of oh
like a of actual
like
or like you know the organization or
whatever that is the
the publisher right over have a contract
yeah I agree we don't we don't have any
notion about that I'm I'm I'm advocating
for us to come up with a notion rather
than
saying but that notion like I think
it becomes
I think
in that space I don't think you can
necessarily come up with a one size fits
all
because it's not like in you know in
in like normal I mean they say like
normal software like you have a company
and they ship their their their thing
and and that's kind of it like here
and also like if you depend on on a
specific version right as a as a
my installer basically is going to just
grab
cause the os right to grab that version
that I depend on
only in a
blockchain type of situation
if that other version like the cost of
of of
keeping that other version around
is actually on the
on whoever deployed that contract
so if you say like oh I want to pin all
version like everything is always pinned
I think that that the implication there
would be well okay the publisher is now
has to keep around all versions of of
their contract
which is kind of you know
weird
okay so
is is is
could you make a a concrete proposal
here are you talking about you would
like all contracts to be mutable
immutable not immutable but mutable so
you can mutate them
and

if I ever want to have a pinned
version I vendor it is that what you're
saying
not necessarily so I think it's like
yeah you could have always vendor of
course

I mean that's a solution and I'm I'm
okay with that I just I think we have to
think through the scenario is what I'm
saying and I think if if what we're
saying is we're not going to give any
thought to the scenarios whatsoever and
then we're going to tell everyone to
proxy every single call they make
because that's the only that's the only
place they're going to have any ability
to
enforce policy
I think we're kind of
losing a good chance to to shape the
system
I think it's more like what are like the
things that are like support like a that
you get with tooling that you know we
can provide
versus things that are actually baked at
the protocol layer

like I think that if you have a way to
say
because you know you'll also also
have the other problem right of
contract discovery on the network like
you know which net which contract do i
trust you know versus the ones that
I partially trust versus the one I don't
trust right
there are some that were basically
you trust you try that contract that
even if it was mutable you trust that
they are not going to
or you're actually fine with with
them modifying it in other situations
you do want to pin because you have like
a
you know an actual
I mean maybe it's a stability thing
or whatever right there you have like
other
problems there

so so some of these data yeah like the
mutability
aspect I think that's actually yeah
that's a property of the of the contract
so that you know basically
can I even
directly depend on this from my contract
or is it potentially going to change
under me but then you have a different
problem I think that
is kind of like when you do you
know like with you know when you have
your manifest files when you're when you
decide that you're going to pin your
dependencies right in your own program
that decision
is something where I think we have to
just develop the the right tooling
but it's not something that actually the
the you know the network doesn't
shouldn't have I think an opinion on
on
you know like the this
even the schema of of how you pin
things
like I said I guess I'm saying that the
network has to provide whatever is
necessary to support it so we we come up
with what we want we come up with what
the network needs to support it and I'm
I'm not seeing that developing in our
conversations we we we talk about
like
bad things can happen and then we throw
up our hands and say obviously we can't
solve it like no no we have I think we
don't fight this one I agree we just try
to figure out what the pattern is and
then figure out what the network needs
to actually support it
right I think
it's we need to have to kind of to solve
those problems what I'm saying is that
we don't need to

because like the things that i've heard
so far they are in the context of a cap
and in the context of the cap
we are saying this is going to be like
we kind of forced a specific model at
the
protocol layer
oh and I see a raised hand who is that
I think
I think it needs to be in a cup like
it needs to be in the design rationale
for this cup like if we're saying that
we're going to provide this limited set
of functionality and you can go and do
whatever you want we still need to
provide in the design rationale this is
how we expect it will be used this is
how we expect it will solve this problem
of versioning
I mean I think like at least I can
already see like there are some gaps
here you know if we say that
you're going to do
all contracts are immutable so with an
eye
and you're going to do versioning
yourself data migration how's data
migration going to work because like
right now we have cap
53
and contracts can't access other
contracts data
so how is contract you know v1
going
its data going to be migrated over to v2
and then if we say that you know v1 and
v2 are gonna
coexist at the same time
you know how do contracts do that
that seems rather complex
yeah and especially
yeah and actually like yeah in the
context of cap 53 I agree that it's also
you would change your like if your id on
the network is your contract like your
you know the contract is your id type of
thing
you don't want to be
changing your id
like if you upgrade that contract
we got a request to speak from the
audience
I know you don't know how to take them
I'm not I don't think after
right
it's android question
hello can you
yep we can hear you great thanks
appreciate the conversation and glad
I was able to hop in I'm going to
keep it short because it's going to be
slightly off silo of what you're
discussing

I am a week away from dropping a
tokenized community with a coin that I'm
bringing over from raleigh into solana
and I want to develop on Stellar and
i've looked up online there's some
people that you know help you punch i
you know I'm just trying to understand
based on what I know
with moving forward
I mean who can I connect with so that i
can understand the liquidity and how the
Stellar network works
and how I can make sure that that same
liquidity I'm probably going to lose it
but how I can transfer over to the
Stellar network and really make sure
that I look at all aspects of where I'm
launching so they don't have to wrap and
do it differently in a month do you know
what I'm saying
yeah cassandra I appreciate the question
this specific conversation is about
a very like specific topic technical
topic
and
you can
go to the
to one of the other channels support
for example
ask that question and i'll be happy
to help you there okay thank you I'm
just looking for a personal contact I'm
not bringing in the conversation I just
I can't find anyone and i've left a
couple messages in the chat but I'm not
sure who to talk to so who is just
speaking and then i'll tag you
okay so
we're we're over time and I think
there's some really interesting
discussions here that we probably need
to continue either on the jump cannon
dev channel here on discord or on the
mailing list
so I think we should go with that
thank you all for joining and tuning in
and have a great rest of your day
you
