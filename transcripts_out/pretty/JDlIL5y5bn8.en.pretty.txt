Um the first is partial stay archival.Cap 62 and then in memory uh sorbon.State for CAP 66 and no Tom I will never.Stop using in Centric Linux distro cool.So I guess um before I get started guess.A little bit of background uh so.Protocol 23 um is kind of where the.Rubber is going to start hitting the.Road uh as far as state archival is.Concerned so um kind of some you know.Background to you when soron launched we.Of course had the interface for state.Archival with rent um and all those sort.Of stuff with the intention that.Eventually entries that have run out of.Rent will be archived and then removed.From validators uh in order to free up.Space so you don't have you know the.Issues um that come with large amounts.Of state that have to be.Maintained um and so um that's kind of.Where we're going now today um the.Interface is such that you still pay.Rent you still have to issue restores.And all the sort of things things um but.The data is not actually yet removed.From.Validators um and so that's where we're.Going so initially uh the plan was for.Protocol 23 was to have what we're.Calling full State archival and in full.State archival what happens is uh.Entries um once they have uh run out of.Rent uh be removed from the live State.And then they are added to this.Temporary data store called the hot.Archive um and the hot archive is still.Maintained by all validators but it's.Just a separate database that just.Maintains um entries that have been.Recently archived uh the thinking being.Was that eventually this hot archive.Would become full and when the hot.Archive is full what you would do is you.Would create um a Merkle tree of that.Data validators would maintain the.Merkel rout and then delete all the um.Information in the hot archive and then.You just repeat this process iteratively.So essentially you um evict entries from.The live state to the hot archive state.Eventually that hot archive cach will.Become full and then you'll actually.Delete and remove those entries from the.Validator and then um the restoration.Process once an entry has been archived.In this way if it no longer lives in the.Validator there's like a Merkel style.Proving scheme um to which you are able.To restore an entry back to the live ler.State so that's kind of what we consider.Full state archival which is where.Entries actually get deleted from.Validators but um thinking about this.Problem a little bit more and looking at.The current metrics of soron um it seems.That we're still a little too early for.This full C archival I think long term.If you look at um smart contract.Platforms that have large amounts of.State there are significant issues with.Maintaining all that state you have um.Lots of um you know Hardware.Requirements on a network like salana to.Maintain large amounts of cashes and.Then um you have a networks like.Ethereum that don't have large hard.Requirements but they're very slow just.Due to maintaining these very large.Databases so long term at scale I think.It's still very important to have the.Full State archival solution where.Entries are deleted from validators and.Then restored VI proving schemes um but.The reality of the situation is we're.Not quite there yet and I think that.Currently there is less than a gigabyte.Of soron state currently live stellar.And so going through all these um hoops.And adding all this complexity um for.This proving scheme just to delete a.Small amount of data isn't really uh.Worthwhile at this point and so that's.Why for protocol 23 instead of going the.Full cival route where we actually.Delete entries we are doing something um.Called or I'm proposing something called.Partial SE archival and this is what cap.62 um kind of explains and so in partial.State archival what we do is we do kind.Of the first half of the full state.Archival so we still maintain uh two.Different databases on the validator you.Have the live bucket list which contains.All of your live state which is the um.The Ledger that exist today and then you.Still have a um what's called the hot.Archive which is a cache of recently.Archived entries and so what you would.Do is that whenever an entry runs out of.Rent it would be evicted and removed.From the live State and added to this um.Hot archive database now the key.Distinction here with partial stay.Archival is even though you still remove.Things from the live bucket list and add.Them to to the hot archive you never.Actually delete the hot archive um the.Hot archive never becomes full such that.Uh the entries are never actually.Removed from validators and so um it's.Partial St archival because you are.Still kind of storing live state in one.Database and storing archive state in a.Different database but you're not.Actually removing any state from.Validators and so I think my current.Proposal would be in uh protocol 23 to.Implement the partial state archival.With the intention of later on in the.Future extending this to the full State.Archival solution um the only reason is.Is that I.Think that um for the uh you can the the.Size at which the um the hot archive.Becomes full and becomes deleted is.Configurable and so I think um we could.Do something reasonable such as we could.Implement on this St archival solution.But uh we could set the the capacity of.The hot archive to something very high.Like 50 GB such that it would take a.Long time and a lot of network activity.Uh to actually start deleting State and.Then you know if the network was to grow.That much such that we had 50 gigabytes.Of archived uh sorb on state then it.Would actually make more sense to start.Deleting State and requiring proofs for.All the operational benefit that we get.There um and so that's kind of um the.Current proposal of 62 is we're going to.Still maintain all information on the.Validators but we're just going to move.Archived information from one database.Uh to a different database um now the.Reason that we want to do this this.Separation of archive State and um live.State is that it actually opens up a.Large number of optimizations and so.That's what we get into in CAP 66 which.Is uh inmemory sorbon State and so uh we.Can because we have the system where the.Live State uh or the live bucket list.Holds all classic information and all.Live soron information and we have a.Completely separate database that stores.All the archive State now because of the.Rent system and because of the uh the.Way that we do write fees you know where.A write fee is a function of the total.Size of the bucket list we actually have.A way to put a soft limit on the amount.Of live soron State at all times the.Reason being is that you know with the.Um the current bucketless size if you.Were to add enough life State such that.You go beyond the target bucka size.Rights become very very expensive such.That the network um users are.Incentivized to allow entries to run out.Of rent and become archived and so.Because of the way our fee system works.Uh we have a way to essentially have a.Soft limit on the amount of State in the.Light bucket list at all times um and so.What I'm proposing is to change that fee.Slightly um such that instead of the.Sorbon right fee um corresponding to the.Size of the entire live leer of both.Classic and sorbon entries it only.Applies to the life soron entries and so.Essentially the uh the buckus target.Size instead of being a buckus size.Would change to just be the uh the life.Soron uh Target size and I think this is.Much more fair uh given that classic.Entries don't actually have to pay rent.Yet and so it's a little unfair that.Adding classic entries actually changes.The WR fees and the rent fees for sorban.Um and especially as the network exists.Today classic State dominates sorant.State size and so changes in sorant.Usage don't actually really affect soron.Right fees rather changes in classic.Usage affect soron right fees and so by.Changing the The Bucket List Target size.To a sorbon state instead of just all.Total State um we have a much more fair.Fee system but what that also allows us.To do is to prioritize live sorup on.State um above arive state so what do I.Mean by that so if we change the uh the.Way that we calculate fees to only look.At sbon size we can use the fee system.To enforce a maximum amount of Life.State at any time so for instance we.Could set the target soron State size to.1 Gigabyte and then the fee system would.Ensure that there's not much more than 1.Gigabyte of Life Sor on state at a given.Point now you could maybe you know go a.Little bit above that if people are.Willing to pay expensive fees.Um but the way that the fee growth works.Is that um you know you are reasonably.Capped to a small amount of state and so.Because we have the system where the.Amount of live sorb on state at any.Given time is fixed what we can actually.Do is just store all sorbon state in.Memory and not have disk access at all.And so that's the current proposal um in.Cap 66 is to prioritize all live uh.Soron state in memory uh and this is.Made possible because we store live.Soron state in one database and archived.State in a different database and so by.Splitting the state into two separate.Databases we can very easily just.Iterate over the the live database and.Store all that sorab on state in.Memory and so that's kind of what's.Happening um behind the scenes um as to.What the uh the validator is doing um.Now we're able to do this because of the.Maximum soron State size if we didn't.Have this and if.Sorban uh LIF state was able to grow.Unboundedly this would be a very.Dangerous optimization because.Validators might run out of ram um but.Because of the St kival system we can.Actually fix the amount of Life State.And so there's no runaway R risk and so.We can um very reasonably store all.Soron state in memory um and so there.Are some changes we need to make to the.Uh developer experience um and the user.Experience to make this possible um so.First we are going to to change uh some.Of the resource types A little bit um So.Currently today we only have one read.Resource which is read bites and read.Entries um and this assumes that all the.Information you're reading is on disk.And so what we're going to do uh or what.Cap 66 proposes is to split this into.Two different resource types so they're.Going to be an explicit inmemory read.Resource and then an explicit um on disk.Read resource now the reason we're doing.This is that even though all sorbon.State is or all life soron state is held.In memory sorbon contracts can still.Access classic State and classic State.Needs to be on disk now because classic.Entries aren't subject to State archival.They have the runaway Ram uh risk and so.We can't store classic entries in memory.And so um sorbon uh contracts will still.Have to pay disk fees for for classic.State that's exist additionally we're.Only storing live state in memory and so.If you access archive state for example.A restore operation then you would still.Have to do dis reads and so there's a.Dis vew for that but essentially what.Would be changed is that you would um uh.There would be a network limit for the.Um maximum number of on disk read.Entries as well as the maximum uh number.Of inmemory read entries now that being.Said because uh the inmemory reads are a.Lot cheaper than the on disk reads we.Can actually pass aot that savings down.To the user so um in this proposal uh.There would actually be no in memory uh.Read bites limit um so essentially the.Read limit for Life s on state would.Just completely go away uh because in.Memory reads are cheap and so there's no.Reason to limit that um now we would.Still limit the total number of entries.Being read but the bytes being read.Would not be limited additionally uh.Because we're not doing dis access uh.There would no longer be a read fee.Associated with accessing Sor on State.And so um because you you still have to.Pay like a instruction um like CPU uh.Count and things like that to actually.Process large amounts of data but.Because we're not going to disk there.Doesn't need to be an explicit fee or.Resource for that um and so essentially.Uh for live SW on state you don't have.To pay for reads and you can read as.Many bytes as you want um you still have.To pay for the CPU though so it's still.An implicit fee but there's no explicit.Read fee and so that's kind of um the.First Advantage to the inmemory versus.On disk resource now the second thing.This allows us to do um is is also.Implement autor restore functionality.And so um previously when we first.Launched sorbon we weren't sure what the.Final State archival proof system was.Going to look like and so while from a.Technological or from a technical.Standpoint there was no reason to.Require um a separate restore operation.And a separate invoke host function.Operation uh we did that just to give us.Flexibility later on in case the proof.System turned out to be very involved um.But in CAP 57 we've actually outlined a.Pretty lightweight proof system that.Works with invoke host function and so.What we're going to do uh in CAP 66 is.We're going to allow automatic restore.And so what this means is that you no.Longer will have to um issue a restore.Operation prior to your invo host.Function but actually your invo host.Function operation will just.Automatically restore any um archive.Keys that are in the footprint and so uh.This you know reduces the transaction.Count required reduces fees and should.Just offer a much better user experience.Now the the way this works in resources.Though is that like I mentioned before.The um live sorbon state is all cached.In memory in one database and archive.State is uncached and on dis in a.Separate database and so if you call.Info Coast function and every entry.You're using um is currently uh live.Then you would have the free inmemory.Resource bites and you wouldn't have to.Pay for dis that being said if you're.Using automatic restore the entries.Being restored would come out of the.Disk read bytes and would be charged uh.Disk fees because again for the the.Entries that are archived and live in.The hot archive database those do have.To be read off a.Disk um and so I think that's kind of a.Um kind of at the high level of what.We're proposing kind of uh you know the.Tldr uh except been talking for for a.Little bit is that um you know C the.Archived entries live in their own.Database and live sbond State lives in.Um the separate dat or a live database.Um we are then going to Cache uh all the.Sbond state in the live database um in.Order to pass that savings on to you.There will be an inmemory rebite limit.And an on disk uh rebite limit in fee.And then um finally there will be.Automatic restore um to um you know.Essentially um remove the need for the.Restore operation in most.Cases so I guess um are there there any.Questions or any uh conversation points.We'd like to touch on.More looks like uh there's a question in.In the uh this in the chat the chat box.But uh I think a lot of daps and uh.Extend TTL by default will that still be.Necessary.Ah yeah so I think um so just because we.Have automatic restore doesn't mean that.You don't want to still manage your TTL.And so like I managed before if all the.Entries um that you're using are.Currently live then um what're or then.You don't have to pay read fees um and.You have much larger uh read.Limits and so um you are still.Incentivized to pay rent um also so but.The issue is when you restore when you.Restore something you have to pay right.Fees for the restoration and you also.Have to pay discre fees for the.Restoration and so I think from a um.From a fees perspective if you're using.An entry a lot it's still in your best.Interest to extend the TTL um to save.Money um as you know even BEC just.Because the restore is automatic do not.Make mean it's free and so you still.Have to pay uh for that restore and even.If it's the same invo Coast function an.Invo Coast function um invoking a.Function that uh only accesses live.State is significantly less expensive.Than invoking a host function that has a.Automatic restore on the front end of.That um so we still definitely want to.Extent.TL.Um yeah uh let's.See uh oh so for orbit lens will it be.Possible to tell in advance whether the.Entry will be automatically restored.During the simulation uh yes and so this.Is um kind of more of the implement.Details which are included in the CAP um.But what we're doing is captive core has.Recently added a couple of um HTTP n.Points for querying Ledger State um that.Will be used um uh by RPC in order to.Simulate transactions correctly and so.Essentially this endpoint is a high.Performance you know multi-thread HTTP.Endpoint um that uh has a similar.Performance to a SQL uh table queries.And so it should be appropriate for for.Production use cases and what this.Endpoint will do is it's a key value.Search where for every key you provide.It it will tell you um if that key.Exists and then if it exists it'll give.You the value um and then it will also.Give you meta information about that key.And so it will give you the Ledger entry.It will tell you if it's live or.Archived um and then it will also tell.You um you know what its current TTL.Value is and if it's in memory or on.Disk and so the captive core endpoint um.Is kind of Ed to be the new kind of.Entry point for this information um and.So uh you should be able to query the.Current archival state and the current.In memory versus on dis state of any.Entry directly via captive core um again.There's also meta that we're emitting.For all these events so if you wanted to.It's theoretically possible to injust.Meta and maintain the state of sorbon um.Entries that way uh but if you don't.Want to do that and create your own SQL.Table and Pipeline and pipeline you can.Just use the the captive core htpn.Points uh so will automatic restore.Become automatically available for.Existing contracts uh yes so the this is.All handled um at the RPC level and so.The essentially what's changing is that.With protocol 23 um and this is detailed.In CAP 66 specifically.Is that we are changing the footprint to.Um have this field where you distinguish.In the footprint if a sorond key is.Either in memory or on disk and so.Essentially uh what the validators will.Do is that for um whenever they receive.And apply an info Coast function they.Will look at the footprint and for every.Sore Bond entry uh that is marked as.Being on disk AKA marked as being Arch.Um before running that transaction um.The validator will essentially restore.Those entries automatically um and so.The actual contract and the contract.Logic will not change which means that.All deployed contracts are automatically.Um compatible with this uh now the.Invocations to those contracts will.Change slightly because of the footprint.Changes but again this will all be.Handled by RPC and so pre-flight will uh.Will do all this automatically.Uh let's see other.Questions oh so ler streaming mode um.I'm I'm not sure about the context um.But.Behind um enabling or disallowing um.Metast streams um on validators versus.Captive core instances I imagine um it.Has to do with performance reasons where.You don't want ingestion to make a.Validator fall out of sync um and.Essentially that config setting is uh an.Opinionated way of saying that validator.Should be high performance um and never.Get blocked whereas like a watcher node.That's not participating in validation.Um would be more appropriate for.Observing and adjusting the meta because.There's not um it doesn't depend on a.Downstream system where if the meta.Stream gets clogged because the.Downstream system isn't adjusting fast.Enough you wouldn't want to uh lose sync.And have a validating node fall off the.Network because of a um a downst stream.Issue.Cool so I guess.Um.I'll from George about the CAP mentioned.Somewhere that autor restore won't.Always be possible can you elaborate on.These scenarios ah yes okay uh thank you.For pointing this out um so there are a.Couple of edge.Cases where um an invocation will still.Require an explicit R operation.Um uh sorry uh and so essentially.Um because the inmemory reads are so.Much cheaper um they don't have uh.Limits um like the on desk read uh the.On just do um while there there is no.Read byit limit at all um and while.There is an entry read limit the.Expectation is that this limit will be.Significantly higher than the dis limits.And so uh just for um you know example.Suppose that a uh in protocol 23 um the.Transaction in memory read limit is 40.Entries and the on disk read limit is 20.Entries and so say you have like this.Dex um you know trade that will access.40 soron entries now if all of those.Entries are Al then um you know the uh.It's within the limits the invocation.Works no problem but say that all 40 of.Those entries are archived now even.Though the inmemory limits are large.Enough for that transaction to succeed.Um you can only the the automatic.Restorations will come from the on disk.Uh limits and so because you have to pay.Um disk fees and are subject to the dis.Limits for the restore operation you can.Only restore um in this example 20.Entries um automatically even though you.Need to um have authority to be live to.Complete this Dex trate operation and so.In this scenario you would need to to uh.Still um manually submit a restore.Operation um just because uh the way.That the limits are set you can't fit.That many restores in a single uh.Transaction um now that being said uh.Especially uh given some other exciting.Work that's happening in 23 um we expect.To raise limits pretty significantly.Across the board and so I suspect that.This Edge case will not affect most.Transactions it will only affect um very.Expensive transactions that are doing.Stuff and so for instance um if you have.A DEX trade and it's trading um assets.That are mostly live you won't be.Affected really you're only going to be.Affected um if you have like a DEX trade.That's crossing a ton of orders and for.Some reason all those orders were.Archived so you mentioned that uh the.Restore op could be deprecated uh.Because of the automatic restore but.This Ed Case requires you to keep.Something something like that around.Right yeah so I think um I I mentioned.The CAP that we met deprecate the.Restore op um and that's just because.That if the footprint is automatically.Restored um then having both the restore.Op and the extend TTL op is kind of.Redundant um because for instance uh say.That you just want to restore something.You don't actually need two um operation.Types you could just essentially use the.Extend TTL put all the keys you want to.Restore in the footprint and then just.Set the TTL extension to zero and this.Is functionally equivalent to the.Restore up and so when I uh mentioned um.Deprecating the restore op I don't mean.Deprecating the ability to restore.Transaction or to restore entries via an.Explicit transaction but just mean like.You know mechanically do we need both.The restore op and the extend TTL op.Where the extend TTL up could now um you.Know in theory at least both do a.Restoration as well as.Extend okay yeah that makes sense Nico.Had a question about how the sorbon.State size is initialized at upgrade.Time it's it's not specified in the cap.Yeah I I think I need to expand on this.A little bit more um so I think um part.Of this CAP is that we are changing uh.The semantic meaning of a network config.Setting um so so in particular The.Bucket List Target size will become the.Sorbon state size now the issue is.Currently the bucket list is like 11 or.12 gigabytes and so we uh all of our.Network settings are assuming um that.The your target size is like 13 gigs uh.But now the issue is um if we you know.Do a protocol upgrade uh protocol.Upgrades previously have never actually.Changed config settings and so if you.Just do the protocol upgrade all of a.Sudden um instead of your Baseline uh.For fees being 12 gigs with a target for.13 gigs because we're only tracking.Sorbon State your target is still 13.Gigs but now your Baseline is like 400.Megabytes uh because there's like a lot.Less soron State compared to life State.And then you have this dos attack where.Until you upgrade the network confix.Settings you essentially have no read or.Write fees um for both uh in memory and.On dis State and so you could have like.A Doss attack where someone writes like.Tons and tons and tons of temp entries.And like you know spams The Ledger for.Essentially zero fees um and so I think.What I'm proposing is that you know.Currently there's like an operational.Lag between upgrades because core.Validators can only cue uh one upgrade.At a time and so we'd have to get all of.Tier one to arm for the protocol 23.Upgrade and then after that goes through.Have them all arm for the network config.Setting upgrade and in between that time.Um you have free free reads and free wrs.Which is a huge security risk and so.What I'm proposing is that because.Protocol 23 is semantically changing uh.What this config setting means the.Protocol upgrade itself should also.Change the value and so you know this is.Slightly different implementation wise.Than what we've done previously but I.Think it should be relatively.Straightforward implementation um.Whereas like the protocol 23 upgrade um.Both semantically changes what the Buist.Target size means as well as it resets.It to a initial starting value that's.More reasonable uh given this new.Interpretation of the data.Okay so we've actually uh updated.Settings on protocol upgrades before.That I think we we know that works cool.Okay great um let's see a couple other.[Music].Questions okay so for orbit lens uh the.Storage for the hot archive uh yes so um.The hot archive and the live Buck list.Are both part of ensus um so we need the.Hash of that state and so for that.Reason um both of the the live database.And the hot archive database are both um.Bucket list DB.Implementations um and that's just.Because they we have to meriz those.Structures then Buck list DB is pretty.Fast these days um now uh with respect.To offering tables to buckless DB um we.We don't really have any plans to do.That um and the reason is it's a very.Difficult structure to add tables to um.So it's a it's a log structured merge.Tree um which is kind of a a variant of.Like database used by like rock CB or.Level DB um and it's also completely.Made inhouse like we didn't Fork levels.We didn't Fork rocks or anything like.That um and so kind of um we we have it.It works very well for um query types.That the valers require and it's very.Efficient at those but we have to.Essentially like hand write C++.Optimized code for those specific.Queries and and so it would be both a.Very significant undertaking to allow.Like you know arbitrary index types um.For Downstream um and it would also.Probably not be a very efficient.Database just because it's a lock.Structured merge treat and so a SQL.Style uh index query would not work very.Well on it and so I think what I'd like.To do with this is um you know we've.We've uh for arbitrary key value lookups.We have exposed um end points that are.On the same scale as SQL queres but.Again they're just raw key value stores.They're not like you know indexes or you.Know really tables um and I think uh.There's been a lot of work done um by.The platform folks on like the um the.CDP um and things like that and so I.Think um given that the complexity of.The database of uh Stellar core is.Increasing a lot um and for a variety of.Reasons we uh only support Buist DB now.And no longer support SQL um I think.That uh any sort of uh raw database.Access needs to move more in the.Direction of utilizing Downstream.Utilizing met ingestion using CDP and.Not rely on direct access to course.Databases just because you know nowadays.With buck DB the core database is very.Specialized and is not suitable for.Generic queries.Cool I.Think there's a couple people typing so.I'll let them finish or if anyone else.Has any other questions if not I have a.Third CAP that I'd like to introduce.Um I'll give it a second and then we can.Move.On all right I feel like that's we've.Had enough time oh uh answer about.Slp1.Um dial would you mind linking that.Question again I'm not quite sure what.The slp1 question is.Oh yeah the new limits sorry.Yeah cool um so I guess uh now I'd like.To uh move on to CAP 65 the reusable.Module cache and so um like I mentioned.Before we were doing all this.Optimization stuff um for a memory State.Um and essentially uh in addition to uh.Saving all the contract data in memory.Uh we can also save all the contract.Code um and by extension all the.Contract modules in memory um because.You know we have a way of EX of.Archiving um contract instances and.Contract code that uh hasn't paid rep.Recently and so um with that I think.Gr's on the call if you want come up and.Talk about CAP 65.I don't think uh gr's on the call I I.Believe we were going to speak about CFE.65 next week right oh sorry I guess I.Got a I gave youall a little teaser for.Next week then my apologies got a.Littlee of the gun but so yeah so.Um I I don't want to steal grain thunder.So I'll just you know leave you with a.Teaser that we can you know have lots of.Um this not only helps optimize the the.Um read limits but also optimize CPU.Utilization as well but we'll talk about.That more later.All right uh unless there are any other.Questions.Uh we can conclude this.Meeting thanks great uh Garen it was a.Great.Talk all right thank you and you know.The dis uh if youall have any more.Questions or concerns you know there's a.Couple of discussion tabs on the Caps or.Just ping me on Discord.
