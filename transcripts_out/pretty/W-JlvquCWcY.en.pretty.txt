Hi everyone. hope everybody is having a fantastic.Friday morning afternoon wherever you.Are.My name is devneal I'm a software.Engineer at SDF on the voyager team.We focus on exploring the Stellar.Ecosystem through data.Analytics and liquidity today i'll be.Talking about data analytics on Stellar.So the goal of our team as well as a lot.Of the work that I do at sdf.Is to do the fundamental meta work that.Powers the rest of the network.How can we ensure that assets have.Robust markets.How can we improve the visibility of the.Network both inside.And outside SDF and how do we use data.To further drive network growth.In this talk i'll be talking about the.First version of our data analytics.System.Targeted and answering those three.Questions as well as many others.So who is this helpful for anyone.Interested in.One building analytics pipelines from.Zero to one.I hope our experience is generally.Helpful to people who have a bunch of.Data.That are just trying to build out some.Analytic system.Two exploring the Stellar network i'll.Present some useful access patterns.Tips for navigating the Stellar data set.And a few illustrative examples.With basic sql knowledge you can ask.Some powerful network-wide questions.Three scaling analytics from one to one.Hundred.If you have a working data pipeline.What's next i'll share some of my.Thoughts on the subject.And i'd love to hear your wants.As a brief roadmap of the talk it'll be.In four parts.First design what we wanted to build.Second implementation how we built it.Design and implementation are the meat.Of this talk.As it is an engineering talk after all.Third analysis.I'll display some interesting queries.Along with some strategic tips on using.Our public data set.In the interest of time and network.Connections over the talk.I'm not going to live code but I will.Share links for folks do their own.Exploring.And finally next steps what's in the.Pipeline no pun intended for data.Analytics.I note that if you have questions.There's a qr code.On the youtube and you can also ask.Questions of the youtube chat.And i'll be answering those questions at.The end to make sure I get through all.This material.So first design.To understand the design goals let's.First talk about our motivations.Why did we even want a cloud scale data.Pipeline.Ultimately we want to quickly and.Efficiently generate the histories of.Assets accounts markets whatever over.The history of the network.The primary use case is internal.Business analytics we wanted to make it.As.Easy as possible for our business.Development and ecosystem teams.To understand the effects of their.Efforts so Stellar uses the widely used.Database as some postgres which has been.An industry standard for.20 25 years so Stellar core writes data.To postgres tables.And then horizon our api to Stellar core.Uses.The core data but it also has its own.Database because it's a web application.So the pros of this are that it's well.Known and well documented.It's great for tran processing a bunch.Of online transactions.When there's a bunch coming into you in.A highly concurrent heavy write.Environment. the cons are also pretty well.Documented.It's slow for historical queries for.Specific column values.Which makes it much worse for analytics.Horizons database structure and indices.Are optimized for the exposed api of.Verizon.That also means that if you're trying to.Do deeper queries across a bunch of.Different accounts or tables.It quickly becomes inefficient so for.Example complex joins or filters.Become really extremely time inefficient.Apps and indexes.A good example of this would be trying.To do analysis of the network but.Excluding.Arbitrage operations because that would.Require a bunch of joints of different.Tables.And then now you have to make filters.Over a really really big table.So to understand behavior at scale it.Became pretty clear to us that we would.Have to build some new infrastructure.It's also important to talk about our.Goals so above all.We wanted value for internal.Stakeholders this is a really important.Node when you're building data products.It's really easy to nerd out and build.Something that's really cool for.Engineers.But the end user is always your.Non-technical users if product.Guarantees.Don't align with business needs then you.Won't ship the right solution for your.End users.And even worse you might just make more.Work for yourself because you have to.Onboard a bunch of people into a.Non-intuitive system.Second open source or open access open.Source is really core to fdf dna.And we want to support open source tools.Wherever possible.Of course some things require spend like.Cloud infrastructure.But we want to optimize for open access.In the organization.Followed by easy public access third.A daily updated data set well some.Businesses need real-time updates.At the point that we were when we.Started building this daily updates were.Sufficient for the course grain metrics.That we wanted to do.Fourth seamless integration with our.Current staff.As mentioned before we rely heavily on.Postgres.As our databases and when you have.Existing tools.You need to make sure that support for.Those tools is well tested in anything.You bolt onto your system.Additionally it had to also be deployed.Easily.It had to be in-house controlled on our.Infrastructure and easy to deploy by our.Site for liability and ops team.So our stack last year migrated from.Puppet to docker and kubernetes.So we needed technology that easily.Integrated with this and for some quick.Buzzword explanation.Puppet is a legacy system administration.Software docker lets you.Contain custom programs in their.Environment and kubernetes is.Orchestration software that lets you.Decide how those containers.Are then deployed and run and fifth and.Finally easy metrics and visualization. this is pretty self-explanatory it.Lowers the bar for non-technical users.And it brings much more value to the.Organization as a whole.So many of the requirements about birth.Fuzzy some are more technical and tools.Oriented.So how do those break down into.Technical decisions.So this is ordered from most to least.Defined.As a deployment infrastructure we would.Use docker and kubernetes.This would be important for.Orchestrating software having regularly.Scheduled cron jobs and the like.It was already our technical stack so we.Needed to play with it.Second the cloud scale warehouse this.Needed a bunch of capabilities.Determined by what I just said.It had to have scripting abilities.Really good postgres integration.And really easy organization-wide.Credentialing so all engineers and.Non-engineers.Could easily access the warehouse and.Make their own queries.Third metrics and visualization this was.The least defined for sure.We decided that it would be secondary to.A data warehouse because ultimately the.Engine.Matters more than whatever you're doing.On top of it but we did know that we.Want this to be open source.Free and easy for non-technical users.Through something like the drag and drop.Interface.Let's now talk about implementation.So the first was infrastructure docker.And kubernetes.While this decision was made for us some.Details of it made everything a lot.Easier.Which is what commonly happens when you.Have to integrate with an existing stack.Docker-based containerization gave us a.Really easy standard for evaluation of.Other tools.Particularly visualization could it be.Easily deployed on our infrastructure.Second kubernetes cron jobs gave us an.Easy scheduling method.For running scripts deployed via docker.We could just set a schedule and run the.Procedure as.Is it was also really easy to integrate.The postgres cluster.It was easy to provision external.Storage in case we needed to have.External memory for the data pipeline.And it also allowed for retries on.Failure.In all this is a really good deployment.Stack and it's pretty clear why it's.Become the modern stack of choice.It's really good for integrating a lot.Of different technology it's really.Resilient.In general it's pretty easy to work with.No real complaints about docker and.Kubernetes.Second for the cloud data warehouse i.Was kind of worried about this at first.Because there are a ton of options.But google bigquery actually ended up.Being a really easy choice for us.For one the queries are super fast they.Were 10 to even a thousand times faster.Than some queries from the verizon.Database.We were honestly just blown away by how.Much better it was.Second intermediate cloud storage where.Do files live.During the data pipeline google cloud.Has a cloud storage service which is.Called google cloud storage.This means that we can separate the.Pipeline into a few different parts.One export tables from postgres to disk.To.Upload from the disk to cloud storage.And three download from cloud storage to.Bigquery.So separating the pipeline like that.Reduced the risk of failure and it also.Made retries less expensive because we.Would just retry one of the scripts.Third there was really painless.Command-line scripting.It's super straightforward to script.Exporting files from postgres then.Uploading to bigquery.It made creating a basic pipeline really.Easy to reason about from the command.Line.And also in turn pretty easy to.Containerize because we were just.Uploading a bash script into the docker.Container. fourth google suite authentication so.Within.Sdf we used g suite so it meant that.Everyone had g suite permissions.It also meant it was really easy to set.Up and share permissions across the.Organization.I would say that this is like a low.Priority in terms of deciding a.Warehouse but it is actually why we.Tried.Bigquery up first because it was really.Easy to integrate within our.And I suspect that organizations that.Run on microsoft like microsoft teams or.Whatever.Might have similar experience with azure.So I do think it probably determines.Live organizational needs.Third for metrics we decided to use.Google sheets.This was actually more straightforward.Than the warehouse because we decided.That we wanted some basic data science.Capabilities that were slightly outside.The.Scope of queries and after we talked to.A bunch of folks around the organization.It made the most sense to surface this.Through spreadsheets it's a really.Common user interface for non-technical.Folks and it keeps data tabular so you.Can pretty easily connect.Here's whatever the table I got out and.Here's how it looks on the actual.Spreadsheet.So python scripts are also the right.Tool for post processing by a lot.It really is a swiss army knife for data.You can really easily read a table.From bigquery run some custom.Post-processing using common data.Science tools like pandas.And then write the output to a google.Sheet finally note that we deployed.These scripts in a server-less fashion.On google cloud scheduler.Once the bigquery dataset was updated by.The kubernetes orchestrated cron job.You could then trigger these scripts and.Then update the spreadsheet it was a.Really cool event driven architecture.Really simple really powerful and.Honestly I think that.This sort of way to automate metrics is.A really good one.Fourth and finally visualization this.Was by and far away the hardest part of.The implementation.Since there's actually a million tools.And documentation that compares them.Directly doesn't really exist.So at this point like we've settled on.Bigquery so we wanted really easy.Bigquery integration.Really easy deployment on docker and.Kubernetes some nice visualizations that.Use drag and drop.And some sql and we also wanted to be.Open source so I tried a lot of tools.That.Fit some or all of those capabilities.Some of the most.Prominent ones that we looked at were.Google data studio apache superset and.Looker.What we generally found was the tools.That played nice with bigquery.Usually either didn't have drag and drop.Or weren't free.And so we ended up settling on metabase.Because it honestly fit all of our.Original pillars quite well.It has both sql as well as a drag and.Drop interface.It's free and really easy to set up and.I think if you need to mvp a pipeline.It's a really good option.Finally let's talk about some of the.Downsides of our v1 implementation.For one daily frequency is slow while.Good enough for initial use cases.A daily update limits building out data.Intensive apps on seller.We want to get closer to real-time.Updates for next version of the system.Two observing failures is hard we were.Still learning our way around kubernetes.Logging.And ideally we want to be able to reach.By really specific granular portions.When they fail.So there are good task management.Systems out there and that seemed like a.Natural evolution of the system.Three visualizations were private we'd.Love to expose our information.Visualizations publicly.But we can't mix the platform that we.Use for internal business analytics with.Everyone else.So we decided that we'd have to think.About an intermediate solution.And four exploring data is painful so.You'll notice I didn't actually talk.About a data science platform above.And while serverless functions provide.Some capabilities for robust and regular.Jobs.Ideally the platform enables exploratory.Data analysis through scripts.Every data scientist now uses jupiter.Notebooks and.We want to make it really easy to do the.Same on Stellar data.So now let's look at some basic analysis.Of some queries on our system.So for one we'll start off with an easy.Query what account has the highest lumen.Balance.I call this easy because it's pretty.Short but it's really cool because it.Illustrates.How powerful even basic sql tools are so.This shows some really basic sql syntax.Select from order by and limit.Those give you the tools to ask basic.Ordinal questions about Stellar history.Select from chooses specific fields from.A table order by orders the results of.The field.And limit says how many you want to see.As you can all see the account with the.Most lumens is the galaxy void account.Which received the lumen burn last.Merity.Now a medium difficulty ferry how many.Payments of an asset are made daily.So we show you some new syntax here date.Converts a time to justice day.Sum is an example aggregation function.Which takes a bunch of.Results and then combines them into one.Specific number.And where can be used for various.Condition where clauses.Shown here finally group by will group.By.A specific field so this lets us pretty.Organize.Pretty quickly organize and group.Results by day.Compute daily amounts and do some pretty.Good aggregations.All in all it's actually really simple.To be able to make these.Sort of day-by-day analysis and it's.Really core part of some of.The organizational metrics we track.So finally a high difficulty query how.Many trades of a trading pair are made.Per day.You'll notice that this has a lot of.Lines but the actual primitives are.Pretty similar to things you just saw.It has some new syntax so we use with as.To create some temporary in-memory.Tables to query and join lets us join.Together different tables on common.Fields.So you can have some really big tables.That you can now condition.Filter group so on so forth.Once again I want to say that like the.Queries are not the focus of this.Particular talk the engineering is much.More of one.But feel free to reach out to me either.On the youtube chat or after.I'm happy to provide more instructions.On how anyone.Can make these queries we also put out a.Blog post earlier this summer.That covers a lot of this and I highly.Recommend checking it out.So forth and finally let's talk about.Some next steps.To motivate these think about the.Downsides from implementation and we'll.Talk about some of the things we could.Do.So for one going from slow to real-time.Frequency.So new horizon capabilities like the new.Ingestion engine.Help us get closer to real time because.That enables much faster extraction of.Data from Stellar core.And history archives our awesome intern.Isaiah turner.Has been building a command line tool.That uses these capabilities.To read in data in close to real time.And then.Output it in the expected schema and the.Link to it is right here in the slice.Second going from low to high.Observability.Fixing failures is pretty hard but task.Management makes observing and debugging.A lot easier.So while our current system wasn't.Unmanageable the number of moving parts.Made reading kubernetes logs the primary.Solution.It's important to note that we added.Some sentry logging around business.Metric.But the thing about all of that is that.It shows you that a problem happened.It doesn't really tell you where it did.Or what to do to fix it.So combined with the above tooling.Isaiah has been working on an airflow.Task management system for the pipeline.Airflow is a task management system that.Airbnb open sourced a few years ago.And honestly it's been great in our.Experience at being able to orchestrate.A bunch of different smaller scripts.Being able to say hey this failed and.Giving a really nice ui.For engineers to go and just retry.Different parts so this has the same.Overall flow as before.It reads ledgers from Stellar core it.Writes structured documents to google.Cloud.And then it uses those as changes to the.Bigquery tables.So this one is almost done it should be.Done next week and the link to that.Which is it's already open.Source is on the slides as well.Third going from private to soon public.Visualization.We're thinking about displaying markets.And corridors in a public-facing site.Like `stellar.org`.It'll make it easier for prospective.Anchors to see what the volume looks.Like in and out of specific businesses.And countries.We're just reasoning about the right.Strategy in a way that meets both.Internal and external needs.So once the new data system is stable.We're going to see how you can leverage.Bigquery data warehousing.To build a scalable web application on.Top of it it's its own engineering.Challenge.Because it's pretty hard to do that in a.Time efficient way.But it's a really exciting one and I'm.Excited to get cracking on it.Fourth and finally data exploration is.Painful so a data science platform is.Very much tbd.If anybody in the community wants to.Take this challenge on.I'm more than happy to see some.Community implementation.And feel free to reach out to me with.Either ideas or.A desire to try to figure out how to do.It I'm very happy to talk about it.So that's all for me I hope you really.Enjoyed hearing about this pipeline and.All the stuff we've been doing.And I'm happy to answer any questions.So one question that's been asked are.There any cool projects using this data.Set that you wish existed.So i'll start by plugging again the two.Things that I just said.Because I think they enable a lot of the.Cool projects so for one I think it.Should be really easy to see.Here's what all the major anchors on.Stellar are doing and.It should be really easy to see what the.Volume in and out of specific quarters.Looks like.And it should be even easier to see what.The rates look like.I think the rates are really hard to be.Able to do on this data set but really.Powerful.So for example one of the reasons that. the euro t to naira corridor has been.Killing it lately.Has been because the rates for that are.So much better than what you would get.When you have really good rates on.Stellar it makes the whole network work.And I think that applications that.Surface information like that like the.Key killer applications of Stellar.Are the most useful to leverage this.Data set in the short term.Longer term things that I think would be.Cool that leveraged this data set. I think it could be really cool to.See how assets on the.Decks itself can function as a hedge.Against inflation.Through sort of price histories over.Time this is one of the things that.People talk about as a goal for crypto.And one of the things that with vibrant.Sdf has started working on.And I think that if there were projects.That demonstrated that longitudinal.History it would be another really cool.Value.For Stellar in the network.So another question what can a community.Do to help expand Stellar.And improve it for those who do not.Understand the engineering aspect of.So that's a good and interesting.Question.I will say that you don't need to.What I just talked about to use the data.Set.That's one thing that we really tried to.Make sure we could do which is just.Being able to use some basic sql queries..And show some really powerful data what.Can the community do to expand.On it lots of things but within the.Scope of this talk.I would say that it looks like telling.People that hey Stellar is really cool.But then also showing them what the.Volume on Stellar looks like.So this is one thing that i've thought a.Lot over the course of the summer.When we've had a lot of volume on.Ethereum with d5 right.How do you show similar volume with.Stellar because Stellar is one of the.Few layer ones that can actually do.Layer two things like that.So I think demonstrating that value like.That exists.And showing people that seller isn't.Just a payments platform with xlm.There's all this other stuff you can do.On top of it I think that's one of the.Things that becomes a lot easier.With really publicly queriable network.History it would be really cool for.Community members.To promote it in a data-driven fashion.Awesome so it looks like no more.Questions so.Last call if anyone has anything else.But.Feel free to reach out to me either over.Key base my key base username is devnet.Nil or over twitter my twitter username.Is debna sir.D-e-b-n-i-l-s-u-r I'm happy to.Talk about things Stellar related as.Well as waste lovers data set in the.Community.
