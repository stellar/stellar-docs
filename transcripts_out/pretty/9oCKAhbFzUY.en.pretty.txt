Protocol 23 we are introducing the.Hot archive bucket list where we will.Actually.Store archived entries in a separate.Database and what this allows us to do.Is to store all live sorb on state in.Memory as well as store andan.Module caches in memory and we can do.This safely because the rent system is.St archival so what we were talking.About today is an update that we made.Since the last time we talked.Particularly about.The way that we calculate the what.Today is called the target bucket list.Size and so in protocol 22 whenever you.Pay rent or write an entry to The Ledger. this fee is variable based on the.Current size of the bucket list which is.Just the size of the you know.Network's database on disk and.Essentially we have a Target size and.If this Target size is exceeded then.The cost of right and R bums increase.Very rapidly and essentially this allows.Us to have a soft CAP on the current.Size of the bucket list or the the .Network database from a fee.Perspective so you can technically still.Write but'll just be very expensive to.Do so so people will stop writing and.Then eviction and styal will do a.Back pressure to overtime reduce the.Size of the bucket list now the thing is. previous. to protocol 23 this was all measured.In database size now there were two.Issues with this the first one being.That this is a sorond based fee and most.Of the sorond state or most of the .Core database wasn't soron state classic.State significantly dominated the size.And so we had this weird system where.Even though Soroban took up very little.Space in the bucket list it was being.Charged fees whenever classic entries.Would make changes so that's kind of.Issue number one that you have.Classic influencing the cost of sore.Bond State now the second issue is that.With protocol 23 we now have a bunch of.Storage or all the sore Bond state that.We actually store in memory instead of.On disk and so it would our kind of goal.With this setting is to make sure.That the protocol can CAP the maximum.Amount of memory that Val to use it.At a given time and if we were to.Continue to use just the disk based.Metric this is no longer able.To actually CAP the amount of memory.That you need to store now that we cach.Everything in memory and so the update.To CAP 66 is that we change the.Bucketless target size byes to soron LIF.State Target size byes and so what this.Means is that instead of using the.Entire size of the database to calculate.Rent fees and write fees we only use the.Size of the live Soroban state that we.Actually have to store in memory .Now there's a couple of details here .Particularly I want to talk about.Contract code and so when storing So.The plan is to store all contract data.And all TTL entries in memory and then.To store instantiated contract code in.Memory now contract data and contract.TTL the thing that we store is the.Same size as the entry on disk so for.Instance like if you have a contract.Data on diss that's The Ledger entry.Is you know 64 bytes then we store 64.Bytes in memory so there's a one to one.Ratio for data and TTL for contract code.This is not the case because of the.Module instantiation cach changes or the.Instantiated module cache we are not.Actually storing just the contract code.Bytes in memory but we're storing an.Instantiated model and this model can be.Up to 40 times the size of the on disk.Wasm bites in the worst case and so what.We're doing instead is that for.Contract code in particular instead of.Using the size of the contract code.Entry for fees which is what we'.Been doing up to this point you're going.To instead use the size of the memory .Given by the instantiated contract.Code module and so in the worst case.This is a 40 times increase but in the.Average case this is only a 10 to 15 15. times increase over the size of the.Contract code as it's calculated today.And so that's change number one is that.Essentially because we need to account.For the in memory size of state now.Instead of the on disk size we need to.Make this adjustment in the contract.Code size calculation now the other.Adjustment we're making is with r fees.So for rent fees essentially you are.Renting under you know protocol 23.You are renting space in the in.Memory cach of the network that's.Essentially what rent is doing now and.So we the rent fee will remain.Unchange how we still have like this.Target live State size and then the.Rent fee will you know increase very.Rapidly if the of Life State increases.This for right.Fees doesn't really make sense to have.The right fee .A function of the current amount of Life.State because.We're to using memory based bounds.Instead of dis based bounds in the Life.State we a dynamic wrry doesn't make.Sense and so what we're proposing is.That we still need to have a right fee. but you are not it doesn't need to.Be based on the current size of the.Bucket list rather it just needs to be.Based on the computational cost of doing.A right while you're applying the.Transaction and so we're making a.Change that right fees are now a flat.Fee per BTE or per kilobyte rather.And so instead of having like this curve. the rent fees will still have that.Curve but the right fees there'll be a.Network config setting which is fee.Per 1 kilobyte R and then this will.Be applied based on the size of the.Entry you're writing and this will be.Applied to both Soroban entries you write.As well as classic entries you write .So that's that's the primary changes.Essentially we are changing the way we.Calculate contract wasm size respect.To fees rent fee is still variable based.On the amount of Life state right fees.Are now.Flapped any questions or or concerns.Morgan's question about the the cheap.Storage we don't it's not so much.That the storage is cheap and then non-e.It's that before you hit the target.The storage is reasonable reasonably.Priced and then past the target the.Storage is like very very ridiculously.Priced so the idea is that you know.We still even before you hit the target.We still charge fees and the intention.Of this fees is to dissuade people.Who aren't using like who aren't.Legitimate apps using the space but.Essentially the reason that we have this.Target is that we don't want a Dos angle.Where an attacker could just write you.Know gigabytes and gigabytes of state.That we all out store memory and then.Oom kill them nodes and so I don't think.The distinction is cheap versus non-.Cheap it's you know reasonable versus.Like ridiculous pricing that we only you.Know resort to in kind of the to protect.Ourselves from malicious attack. see any numbers for the threshold .I think exact numbers are TBD for.Reference we're are so currently .The we use bucket list total size and so.I think it's like 12 and a half gigs or.Something like that the target something.In that range but because we're.Switching to only meter the life.Sorbon State it'll probably be something.On the scale of hundreds of megabytes.Just because today the total sore on.State size is like 40 or 50 megabytes .So that still gives us you know lots of.Growing room but is a very small you.Know value requirement as far as memory.But again you know we we still need to.Think about these numbers a bit more as.It's kind of you know on the range of.What we're looking.At yeah to add to that since vasm kind.Of takes more space now than it used to.Take in the database yes we need to.Re-evaluate this in memory State size.Including the module cache and I.Don't think they have done this yet so.But the it is I don't think it will be. significantly bigger.Than tens or hundreds of megabytes and. threshold to be set I think quite.High compared to that probably it can.Easily be a few times higher that you.Know we didn't get that much dat in a.Year I don't think like if you said.It to 2x 3x of the current state I.Don't think we'll run out of space.Anytime.Soon yeah and to just to make it.Clear for for Morgan's question like.We like the purpose of these limits is.Not to you know limit good users so.You know if we see a Dap that has 100K.Daily activ users I mean that's not.Going to happen overnight there's going.To be a ramp up but we would definitely.Probably you know introduce the slip.SL SLP to raise those limit so the.Intention is not to you know reduce.Actual good good usage of network.Yeah and I guess Nico pointed out a.Good point that I forgot to mention is.That because we are switching from on.Disk metering of contract code to in.Memory metering the size of the.Current instantiation model is protocol.Dependent and so for instance if in.The future we change to a like a g .Instead of interpreter then the size of.The instantiated model will increase .And so I didn't mention this but well .For fees we are for the rent fee .In particular we are using the .Inmemory size of the contract code .But for limits and for the right fee we.Using the on disk size and we have to.Do this because for instance like we.Have like a a maximum contract size .Config setting and if you can assume.That you know a contract today has.The maximum size we wouldn't want want.To break that contract in a future.Protocol upgrade if the inmemory size.Just happen to change and so yeah I.Think that's that's a good point I.Forgot to to mention is that only the.Rent fee is dependent on the contract.In memory size and that means that.Like a and that's and the target sorond.Life State size is also calculated using.The inmemory size of contract code .But as far as write fees and .Transaction WR limits and contract code.Size limits those are still determined.By the wasum and we have to do that.Because only the actual on disk you know.Wasm size is consistent protocol to.Protocol actually so my comment was.Actually a little bit different like .Because I had a chat with grer about.This and it sounds like the current P.Request we have.For CAP 65 is actually caching per host.Basically so like if you have like.Two host like core can be configured.Right with multiple host to support.Multiple protocol versions and right now. we keep in.Memory the version of the WM.Basically like the the the the.Pre-processed version per host so it.It gets basically like if you have like.Two H in memory is going to have two.Versions of the wesm cache per.Contract and that's kind of a to make.It easier.To to to reason about but at the same.Time that means that the overhead in.Memory is actually dependent on how core.Is is.Compiled yeah I think this is I agree.There something to look out for but I.Think this is primarily an.Implementation detail at this point.Because I think in the.Yeah no it's totally is it's just like.That's why I said there needs to be an.SLP to to kind of discuss that because.The your memory limit.Basically that you you pick is has to.Take into account the fact that you have.Basically a overhead of two or.3x for you know for the resident.Wiom that maybe is not as trivial as.You know if you look at calibration.While like if you're asking one host how.Much memory you know are you using right.Now that's actually not the truth you.You need to to basically lie.Through the through the network settings.That you know you actually are are going.To count was them in memory let's say.With a 3X you know a multiplier on top.Of what's actually using per per.Host does that make sense it's basically.The yeah the calibration .Settings yeah yeah I think I think.Longterm like a like so longterm when we.Actually have like large amounts of.States say like in the gigabytes then I.Think we can probably avoid this issue.By doing something clever like you know.Like if we are armed for an upgrade then.We can like you know like do the.Compiling for the new the new protocol.Version in the background laely and then.Serialize it such that we don't have.Like the double memory overhead but but.I agree as like as far as like.Short-term and medium-term plan goes I.Mean I think like the state size is.Still small enough that like we can set.Limits like with these assumptions it'.Be.Fine but yeah I agree that we should we.Should have these these this like.3x factor in.Mind cool I guess if we don't have any.Other questions or comments about this.We can I'll hand it off to the next.Cap right thanks Garen for presenting.This and yes the next thing we have.On our agenda today is CAP 6 to7 and.Specifically the issues that concern. handling of transaction memers and.Maxed accounts in the event not.Sure if CAP is even an appropriate link.To share right now because most of.The revant stuff is captured in.The.Discussions to the CAP and I think.The the point of contention during the.Previous meeting was how exactly can we.Enable maxed accounts in soran and after.Spiking a few.Options considering the fact that it is.Likely for pretty much any custom token.To be interested in being listed in.Centralized exchanges and as being.Compatible with custo Vols that .Centralized exchange may use for.Multiplexing. it seems like this is something that.Most of the tokens will want will want.And I think this is one of the key.Requirements that kind of made the.Whole discussion pretty hard to.Converge on but since kind of came.To an agreement that this is probably a.Reasonable feature to have and .Contracts will need to deal with Max.Addresses.Anyways the current preference is to.Go with an option of add in simulus.Support of Max addresses to the SDK and.To the environment and what tokens.Extend well not extend but update the .Transfer function interface to be able.To accept Mark addresses I don't know.You do want to add something we di into.Details you asked to.Speak yeah I think maybe it's just.Worth calling out for people who.Listening of the conversation last week.I think an assumption we were making to.Be really clear it was an assumption .That this needed to be an.Extension and I think you know to has.Sort of pointed out that it's safe to.Assume that pretty much every token.Would be interested in being listed in.An exchange you know if if that was if.That was to happen and so there's.Little reason to make this an extension.Where it's sort of targeting a very.Small number of tokens like really.Just sort of all tokens should should be.Able to handle the case where they're.Given a Max address or a.Memo and so much of the complexity I.Think in some of the spikes sort of.Disappears once we don't try to make.This an.Extension right so to summarize like.What we are currently going to do with a.Protocol is that we will add a new.Object type to soran specifically for.Handling maxed addresses. max accounts currently Max accounts.Currently but I'm actually not sure if.You want to do contracts or not but we.Could we wanted to and the interesting.Thing about it is that since it's in the.Protocol well the primary reason for.Why we are doing this work is for the.Tokens to be able to support this .Basically any.Protocol will be able to use them if.They want to so if custodial.Solutions are necessary some other.Protocols basically the solutions for.Single address that can have multiple.Sub virtual sub accounts they will be.Able to implement this and I think this.Is one of the benefits of this it hasn't.Been discussed before even though.Like we don't have limited obvious use.Case but know folks may come up with.Something I definitely recall having .Recall seen some discussions here on.Discord regarding virtual accounts.For multiplexing support.So yeah we we had this new new object.Type and also at the SDK level we make.It so the regular address type and.This new Multiplex address type are.Wrapped in the same SDK type which means.That for example if contract .Just operates on the addresses and it.Doesn't know exit your token started.Except in multiplexed addresses for the.Transfer as will break so for.Example if contract a calls a token.Contract and passes a regular address.Non Max address to it things will.Keep working even the token contract.Updates to this new proposed feature. which is to allow transfers to.Have multiplexed destinations and.Sources and this is what what has.Been spiked and this seems to kind of.Work and in terms of complexity on the.Token side it doesn't seem like there is.Too much it's few additional lines of.Code to convert from multiple addresses.Back to normal addresses that can do.All the interesting things that.Addresses can.Do that's kind of a hell level thing. I am not sure if they need to go.Too deep into details right now I guess.The conclusion from the discussions for.Is really that seems like we have a.Way of making this work in a non.Disruption non-disruptive fashion.Meaning that the existing contracts.Will not be.Broken and it is also possible for the.Clients to discover if a contract.Reports multiplexed addresses at all or.Not which may be relevant in some.Context right so I think this part is.More or less clear part is still.Unclear and I wanted to talk about a bit.More is how exactly are we going to.Represent the multiplex destinations and.The events and the option of just.Putting them into topic is problematic.Because it would break index if they.Don't do anything special this Multiplex.Addresses break in a sense that they.Will have too many virtual destinations.That they really shouldn't be caring.About so in the link I posted. yeah thanks Le for posting other.Discussions yeah in the discussion.The da post ly has listed different.Approaches .To actually how to handle the events.With mlex.Destinations and. I think we haven't reached the full.Agreement on this but again it seems.Like from my own preference and I see.Alex has commented on.This. seems like what we could do is you.Could just.Converge yeah I say want to talk but.I guess the current preference is to.Converge all the possible memo that we.Currently have support in the.Transaction converge everything into the.Single Multiplex address data.Structure so that for the classic.Transactions you will be able to.Generate Multiplex destination based on.The transaction memo even of this.Transaction mem is non ID and for s b.Use cases you only support ID MERS to.Kind of ruce the potential amount of.Confusion I know Alex if you want.To okay Alex doesn't want to talk I.Don't know if you want.To yeah no I just you know my comment.Was basically my preference just.As one point of view on like which one.We should use I guess I do have a.Question now thinking about it like.Would this cause any breakage to how.People parse M addresses if we were to.Expand M addresses to actually cover all.Types of.Memos well it depends on where exactly.We put it in the sketch I've posted.Above the above in the same discussion. I'm actually only extending SC.Address.Type which means that classic Max.Account data structure will stay as is.And the reason to keep it as is is that.It's kind of over the place in the.Protocol like a lot of transactions have.Maxed accounts as sources or.Destinations and yes there is really.No good reason to kind of .Retroactively Plum all this memos .Into the classic Max account type and.Yeah this m will just remain in address.And it will be just yet another special.SC address as the remainder of with.Cap 67 we introduced like chable balance.Address and liquidity po C address and.Both can only appear in the context of.This unified events sources of or.Destinations of payments and this is.Basically the Third special address kind.That only appears in unified events.Coming from classic I cannot say this.Will definitely not cause any breakage.Because well we are kind of in order to.Achieve this we need to not well kind.Of break right the token events by.Converting the data field from a single.Integer to a map there is definitely.This breakage and you know if someone.There something to address they may.Or may not be broken so PR definitely.Some cost to it but it's not really.Specific to this proposal of adding.This .Maxed address with memo whatever new.Type because we kind of already have.Have this issue. so yeah I hope the breaker scope.Is really minimal especially if like we.Support this iny libraries and whoever.Is using string key library to just.Convert to and from a c address they.Will just need to update the version and.Hopefully everything just works. yeah George you turn stage yeah yeah.I just wanted to add that yeah I 100%.Agree that we should minimize or we.Should try not to make any changes to.The actual mxed account xdr address or.Like the M string key format because.There's deep assumptions in a lot of.Places Downstream both in platform.Products and Beyond in the ecosystem.About the M string key specifically.Having the integer as the ID so yeah.If we can isolate that to SC address.That sounds that sounds.Great right that is option four right or.Option yeah four right.Yeah or it's it's more like it's not.Option three I think option three is the.Only one that modifies the string.Key yeah I think option two is the.Closest yeah like using M wherever.Possible but not trying to shoehorn text.And hash into an M address as long as.We're talking about the stry.Representation not some other.Representation.Yes yeah I think that's right yeah I.Understand that concern I I.Personally when I look at these four.Options and I think there's more than.These four options so I think you know.If people have other ideas like please.Post them to this thread these are.Just the first four that came to Wine .The attractive thing about the first.Option and the third option is.That it's very consistent with what you.See going into a.Transaction for I think for.Exchangers at least or for legacy users.So if somebody puts in a memo that's a.String it comes out as being labeled.This is the here's the string in in.Exact like it looks exactly the same so.If somebody's using developer tooling.They see they decode a transaction they.See this transaction had a particular.Memo they decode the event they can see.The memo right there like they look.Exactly the same and then the same.Thing if you know if the mammo is an.Integer it looks exactly the same and.Because we need to decode the M into a g.For the topics I don't think it's such a.Stretch for option one to to like break.It apart and for even in the MX case the.Integer to come out at least there's.Some consistency there the concern I.Have with the two case is that.Because it's sort of a little bit.Less consistent.You can create a transaction with a Max.Address and you get an m on the other.Side which makes a lot of sense but.Then you create a transaction with a g.In a memo and some memos result in an m.And some memos don't result in an.M so yeah this might just.Be maybe this is an edge case we.Shouldn't worry too much about but just.That inconsistency seems surprising to.Me I think it will be surprising to.Somebody. just to clarify real quick isn't it.Normally the case that when people use.The ID for .Differentiation on like Omni bus or.Custodial accounts that they would use.The ID form like text and hash don't.Seem like they would apply necessarily.There so it might be like an edge case.That you know they're attaching it as.A piece of text that is supposed to like.Display something so putting that into a.Memo would be it's not intended to.Differentiate yeah we should I mean we.Should use we should use the big query.Data set just to validate just to.Make sure we got these numbers exactly.Right but when I've looked at this in.The past I have seen plenty of text.Usage actually but a lot of the text.Usages exchanges placing numbers into.Text form and then using the text the.Memo text.I see so kind of a misuse of the text.Form yeah I think Jake has just.Commented on that I think just our.Comments about what most exchanges are.Doing we just need to validate that.Because I know like one exchange that.Actually does use MOX counts well.They support both moxed and non-med for.The non-med they use text and then for.The moxed they're obviously using the ID. which is interesting.I think one of the advantages of option.Three which.Modifies the M string key format so that.It can contain more.Information. is that it sort of.Pushes everything towards that MOX.Address format I don't know if that.Would really change adoption of it.Probably.Not but it does create like a single.Unified View and the data that comes out.The other end I'd be interested in Simon.If he's here on like you know what the.Perspective is from like a data consumer.Perspective like a future data consumer.Perspective assuming that the network.Will always have more users tomorrow.Than it does today yeah like which of.These approaches makes the most sense.For future users and future.Consumers hello I would actually say.For I guess from like a big data or like.Olab perspective I don't think any of.These would cause a problem for.Scalability .Yeah I'm not sure if that's helpful or.Not but I think the amount of users.That unless it gets into I guess like.The billions of something range which. I guess is the like possible won't be.An issue for like the next decade or so. I don't think this would be a problem.From like an olap perspective.What about from the perspective of your.A data engineer trying to represent this.Data and so you're looking at a lot of.The data and a lot of the data has just.A single value for this destination and.Then in some cases oh this value is.Actually a combination of multiple.Fields or or something like that does.That make it more or less difficult.When integrating into other systems or.Would that be like a concern or.Something that could be like a foot.Gun like something that'd be easy to.Make a mistake.With I think for our use case I don't.Really have a concern with that I think.The concern would be more for whoever is.Ingesting from like RPC or creating like.Some Horizon like end.Point so I don't know if I'm the best to.Speak on that.Yeah from from Horizon so from Horizon's.Perspective because we already support M.Addresses in their current.Form it would be like the format.Itself would break right because you.Would need a way to distinguish what.Kind of memo it is now and so anybody.Who's relying on the current string key.Format would know longer be open be able.To use the same string key format right.Any existing you know queries or .Lookups or parsing routines anyone.Who's trying to find stuff in Hubble.Right they would have to reformulate.Their M address as far as I understand.It I mean it depends on how it gets.Formatted in the xdr I guess or how the.Shinky definition defines it but yeah.It seems like this would cause a lot.Of pain for Downstream if we extended it.To be more than just strictly integer.IDs I think it can when I had to look at.I I did a quick Spike having a look at.What would need a change about the M.String key and it's 100% extendable.Without breaking existing string keys so.Without confusion so like existing.String Keys.Would continue to decode to the exact.Same value and for any existing.Decoder that follows the back any new.String key wouldn't overlap with any.Existing string key so that every new.String key that didn't follow a.Different format would definitely fail. like say like when I say new I mean.Like the text the return and the hash.Typ of MOT types so I think we.Definitely could do that but without.Breaking existing systems.But you know whether that's the right.Way still to do that is another question.You know based.On. there will be systems that probably.Won't upgrade to the new format and they.Might assume that there's nothing to.Change and then they become broken yeah.I don't know I'm interested to hear like.Nico you were just talking about.Exchangers will crack open the M address.For case three so that's strictly worse. as a use case could you unpack.That a little bit yeah I mean it's.Basically like you know in in the in.Their inje system what they are looking.At is deposits to their hot.Wallet and then they and then the memo.That's how they you know ingest the.Data like I don't.See like why they would want to.Basically track like M addresses as the.Kind of the the deposit key.For for you know separate from the hot.Wet.Basically yeah that's a good point.Also for for exchanges that use Define.Those identifiers like per customer too. like you're you are really interested.In that identifier whether it be an ID.Or a string I I.Think I think what I hear you saying is.Exchangers actually don't really care.About the M address after that import.Other than using it as an input so that.Users can just enter that one value when.They're ingesting syst doesn't really.Am I guess you can still argue that .As a someone who's we're basically.Deciding what is going to be P published.As an event but you can always go.Back and forth between the M or the.Address plus memo right and you can.Abtract that away in the SDK or even.Like as an indexer was actually.Ingesting this data and converting and.Serving at however you want right so.Does it really matter I think the.Point about breakage though is very.Important right CU I feel like you first.Said it wouldn't cause any breakage but.For anyone who is parsing currently M.Current M addresses and they don't.Upgrade their implementation to the new.Thing if they see an address that .Is using the extended version they would.Probably break.Right yeah in that in that case they're.Not going to be able to decode it.Because it's it'll be a different.Format yeah I think it's really some.Confusion because I think we might be.Reading this proposals.Differently I do not think any proposal.Assumes didn't shink keiss anywhere in.Events yeah that's actually exactly what.I was about to ask like what when we say.From M and to M right in the in the.Transfer topic from G and 2G is an.Address yes basically basically in my.Mind from G2 g means that from SC.Address account.Ed25 whatever to C address account and.From M to m means address maxed account.Which I have introduced in my Spike rate.And address Max account variant has the.Max account XD payload U which is.Existing Max account which it doesn't.Even have to to be frankly it could be.Just you know Ed plus ID there is no.Additional level of neration. and the same goes for this new .Memo account from z x j point it can.Be just a variant of a c address that.Contains G well.Ed25 key and .Memo which is just the transaction.Memo tapee for example so this key.Discussion is rather like how how do we.Like like if we want to convert to.String key or not but you know the event.Structure does not change I think I.Think well I think there is an impact.Here so yes string keys are not part of.The protocol and they're not part of the.Xdr but they are part of the developer.Experience and how we structure this.Will impact it to a degree so you.Know we you I agree that you know we can.Bundle these two pieces of information.The address the actual address and the.The memo separately or together in.Different structures the same structures. but we do need to make a decision in.The event if it's a top like if it's if.If this event the data section is going.To become a map is one of these.Fields going to be something that we.Expect to be a single unit or we going.To expect them to be separate units and.So and that affects how it's going to.Render in things like the developer.Tooling in the RPC Json API because.That's yeah like that's where we.Actually do map things to string keys.And they either map or they don't map.Well if if we make them like separate.Fields in the data nothing that we built.Today for developer tooling is going to.Be able to map those things to a string.Key we want them to be a string key if.We put them in a single field then we.Have that option but that may or may.Not be a good idea and we're also.Talking about redundant data too so does.It you know if the G is in the topic.Does it really make sense to spend.Another 32 bytes and actually keep that.G in the data as well that's unclear to.Me right yeah yeah I just want to point.Out that like basically for the.Exchanges I don't think any option is.Specifically like bad or inefficient.Because like still have a.Somewhere in the event so they you.Never need to like par this out of.Strink key as for the overhead yeah I.Think it's a.Valid concern. yeah I think I think like number one.I think niik sort of pointed out that.That was an option that was is like low.Risk of doing the wrong thing with .Like option one is so simple you know.That everything's pulled apart you've.Got everything in the event the really.Only downside of option one where.Everything's broken apart is the.Developer experience isn't quite the.Same so when you're looking at a human.Readable version of an event you're.Going to see this memo and a g address.You're not going to see that M that was.In the original but this already.Happens everywhere you know when you go.To stella.expert or other explorers.And you drop in an M address it.Immediately drops you into a page about.The G address so I don't actually.Think we need to have complete.Consistency with madress goes in madress.Comes out. yeah yeah I guess I just generally.Say I'm generally for option one I think.It's simple the messages.Smaller. yeah.I yeah I think this may be fine and also.Like for the consistency part like at.Least like if you can have M addresses.In the contract invocations things.Are immediately much less confusion.Because you know if you wanted to check.Your.Transaction like there is a very good.Chance that block Explorer will.Correctly say that you actually have.Performed the transfer to an M address.And you know not some pair which was my.Concern for for the input but for the.Events I agree it may be not as.Important and events are generally much.Less human reable so maybe yeah it's not.Worth to optimize for exactly the same.Range.As rendering as we would have expected.From the signature of the function..So.Fine okay so it sounds like we're.Leaning towards option one.No one's opposed to.That if so we can we can move on to .The next.Topic yeah I'm good all right the.Next thing I think we should discuss.Is what to do about the TX memo if we.Should admit that as a separate event .Because you know for example if you're.Replaying if if an exchange is replaying.Using Classic Events and someone sent a.You know a payment to.Coinbase with TX memo and the M and a.M account we don't have space in the in.The current events as there as they.Defined to include both like both the.Txl and the MX information so an option.That Dio mentioned is we could just emit.A system event similar to how we CAP 67.Emits a fee event and that event will.Just contain the the the TX memo if.One existed.That I mean I you're saying sorry sorry.Like so you're saying like you would not.Evit emit a m or whatever like know this. destination.Memo event if you have a transaction.Memo no so when I'm saying is like on.A payment if you use a MOX account you.Will have a des station memo in the.Transfer event right.But if that transaction also had a TX.Memo set what do you do yeah but but.First of all like I'd like to understand.What happens if you only have a.Transaction memo and not a Max.Destination yeah so the CAP currently.That says that you there's like an order.Of Precedence so you would pull the TX.Memo and put it into the event but if.You emitted the TX.Memo separately it would probably.Make sense to ignore ignore that order.Of Precedence and have the consumer pull.The information that they want right no.That's mistake yeah because we saw with.Horizon right like the this model where.You let clients kind of.Decide we I think this is the.Opportunity to.Actually fully specify this precedence.Order so that you don't have.Ambiguity right because I'm pretty.Sure I mean I don't know I don't want to.Put you know exchanges in in you know in.Trouble but I imagine that the. This president's order is not.Actually respected because it's actually.Fairly complicated no what well let's.Let's I step back for a second is is the.Ability to re for an exchange to replay.From Genesis using events a requirement.Because if it is then you you can't make.Assumptions about if they care like you.Know certain exchanges May care about.The TX Momo and some may care about the.M information.Right I think this is a new event stream.So we get to do what makes sense if I.I think yes you don't want to lose the.Information that there was a TX memo.Yeah at the same time I do think that.The transfer events should contain the. you know what we think is the I.Mean as as per specific as per the.Specification of transfer.Events what we believe is the the.Right you know destination memo in.This case.Yeah and I think on that point it.Doesn't I'm not actually sure if reap.Like we can't we change this this is a.New event Stream So the replay can use.The new.Semantics while the previous versions of.Meta and the transactions still describe.The old.Semantics yeah the the events can you.Know do whatever we can be specified.However we want them to be maybe I.Wasn't clear like like what I was.Initially saying is that the the TX memo.Would be in this new event and the.Transfer event would only have any it.Only have MOX account information and.And I think ni you're saying that in.Some some cases we should push the TX.Memo into the into the transfer event as.Well I think for yeah like if we.Are going forward like it's more like.Yeah what do we want the those events to.Look like for Classic Events I think we.Want them to properly represent the.Destination and therefore you need to.See the transfer event with the proper.Destination plus memo like you know like.Option one let's say.Right even if the memo is actually at.The trans has been specified at the.Transac only at the transaction.Layer okay so if the if the account.Wasn't moxed then for the the memo.Into the transfer event and.The M basically.Indistinguishable if you have like only.The transaction level memo only the.Destination you know M account on.Either transaction or operation right.Those three cases should all end up with.The right U you know transfer event with.The added memo information as far as.Like the you know do we need a an event.For the memo I don't know what the use.Cases for that are so.Yeah like like.People you can already get that by by.Pulling the transaction itself and you.Know cracking it open basically if you.Want if you really.That but I agree that that you can do.That.But like didn't didn't we want .Consumers to not have to do.That I guess I guess it depends on if.That's a requirement or not we care.About in the context of if I think the.Context of the CAP was.Transfers in the context of transfers.This sound seems to be out of.Sorry I'm I'm not totally tracking with.With what we've just.Said so Nico is saying that the that.That the emitting an event purely for.The TX memo like a separate event is out.Of scope in in terms in in the context.Of transfers I'm not sure about this spe.I'm specifically thinking about the the.Replay case for exchanges. cuz I you know if you what what what.Is what happens if you have a.Transaction of the TX memo and MX MX.Destination account because without.The TX memo event you can only show one.Right but in that case the thing is that.If people have like complex logic they.Will have to actually derive it from the.Transaction they have to know exactly.Okay did we which combination of things.Right where they into I think that's.Why I'm saying like the the transaction.Memo is not sufficient if you if you're.Get if you're starting to pull that.Thread like you have to do transaction.Memo two second on the transac I mean.Destination oh actually no this one I.Guess the override can only happen on.The two so it there's only one place but.If it's the.Source that's where you have like the.Transaction Source or the operation.Source right that are on yeah yeah the.Source does make this more.Complicated this is well I'm actually.Not sure if the source makes it though.Much more complicated does it isn't.Isn't the complexity that an exchange.Might be reading the TX memo today and.Actually ignoring the M the MOX address.And just using the G part of the max.Address isn't that the complex part yeah.Well so that's the that's scenario that.I like this event might help you solve.Maybe right because I'm sure that I'm.Guessing that happens today like if I.Send a an a payment to coinbase with.The MOX account set and a TX memo like.They if they they probably just I'm.Guessing they treat it as if the account.Wasn't mxed.Right no that's why we don't know.Because it's yeah under.Specified okay I guess for this we'll .Like to me that yeah like the we this.Discussion is more like I see the.Historical stuff as more like as a best.Effort because really like exchanges in.Particular are they really I mean do you.Really want people to to be reinges .You know from Genesis or something like. they already ingested already.Reconcile all these data in their back.End yeah that's a good.Point.Okay there's also the additional .Ambiguity of if someone's sending to a m.Address and has a memo at the.Transaction level you shouldn't.Necessarily assume that that transaction.Memo is a specification of a user on the.Destination side right it could just be.Some free text just for the purposes of.Describing the.Transaction yeah the point of this event.Was just to you know show all the.Information and have the consumers deal.With it but it sounds like we we we.Don't need to deal with this right now. also we're out of time the last thing.We were going to discuss we we can maybe.Do this offline is that we don't we no.Longer need CAP 64. but I don't know Deo if you want.To say anything about that yeah I guess.The consensus of the discussions has.Been that yeah CAP 64 is not needed and.Yeah this is just an official.Announcement that we Are CL on it.Because MOS will be implemented.Differently yeah that's.It all right then we're out of time.For today thank thank you for joining.And thanks everyone for participating.
