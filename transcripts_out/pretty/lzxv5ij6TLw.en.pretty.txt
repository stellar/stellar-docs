Hello everyone welcome to this week's.Protocol meeting we have a couple.Caps to discuss today and I'll start.By presenting cap.67 I'll share the link.Here so this CAP is about emitting.Events in Stellar in the same format.That we see in sorine for Stellar assets.So that a user can use the stream of.Events track.Balances and this CAP covers three.Separate High higher level changes .They're listed in the abstract of the.Cap and I'll first go over the the.Two smaller changes that make some.Semantic fixes to the seller asset.Contract events for compatibility with.Set 41 which is the token interface.Set so the first is that we're going.To stop emitting the admin the.Admin on the mint and clawback events .The admin is an implementation detail of.The.Sa and does not belong in the event.The admin doesn't even exist in sub 41.As a concept. the second change is that we're going.To update an edge case with regards to.Transfers to and from an issuer the.Sa was written in a way that matched.Classic payment semantics where a.Transfer from an issuer Min the asset.And a transfer to an issuer burns the.Asset the issue is that in these two.Scenarios we still emit a transfer event.Instead of the appropriate Mint or burn.Event so this CAP just proposes.Fixing that to Mint the the mint the.Correct Mint or burn event instead of.The transfer.Prevent so I'll pause there real.Quick if there are any questions about.Those two changes before I move on to.The the bigger change at.Hand all right then I'll move on to the.Third and most significant change of.This CAP which is emitting an event in.The same format as the Stellar asset.Contract for any asset.Movement and this means that a a.Stellar operation that results in the.Movement of an asset can emit. a transfer mint burn or clawback.Event and we're also adding a fee.Event to represent the fee paid by by.The Source count so the the cap.Specifies what what the events for.Each operation will look like but I'll.Just mention some interesting points so.A trade that occurs through either.Through an offer operation or path.Payment operation will result in two.Events one for each side of the trade .And the two parties on both events will.Be the source account of the operation.And the owner of the.Offer and it's and these these events.Can be transfer events but they can also.Be instead be a mint or burn if the.Issuer is.Involved we're also adding new SD.Address types to be able to represent.The claimable balances and liquidity.Pools as from or two addresses in these.Events and this allows us to represent.Movements to and from these entries.Yeah so silence asked can you.Explain removing the admin from the sa.Mint and Clack events So currently on.The mint and Clack events of the sa not.Not in set.41 the admin is one of is is one.Of the topics of the event but this is. this isn't necessary and the concept.Of an admin is not required for a.Token exactly yeah it's not it's not.We're not removing the admin from the sa. just it just says the topic we're.Removing it from the the topic of the.Event all right so yeah we're also in.Addition to those two new address types. for clal balances and liquidity pools.We're also adding another SC address.Type for MX accounts so that these new.Events can pass along the MX information.That the MX account is used in the.Operation or.Transaction it's also another thing.To.Sh.Screens so I'm I'm actually not.Presenting this in the order that it .Like it shows up in the CAP this is just. some points I thought were.Interesting but if you look at cap.67 these points should be listed.There at a at a with more.Detail then yeah so yeah so I wanted.To mention that these new events will.Not be Into The Ledger therefore will.Not be part of the protocol and we.Also will emit the emit these events.From Genesis if you replay from Genesis.And something to keep in mind here is.That there was a bug in a very old.Protocol version where xlm was minted.And burned and we will admit events. that will allow you to reconcile.Those Balan.Changes and finally we will be reworking.The format of TX meta with the.Transaction meta V4 that generalizes the.Events and separates them by operation.Previously the events were for .Formatted in a way where was very.Specific to Soroban and the fact that.Sorban only has for a single sbon.Transaction there's only one operation.But the transaction meta V4 generalizes.That and the plan is to provide a config.Flag that enables the emission of this.New meta.Format okay so let me look at some of.These questions.So will this resolve the issue with a.Locked issuer sa having an admin Sil can.Do things the issuer.Lock no so I think what you're sounds I.Think what you're asking is if you.Change the admin and you lock the.Issuer the ad okay ignore that yeah they.The admin can still do whatever it wants. what else does back filling mean that.Replaying with these new events enabled.Will emit meta4 for all ledgers rather.Than just post 23 yeah that is the goal. and we and that should work I we we.Probably have some stuff to work through.There but if we want to admit these.Events from the beginning then it has.To be meta V4.So MX addresses and memo so yeah one one.Of the issues is so the CAP addresses.The M partially by forwarding the M.Addresses into the events but there is. another question about.Memos that I'll link this discussion.Topic..From from.Nico it's.Relevant and at a high.Level this this amounts to.Considering a solution where you can.Group an arbitrary memo with a with a.Group of trans events this will allow.You to you know solve some issues we've.Had with in the past with regards to.The design of memos but that but.Yeah I don't know if Lee if you want to.Discuss this a little bit more that's.Something this concept is something we.Still have to.Discuss yeah good yeah so yeah NCO.May want to he he this is sort of I feel.Like his idea so he may want.To speak to this as well but there.You know we have the separate we have.Another CAP CAP 64 that we're not.Actually discussing today that's. doing some things with MOS it's.Touching MOS and where they show up.Where they will show up in.Tooling and you know making it.Possible.To do a soran transaction. with a memo and have that safely.Included in the signature so that's cap.64 and something that niik was bringing.Up with this CAP is that you know right.Now we're in this moment where we're.Making some significant changes to.Events or we're doing a lot of work.Around events and doesn't make sense you.Know we have to sort of we we sort of.Have to get close to Memos because we've.Got this issue with moer dresses and you.Know right now cup.67 .In a very non-controversial way is just.Suggesting that these events now might.Contain addresses that are MX addresses.So we're just going to pass it through.So if you do a payment.Operation that's got a destination of.A Max address that address field in.The San event could be a MX address it.Could be an M address instead of a g.Address or a c address and I think.That is like very not noncontroversial. it's sort of like straight line what.We I guess we would sort of expect the.Cap to do in the situation but there.Is this we have this history where you.Know once upon a time there was memos.Then there were M addresses right now.The ecosystem is pretty there's you.Know both get used in the ecosystem so.Both getting used by.Exchanges and but when you look at.The data the data tells a somewhat.Confusing story because this is memo.Field there's this MOX address they can.Be in the destination you know.Technically sources can also be.Addresses and nik's Nik was sort of.Suggest I don't know if I don't know if.This was actually a proposal but he was.Suggesting like do we do something more.Here do we do we make these events in.This moment more opinionated about what.The memo for this event is and you.Know do we do something like if there's.A transaction memo this event has a memo. is or if there's a destination of a m.Address do we pull the memo out of the.Address and we say well actually that's.The memo for this.Event to some degree like the ecosystem.Has to do this somewhere and you know.Right now it has to do this Downstream.Ecosystem tooling has to make a.Decision about where these events come.From and in the future like you.Know right now memo memos there's like.MX addresses are only a classic thing.Memos only occur at the transaction.Level in CAP 64 but.In the future like how do you attach.Memos to like other groupings or.Transfers if there's you know transfers.Being bundled together yeah sorry I'm.Not sure I'm actually doing a great job.Of summarizing this there's like a lot..Yeah yeah it's definitely something.That still needs to be thought through.But I guess the question is should we.Explore yeah addressing that issue in.Cap 67 it sounds like we should at least.At a high level I think I think we.Should and I think what nio just posted.In the in the chat is a really succinct.Way of for the why we should consider it.Now and that is that you know CAP 64 is.Forcing the memo to be exposed in down.TR systems for San events and so it.Makes sense since this CAP is very.Focused on that consistency between.Classic and Soroban and fixing it across.Those two dimensions of you know San.Versus classic sack versus set 41 it.Makes sense to discuss.Now yeah makes sense okay orbit asked.This question why did we decide to.Extend the meta with the vents if these.Events can be commuted dynamically from.The existing meta seems like redundant.Data in the meta and this is something.We actually.Discussed internally I believe George.Actually mentioned the same single.Source of Tru truth rather than risking.Bugs Downstream renting the wheel and.Dynamic compute also Lee do you have I.Say you mut yourself do you want to.Address this yeah I can I can speak to.This the a large part of this is that.You know when we look at the tooling.That we have today so we have a horizon.Which takes a lot of What's in the.Mattera today and creates this data.Model for people to consume this new.Data model for people consume and then.St RPC was designed with a different.Approach where the Stellar IPC really.Exposes the raw network data and.Tries to introduce as little as possible.In terms of its own data model on top of.That and one of the reasons that Stell.IPC was designed that way is that you.Can go to you know you can take.Contract events that have come out of.Star RPC and they look the same way as.If you go and take meta out of core or.If you use G Galaxy to you know.Generate a data Lake of meta like the.Events look the same way in all these.Different places except if you go to.Horizon you know Horizon has its own.Data model and it has that effects data.Model which.Is you know very different so the.Idea between the ibr like how like why.Bring these events to Classic and the.The the Stellar asset contract and the.CL operations is to create this like one.Unified view that appears everywhere.That looks the same everywhere that.That the data.Occurs okay I hopefully that answers.That question but if not let us.Know I think Lee you mentioned.Another thing about another question.About asset names on the on the the.Topic of the events right because the.Currently go ahead maybe before we move.On it does sound like orbit has some.Things I'm Orit do you want to do you.Want to speak.To yeah hello everyone just wanted to.Add my two Sense on this we've been.Successfully reconstructing everything.From transaction meta xdr on the.Client side.Basically our inje pipeline in Stell.Expert is built on top of the.Library that we built specifically.For this purpose and we had a.Conversation with the Horizon team.Before like several years ago I think .Where I proposed to.Actually remove all this excessive data.Tables like effects and everything.Like this because all of it can be.Reconstructed on the client.Side of course there are very.Specific issues like the memo.Case lay covered before that. but maybe we can just. like conduct some research on it and..Maybe the Parson Library we already have.And which have been tested for years.And which works with classic and.San maybe it's enough because on the.Client side people can just use it.Directly in JavaScript to parse the.Response they receive from.DC. Horizon now doesn't support.Transaction meta so it's.Probably case when we cannot use it.Directly but at least from RPC it has. zero problems is.Parson looks like George has something.To say but I can't bring him up on.Stage there oh okay yeah I actually.Didn't have anything to say I think.It's a I think or makes a good point .But I do think that a unified event.Stream is better for like your.Average dap developer even though we.Can do all the indexing on the Fly it's.Probably more catered towards like.Infrastructure providers like Stellar.Expert and.Horizon so this is still going to have.Value for people who are just.Interacting with RPC.Directly I mean we've been using it to.Actually display transactions in.Sellar.Expert be wallet reflector.Refractor.Everywhere I'd say it's a universal.Approach that can be used everywhere I'm.Not insisting on it it just seems to me.That transaction meta will be even.Larger after that.And it might have some duplicated.Data because of of this.Can you hear me.Well yeah I can hear you yeah okay great. so I have a couple thoughts on this. first if the meta size increase is.Concerned I think I don't know if we.Made it explicit anywhere but I think.This should be optional for the most.Part. at least I don't see a good reason.For not making Z. so I don't think and the same goes.Actually for The Ledger changes .Ideally you would be able to pick one or.Another depending on what is your.Downstream.Processing whatever suits you then.Regarding the library I wanted to point.Out that well a not wa JavaScript is not.The only language out there but also.There is like some Nuance to like there.Is very Nuance difference between Ledger.Divs and events because events tell you.What exactly has happened during the.Transaction whereas weder deeps tell you.What was the state after the transaction.And I think it matters more for soran.But I don't know it might have some.Interesting implications for classic I.Don't know if they're interesting to.Anyone but. just that the format of the event.Like if you're processing Samson you.Might be interested in processing Samson.As a stream of specifically events right.And not a stream of some data types.That depend on the operations like can.See from the library correct me if only.Them missing something there but this.Don't seem to me like this stand and the.Idea here is that you know if I wanted.To track just the movement of the.Balance and I not particularly.Interested in the exact semantics of.Each and every store operation what I.Could do is I could just ingest the.Event stream and track the balance .For a given.Account which I think may be.Interesting to some consumers so yeah.This are just some thoughts on what why.Buer doing this at.All I think the argument regarding.The event stream is pretty solid because. this way the server in this case.RPC handles the streaming itself and. indexing and cses and Etc so.Instead of working with transactions and.Parson transactions it can like.Really rely on these events it's a solid.Argument it point is that I'm not.Proposing to use JavaScript everywhere.Maybe we.Can like Port this to rust or.Other languages and use it inside RPC.Inside other applications instead.Of adding the data we can like do.It on the fly in RPC itself right .Regenerate this events because.Basically all described events that what.We do with.U with the transaction.M xdr that's something we already.Do so maybe it's just one of the.Options instead of extending .The xdr itself.Again because to me it looks already.Pretty.Large my subjective opinion on.This but again if you could opt out of.Imian events would that solve your issue.With the size because it's I think.Besides the size concern I feel like the.Suggestion of let's put Comm.Implementation somewhere we just.Suggesting well why don't we put this.Common implementation right into ore I.Don't think it makes a huge ideological.Difference and I think it does make.Something easier to maintain and to.Ensure that they works correctly for b. yeah especially if we are doing.Protocal changes so yeah and just for.The say I totally agree and.Yeah so I don't know if you have any.Strong arguments but I feel like you're.More or less in the same page here in.Terms of standardization because it I.Guess just in of putting it into cor.Kind of make.Sense yeah the standardization it sits.Inside The Narrative of everything being.A contract so I think like that's what's.Beautiful about this is that you know.Today we have you know classic.Operations and then we have everything.That happens on Soroban and the result.Of those things looks pretty different.But as a result of this work we're.Moving towards this future where.Actually everything that just happens on.Classic looks exactly like a contract.That's executing and you know all assets.Actually do have a contract that's.Reserved for them they have the Stellar.Asset.Contract and so the events are just.Going to look like everything that.Happens with assets no matter if it's.Classic or anything like that it just.Looks like a contract which is also.Much more similar to other ecosystems .Other like blockchain ecosystems as well.So there's that familiarity for.Everybody about how this works versus.Say ethereum and other.Things yeah and I I'll make sure to.Clarify in the the CAP that nothing's.Being removed from meta and this will be.Optional that's a good point.I mentioned the config flag for.Transaction met V4 but I I'll go into.That in a little more detail are.There any other.Questions I didn't quite catch if we.Have made any final decision on.MOS like what we talked about like 10.Minutes ago.Wa wait what is the final stages because.I maybe I've lost it yeah so we're for.In the context of this CAP we are the.We're forwarding the M account.Information right into the the addresses.But for memos we still need to explore. what Nico mentioned earlier like so.There's nothing finalized there but we.But we are going to go look into that.Right soal decision okay I was just kind.Of missed it okay yeah.No yeah I'm not I'm not sure if we want.To discuss that more because it's not.Actually nobody's actually written like.A a formal this is how we're how we.Would propose an alternative for it to.Work I mean I could say something right.Now that I.Think I think aligns with what niik.Was talking about but I don't know if.I I think it'd be valuable if we.Actually get something written down.Then present it in the future meeting.Because I know we we have 30 minutes.Left and dimma has a couple caps to.Present that's probably more valuable.Right.Great all right go ahead okay yeah.Great yeah I don't actually mind.Spending more time on the events if.Necessary but if you're done we.Can go.App . that's six hisory .So this is basically a CAP that .Mostly changes the transaction set .But it does so in a pretty interesting.Way so as you may or may not know .Al a hustle you need to go through with.A Footprints inur transaction is done.For a good reason and this reason is .Being able to run the transactions in.Parallel because if you know your.Footprint we know that you don't have a.Data dependency between two.Transactions and you actually can run.Them in parallel without worrying about.Any.Synchronization so given the.Footprints in theory today what you.Could do is you could right your own.Version of core application logic.That takes the transactions and.Partitions them somehow into threads.That R par from each other but the.Issue with that is that there is no.Good boundary on how much time is it.Supposed to take for example if you take.10 transactions without data.Dependencies you could run them in 10.Shads and they would for example take 10.Milliseconds after synchronization or.You could have the same 10 transactions.But all of them dependent on some W your.Entries are being updated and thus you.Cannot schedule them into 10 different.Threads you would run them in the single.Thread fres and in the end you will.Spend not 10 milliseconds but 100.Milliseconds to apply all.Transactions so basically there is.Since protocol doesn't do anything.Currently about.Pration there is no good way to.Schedule the transactions in such a way.That it takes some bounded and.Expected well well there is still an.Upper bound on the runtime but this.Upper bound rise wildly and it's not.Good if Ledger May close both within 100.Mills or within like two seconds for.Example which is why we are doing.This CAP fix3 which solves exactly that.Problem which is given a set of.Transactions come up with a transaction.Set data structure that guarantees .Certain time for appliances.Transactions is a c of course of that.Time being time in model constructions.And not of course well time CL time.Because we kind of cannot tell it.Beforehand without tring the contracts.But we hope that our cost models are.Good enough and another cave being that. you actually should have the fish is.Sufficient amount of physical course to.Support multi stading because well again.If you have just one core obviously it.Doesn't matter if you run 10 shreds .There some.Here so if you have just a single.Core and you wanted to apply.Transactions in 10 threats obviously.You'll not get any performance gain.Because well transactions are purely CPU.Bound. so that's the motivation for this.Cap and the way it.Works is that we Define a new.Structure for the transaction set that.Instead of having a linear array of.Transactions defines two levels of.Group transactions together and the.First level is called.Stages and I will talk about why.Stages and what does it need for any.Moment and then every stage consists.Of multiple clusters of transactions and.Within the cluster transactions.Generally may have data dependencies so.It is expect Ed in this cluster to have.Data dependent transactions that need to.Run.Sequentially however there are no .Data dependencies between the Clusters.Themselves and thus since every.Cluster is guaranteed to be .Independent of every as cluster what you.Could do is you could take every cluster.And put it into a separate physical.Thread and thus you can apply the.Whole stage in parall is as many.Threats as you have.Rers. also I probably forgot to clarify.What we consider data dependency it is.When a single transaction has an entry.In a retrade footprint and then another.Transaction has the same entry in either.Readon or retrade footprint me that one.Transaction modifies the entry another.Transaction as read so modifies it in.Either case we need to sequence them.Because if you don't do that then we.Will get non deterministic results for.Any these.Transactions now. why do we have this additional .Level of transaction grouping in.Stages the reason is that if we.Just try to naively say hey let's just.Par the put the transactions in separate.Threads put them transaction set and.That's it then you may run into some.Issues with C traffic patterns for.Example.Imagine an oracle contract for.Example right and imagine it updates and.Entry some key and then there are bunch.Of transactions that want to read the.Value from the contract from the ccal.Contract and that key and thus we.Have data dependency on a single.Entry but we have only a single rate.Of the entry but we have a lot of rats.Of the exact same entry so what we.Can do with this stages is that you can.Put right into one stage and then all.Reads into different.Stages.And basically this introduces just a few.Barriers of execution into the.Transaction execution schedule but it.Allows kind of efficiently work around.This traffic patterns and given.Smart enough maybe not even smart enough. algra for actually coming up with.This data structure you can deal with.A surprising amount of conflicts all at.Once without introducing any special .Scheduling and synchronization procedure.So basically just have a few barriers .In between the transaction applications.And this allows resolving a lot of the.Conflicts at the same time before.Driving deeper I go read that for a.Moment and there any questions far. okay the new adjust approach.Okay so this app itself just changes.The structure of the transaction sets it.Doesn't have any particular changes.Around how we which transactions do.We pick at.All but what I can say that the.Protocol specification remains such that. that transaction ification order is.Randomized and I think in case of M.Manipulations nothing changes from.The current approach because for any.Arbitrage you're probably going to have.The data dependency and data dependent.Transactions are applied sequentially.And all we shuffle all the stuff that.Is being applied sequentially so the.Transaction order is still for to.Predict and yeah I don't think it's.Any easier to manipulate than it is.Currently which is to say it's not.Impossible but just spaming Ledger is a.Arbitr transaction for. yeah in terms of the system.Requirements again the cup doesn't.Define them I will will talk about.This in a moment but of course yes.If we want to run transactions in.Parallel then well you need to have .Certain number of course present on.The validator which is I guess a.Tradeoff with the like that's.Horizontal scaling the notot use better.Hardware as then cannot find more.Transaction. M resarch maybe maybe I think I.Already have the section on shoing the.Transactions so may be efficient maybe.Not .Yeah okay I see question regarding Z.And let me actually go a bit deeper.Regarding how exactly things are.Specified first thing is that .Moro does introduce a new network.Configuration setting for the maximum.Number of f per stage which roughly well.Not roughly but basically maps to the.EXP Ed number of course that your.That the network is willing to throw it.When ledgers if you have less course.Then it may take for you longer .Than expected to pric transactions you.Have more cores or at least that many.You should be good and as all the.Network configuration settings this.Setting will be Modified by.Validator vot so you know some.Validators say that we don't have that.Many cores they will not vote for.This and ultimately Network can.Decide if requirements for to rers are.Too expensive or too hard to come.This come up this and also to make it.Clear when we upgrade to protocal 23.This setting will be set to just one so. any parallelism allowed as network.Settings so you know that to enable any.Parallelism Network quote to.Happen Okay .Then to the structure okay I already.Talked through the specification of the. data structure.The phas fee and search pricing will.Work in the same way as it works now.Which is there will be a single base fee.For the.Transactions it is 100 Stoops if.There is no search price in and if there.Is SAR pricing which means like if there.Were transactions that you couldn't.Include in The Ledger but this that were.In the mle we take the fee of the.Cheapest transaction that we do include.And use it as a base fees that everyone.Has to.Pay. okay and now again to the apply order. first there is still a canonical.Apply order that is defined in CAP so.You still can apply all the transactions.Sequentially and arrive at the same.Result as you would if you were if.You applying them in.Parallel and similar to what we do.Now we Shuffle the things before.Applying them first we Shuffle every.Cluster using the hash of the.Transaction set and hash of every.Transaction then the for the Clusters.We don't need to shuffle them but.By the transaction.Hashes and then we Shuffle the stages as.Well so yeah like so transactions are .In different stages one may happen.Before or after another.One. that's kind of what to do for Shing.And I think it's not significantly.Different from our St. yeah I don't think it does.Anything specific.To again it doesn't changes status quo.For M on the large scale like I think.The best thing you can do is still just.Spam the network is heritor transactions.It is the issue we have now and it.Persists like the goal of this CAP is.Not to prevent these types of attacks. but it does not make it any easier to.Guarantee any particular execution order.Between two.Transactions so yeah I don't see like.If you see something specific like maybe.You know maybe we can up Asing later .But I yeah I don't think there is any.Change to to the execution orders that.Is relevant.For. right okay so that's a kind of.Simpler part of the CAP so again .Multiple threads to apply transactions.In parallel oh the most important.Thing that I forgot to mention is that.We limit the number of sequential.Instructions across all the stages which.Means that for every stage we look at.The cluster that takes the longest time.In terms of model instructions and then.We sum up this numbers across all the.Stages and then we come up with some.Number of sequential modeled.Instructions which are hopefully.Approximating the real runtime for.Applian the transaction set given and.Physical threats for example if the.Ledger limit currently is 500 milon.Instructions with this CAP for.Example if you set Ledger Max dependent.TX clusters to for example eight meaning.You need eight physical threads and we.Keep the limited 500 million.Instructions we expect the weder.Application time be roughly a factor of.500 million virtual instructions which.We still on to few hundred milliseconds.Currently our cost.Models. yeah so that's kind of the simpler.Part of the CAP the trickiest thing.Is the GTL update semantic change. so all this data dependency stuff is.Very good and nice but one thing that.We again consciously did when La in.San is we allow extending the dtl of.The entries even if the entries are.In read only footprint and the reason.For that is that well we expect TTL.Updates to be really.Prevalent and if we treated them as.Rights there is a big chance we.Wouldn't be able to AR anything at all.Because things would be just just .Clump together with all this clusters of.For us contract that happen to update.Totl on the same.Entes which is why we decided that we.Actually can reconcile the r changes. after running all the transactions.Without risking introducing any.Nondeterminism and this TTL updates.Someand exchange is doing exactly.That changes how we update ttls in.Such a way that changes to TTL .Done by transactions that only touch the.Readon entries are not observable.Until someone actually writes the.Entry or until we have finished applying.All the.Transactions which is good enough to.Be able to actually run all the.Transactions that have the same ke only.The run them in parallel and still right.Have the proper TTL.Value and the algorithm proposed is .Pretty leny but the gist of it is that. if two transactions update to.TL on the same.Entry they both will see the initial.State of the TTL of the.Tantry and will be charged the.Respective fee for set so for example if.Transaction a has a key in with only.Footprint and updates TL of some say xlm.Contract rate by 200 ledgers.Increases TTL by 200 ledgers and then.Transaction B increases TTL of exm.Contract by 100 ledgers well the.Transaction a will pay for extension by.200 ledgers and transaction B will pay.For the D extension by 100 Ledges but.What we will do in background is we will.Correct this TTL changes and only apply.The maximum change out of that to the.Ler out of this two to the ler which.Means that the exm contract will be.In the end extended by 200.Lers and it does introduce bit of.Annoyance into how mattera should be.Processed which is unfortunate but I.Couldn't think of a way that is both.Compatible with prism and doesn't.Require any changes in met ingestion and. this change is.That basically when ingesting the.Changes in TLS from the mattera you.Should never decrease TT. this is only thing you need to do .It is documented in the CAP and you'll.Probably make another announcement .But yeah we kind of get around doing.Something special for the map and this.Seems like the minimum possible change.So TTL like that you record in the back.End and never go down but the TTL That.You observe in the.Is correct DET change for the mattera is.Correct in the context of a given.Transaction meaning as I mentioned.Before that will charge the fee .Based on the changes that you observe in.Meta. and this is kind of the ju of it and.U right. yeah I can talk a little bit about.Candid generation and then move on to.Question caps don't typically provide.A way of how to actually build a.Transaction sets which makes sense it.Doesn't need to be a part of the.Protocol as long as it is valid it.Doesn't matter how it has been built .But will'll likely implement the simple.Gitty algorithm described in the cap.And the idea behind it is just to pack.The transactions into the stages.R while uring we are utilizing the.Stes properly by being packing from.Time to time and the efficiency of being.Packing heris sixs is surprisingly .Good so in the end I think we can I.Added some benchmarks and we can both.P the leeders pretty efficiently like.Uze most of the instructions and we can.Also deal with a decent amount of.Conflicts like if you have some.Relatively sparse clusters of conflicts. that will not cause any depredation.In lure utilization at all and in order.To have any real degradation un need.Like some super inter interdependent.Transaction for which I don't expect to.Be happening in the wild so I.Think this should be good enough at.Least initially and if we ever.Observe there are issues with that .Ways to deal with that both at the mle.Level and the U wayer of building the.Transaction sets. I think that's all I wanted to talk.About just to present the CAP and now.I will most through the chat and read.Some questions. yeah question from niik about the.Meta changes for.TTL. M net diffs for.Ttls not emit one per.Transaction well we still need to.Emit TT per transaction I think because.They are an effect of the transaction.This is how we charge P and I I'm not.Sure it is correct to just omit.Them and. whether we want to update meta I have.Considered this I I'm not sure it is.Necessarily a better solution so.Basically the alternative to what is.Proposed in this app is well we could.Add yet another field to the mattera.That will explicitly output the.TTL every entry facted by the wer the.Final for it after applying every.Transaction in The Ledger. I like that's potentially a.Significant amount of duplication unless.We also kind of remove the changes for.Transaction I'm not sure if it is a good.Idea to remove the changes per.Transaction because then well meta just.Has some entries drop that transaction.Has actually affected which makes tricky.To debu transactions for example makes.To understand your fees if you ever.Wanted to run the computation logic.Comp so I'm sure we can omit that Pro.Transaction changes but then I'm not.Sure if duplicating this data is.Necessarily making things much easier to.Just not decrease in TTL with your track.On.It. so that's my opinion .And I mean you either need special.Logic for merges or you need special.Logic.For this new types of TT entries and you.Also need to kind of sometimes ignore.That for transaction changes I'm.Honestly not convinced if it is a good.Idea and if you wanted it possible to.Only ingest the T changes after the LED.Then so that you don't even need to.Process per transaction changes at all.Then basically we duplicate every TTL.Change into which is again probably not.Very Fe you don't know n if you want.To talk more about this. okay I don't see any more questions. and we actually have just three.Minutes left so if anyone has any.Questions please feel free to comment.On the ad discussions read it is yeah.I guess the to meta change is really the.Most controversial thing about it.Everything else.Is really uncontroversial I feel because.Though we are just allowing to include.More transactions in Ledger and we still.Shuffle them so no changes for any so.Yeah yeah and one final thing I'll say.Thanks thanks for presenting Dima .We we forgot to talk about back to the.Unified events a one topic about how the.Sa events have the asset name of the.Topics but SE 41 doesn't specify .Having putting the asset name in the.Topic so lee is about to start a.Thread about this so we can discuss it.There yeah yeah Le just posted the .Thread in the chat yeah that's all.That's all we have for today thank you.For joining.
