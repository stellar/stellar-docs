---
title: Installing
sidebar_position: 20
---

We recommend the following ways to deploy your own RPC instance:

1. Deploy to Kubernetes using [Helm](https://helm.sh/docs/intro/install/)
2. Run the [stellar-rpc docker image](https://hub.docker.com/r/stellar/stellar-rpc) directly

### Kubernetes With Helm

If your deployment environment includes Kubernetes infrastructure, this is the preferred way to deploy your RPC instance. Hereâ€™s what you need to do:

1. Install the [Helm CLI tool](https://helm.sh/docs/intro/install/) (minimum version 3)

2. Add the Stellar repo to the helm client's list of repos, update it to the latest published versions

```bash
helm repo add stellar https://helm.stellar.org/charts
helm repo update stellar
```

3. Deploy the RPC instance to Kubernetes using [our Helm chart](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc):

```bash
helm install my-rpc stellar/stellar-rpc \
--namespace my-rpc-namespace-on-cluster \
--set global.image.sorobanRpc.tag=22.1.1 \
--set sorobanRpc.ingress.host=myrpc.example.org \
--set sorobanRpc.persistence.enabled=true \
--set sorobanRpc.persistence.storageClass=default \
--set sorobanRpc.resources.limits.cpu=1 \
--set sorobanRpc.resources.limits.memory=2560Mi
```

This example of Helm chart usage highlights some key aspects:

- Set the `global.image.sorobanRpc.tag` to a tag from the [stellar-rpc dockerhub repo](https://hub.docker.com/r/stellar/stellar-rpc) for the image version you want to run. Refer to [the software versions page](/docs/networks/software-versions) to find the correct tag for the Soroban release you are running.

- The RPC server stores a revolving window of recent data from network ledgers to disk. The size of that data varies depending on the network and its transaction volumes, but it has an estimated range between 10 to 100 MB. To ensure the RPC pod has consistent access to disk storage space and read/write throughput, this example demonstrates how to optionally enable the Helm chart deployment to use a `PersistentVolumeClaim` (PVC) of 100MB from `default` storage class on Kubernetes by enabling these persistence parameters:

```bash
--set sorobanRpc.persistence.enabled=true
--set sorobanRpc.persistence.storageClass=default
```

By default, this is disabled (`sorobanRpc.persistence.enabled=false`) and the RPC deployment will use ephemeral pod storage via `emptyDir`, which will likely be adequate for `futurenet` and `testnet` transaction volumes. However, it's worth highlighting the trade-off of not having storage limitations of the cluster enforced (likely below 100MB).

- Network presets are defined in [`values.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values.yaml), which currently sets network configuration specific to `futurenet`. You can override this default and use other "canned" `values.yaml` files which have been published for other networks. For example, there is a [`values-testnet.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values-testnet.yaml) file to configure the deployment of the RPC server with the `testnet` network. Include this `--values` parameter in your `helm install` to specify the desired network:

```bash
--values https://raw.githubusercontent.com/stellar/helm-charts/main/charts/soroban-rpc/values-testnet.yaml
```

- Configuring RPC to use other custom networks can be accomplished by downloading the [`values.yaml`](https://github.com/stellar/helm-charts/blob/main/charts/soroban-rpc/values.yaml) locally and updating settings under `sorobanRpc.sorobanRpcConfig` and `sorobanRpc.coreConfig`. This is applicable when connecting to specific networks other than the existing files like `values.yaml` available, such as your own standalone network. Include the local `values.yaml` in `helm install`:

```bash
--values my-custom-values.yaml
```

- Verify the `LimitRange` defaults in the target namespace in Kubernetes for deployment. `LimitRange` is optional on the cluster config. If defined, ensure that the defaults provide at least minimum RPC server resource limits of `2.5Gi` of memory and `1` CPU. Otherwise, include the limits explicitly on the `helm install` command via the `sorobanRpc.resources.limits.*` parameters:

```bash
--set sorobanRpc.resources.limits.cpu=1
--set sorobanRpc.resources.limits.memory=2560Mi
```

Even if you're not deploying Stellar RPC using Kubernetes, the manifests generated by the charts may still be a good reference for showing how to configure and run Stellar RPC as a docker container. Just run the `helm template` command to print the container configuration to screen:

```bash
helm template my-rpc stellar/stellar-rpc
```

### Docker Image

If using Kubernetes is not an option, this is the preferred way to deploy your own RPC instance.

:::caution

Although we have a [Quickstart Image](https://github.com/stellar/quickstart), it's for local development and testing only. It is not suitable for production-grade deployments.

:::

Here's how to run the [stellar-rpc docker image](https://hub.docker.com/r/stellar/stellar-rpc):

1. Pull the image at the version you'd like to run from [the tags](https://hub.docker.com/r/stellar/stellar-rpc/tags):

```bash
docker pull stellar/stellar-rpc
```

2. Create a configuration file for [Stellar Core](https://github.com/stellar/stellar-core). Here is a sample configuration file for Testnet:

```toml
HTTP_PORT=11626
PUBLIC_HTTP_PORT=false

NETWORK_PASSPHRASE="Test SDF Network ; September 2015"

DATABASE="sqlite3://stellar.db"

# Stellar Testnet Validators
[[HOME_DOMAINS]]
HOME_DOMAIN="testnet.stellar.org"
QUALITY="HIGH"

[[VALIDATORS]]
NAME="sdftest1"
HOME_DOMAIN="testnet.stellar.org"
PUBLIC_KEY="GDKXE2OZMJIPOSLNA6N6F2BVCI3O777I2OOC4BV7VOYUEHYX7RTRYA7Y"
ADDRESS="core-testnet1.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_001/{0} -o {1}"

[[VALIDATORS]]
NAME="sdftest2"
HOME_DOMAIN="testnet.stellar.org"
PUBLIC_KEY="GCUCJTIYXSOXKBSNFGNFWW5MUQ54HKRPGJUTQFJ5RQXZXNOLNXYDHRAP"
ADDRESS="core-testnet2.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_002/{0} -o {1}"

[[VALIDATORS]]
NAME="sdftest3"
HOME_DOMAIN="testnet.stellar.org"
PUBLIC_KEY="GC2V2EFSXN6SQTWVYA5EPJPBWWIMSD2XQNKUOHGEKB535AQE2I6IXV2Z"
ADDRESS="core-testnet3.stellar.org"
HISTORY="curl -sf http://history.stellar.org/prd/core-testnet/core_testnet_003/{0} -o {1}"
```

3. Create and mount a volume [mount a volume](https://docs.docker.com/storage/volumes/) on your container where the above configuration is stored. An example for Testnet:

If you are running locally on a macOS for instance, you could create a folder in your home directory: `~/test-rpc-config`

```
cd ~
mkdir test-rpc-config
```

Then you would add the following config files to that local directory:

- soroban-rpc-config.toml
- stellar-captive-core.cfg

Then you would mount that volume using by adding the following parameter: `-v /Users/<USERNAME>/test-rpc-config:/opt/stellar` to your `docker run` command.

Your running container would mount that volume at the path `/opt/stellar`

4. Run the image

In that example, you could run the `stellar/stellar-rpc` container with the following command:

```bash
docker run -p 8001:8001 -p 8000:8000 \
-v /Users/<USERNAME>/test-rpc-config:/opt/stellar stellar/stellar-rpc \
--captive-core-config-path=/opt/stellar/stellar-captive-core.cfg \
--captive-core-storage-path="/var/lib/stellar/captive-core" \
--db-path="/var/lib/stellar/soroban-rpc-db.sqlite" \
--stellar-captive-core-http-port=11626 \
--friendbot-url="https://friendbot-testnet.stellar.org/" \
--network-passphrase="Test SDF Network ; September 2015" \
--history-archive-urls="https://history.stellar.org/prd/core-testnet/core_testnet_001" \
--admin-endpoint="0.0.0.0:8001" \
--endpoint="0.0.0.0:8000"
```

## Next Step

After installation is complete, you are now ready to proceed to [Configuring RPC](./configuring.mdx)!
